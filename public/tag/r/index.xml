<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R | Dylan&#39;s Blog</title>
    <link>/tag/r/</link>
      <atom:link href="/tag/r/index.xml" rel="self" type="application/rss+xml" />
    <description>R</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Wed, 27 May 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>R</title>
      <link>/tag/r/</link>
    </image>
    
    <item>
      <title>The Perils of Overly Local Optimization</title>
      <link>/post/local-optimization/</link>
      <pubDate>Wed, 27 May 2020 00:00:00 +0000</pubDate>
      <guid>/post/local-optimization/</guid>
      <description>


&lt;p&gt;WORK IN PROGRESS, scattered notes/plots here visualized, will be turned into proper post soon. Aims to explain in layman’s terms the way that the addition of noise can help gradient descent escape spurious optima (and how this has an intuitive interpretation in our own lives)&lt;/p&gt;
&lt;div id=&#34;outline&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Outline&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Note, this was openly inspired by the writings of Tim Harford. I ound his descriptiption of the London Tube Strike and its implications while struggling through the study of Langevin diffusions, and I had been meaning to try and show some visualizations of noisy optimization for a while. He’s a wonderful popular economics communicator, and a lot of fun to read.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In general, try and think of any and all images you can include! minimize text.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;the-perfect-scrambled-eggs&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The Perfect Scrambled Eggs&lt;/h1&gt;
&lt;p&gt;(or “our optimized lives”)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[rw]My morning routine is perfectly optimized, a blend of art and science. I’ve settled on an order in which to take the ingredients for scrambled eggs, the exact selection of dishes to use to minimize washing, and the fact that my silver bowl has sides just low enough that beaten egg could escape, and that raw eggs go in the red bowl. I didn’t set out to figure this out, I just made scrambled eggs every morning for years, and over time saw tiny choices that make it go a little smoother.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To add…&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;disruption-and-the-london-subway-strike&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Disruption, and the London Subway Strike&lt;/h1&gt;
&lt;p&gt;On February 4th, 2014, the workers of the London Tube (their underground subway), went on strike, forcing a number of station closures throughout the city. For three days, a subset of commuters found their usual routes to work blocked, and were forced to find alternatives. After the strike, the stations reopened, and life went back to normal.&lt;/p&gt;
&lt;p&gt;But three economists saw the opportunity for a natural experiment: only certain commuters were disrupted by the closure, and using the tracking data collected by the Tube, their movements could be compared to the unaffected group. And surprisingly, life &lt;em&gt;didn’t&lt;/em&gt; go right back to normal after the ending of the strike. The ID card data showed that some of the impacted group stuck with their newly discovered routes, and their commuting time decreased as a result.&lt;/p&gt;
&lt;p&gt;The station closures forced commuters to break from their finely tuned routines, but a surprising number found themselves dragged unnwillingly along a route that proved faster than the one they had been taking for years. The economists even claim that on net, the strike decreased the total commuting time.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; Even if only a small fraction found new routes, their new route would benefit them for years to come, and taking this chance only required disruption for a couple of days.&lt;/p&gt;
&lt;p&gt;This story provides a neat way to frame the mathematical challenge of optimization, and in particular, why so many optimization algorithms involve &lt;em&gt;random noise&lt;/em&gt;, a choice which initially seems bizarre. I’ll begin by describing how to think about optimization in intuitive terms, and then detail how that corresponds with the common mathematical algorithms, and why the London Tube Strike provides a neat analogy to very real mathematical properties.&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-mathematical-framework-of-optimization&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The Mathematical Framework of Optimization&lt;/h1&gt;
&lt;!-- This is duplicate with above? --&gt;
&lt;p&gt;The purpose of this post is to show how this concept can be naturally articulated in a mathematical framework.&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; We are describing the challenge of optimization. Here’s a sample definition, from the &lt;a href=&#34;https://deepai.org/machine-learning-glossary-and-terms/mathematical-optimization&#34;&gt;DeepAI website&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Mathematical optimization is the process of maximizing or minimizing an objective function by finding the best available values across a set of inputs.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;For London commuters, their “objective function” might be the duration of the commute,&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;, and the “available values across a set of inputs” might be the route they take.&lt;/li&gt;
&lt;li&gt;If we want to find the highest peak in a mountain range (an example we’ll discuss in detail in the next section), the input might be the choice of latitude and longitude, and the objective the elevation at that point.&lt;/li&gt;
&lt;li&gt;When baking chocolate chip cookies, the possible inputs might be the amount of each ingredient, and the order in which they are combined, and the objective function might be the tastiness of the resulting cookie. This isn’t quite a joke, Google once ran a &lt;a href=&#34;https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46507.pdf&#34;&gt;cookie optimization experiment&lt;/a&gt; along these lines.&lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Optimization is vitally important in applied mathematics and statistics, but it can be quite hard. The vast increase in computational power has broadened our horizons for what optimization challenges are possible, but many useful problems remain out of our reach.&lt;/p&gt;
&lt;p&gt;Generally, in optimization, we assume that we can check the value of the objective function at some chosen input. We can try a certain route on the subway and time the duration, we can bake a batch of cookies and eat them, and we will imagine that we have some computer which takes in latitude and longitude coordinates and spits out the altitude (this is called a “query” to the objective function, or perhaps “oracle”).&lt;/p&gt;
&lt;p&gt;Our naive optimization strategy might be to just try a bunch of points. Maybe we can define a grid of latitude and longitude coordinates, check the elevation of each one, and pick the highest. But this naive approach only works when the potential choices of input are sufficiently narrow. In particular, we look at the “dimension” of the input space. Latitude and longitude represent a mere 2-dimensions. The number of points within an exhaustive “grid” of inputs grows exponentially with the dimension of the input. Cookies are comprised by some dozen ingredients. The famous Jacques Torres &lt;a href=&#34;https://cooking.nytimes.com/recipes/1015819-chocolate-chip-cookies&#34;&gt;cookie recipe&lt;/a&gt; has &lt;span class=&#34;math inline&#34;&gt;\(12\)&lt;/span&gt; ingredients. If we wanted to try all combinations of a mere five levels of each ingredient, that would be over two hundred thousand possibilities, which is a bit much for even the hungriest baker.&lt;/p&gt;
&lt;p&gt;We are constantly confronting optimization challenges in our own life, and we rarely take this naive approach. Instead, we tend to look for small &lt;em&gt;local&lt;/em&gt; optimizations, whose benefits we can easily identify.&lt;/p&gt;
&lt;!-- * Finding the best route on the subway is something we optimize. While often algorithmic services can do the work for us, in other cases (or the past), we have to figure it out ourselves, and we somehow pick our starting point. --&gt;
&lt;!-- * For an example, let&#39;s imagine we want to find the highest point in a mountain range. We can check an individual point, and see how high it is. How do we find the highest point? The analogy would be something like, instead of latitude and longitude in a mountain range, in our morning routine it would be that &#34;series of choices for how we get to work&#34;.  --&gt;
&lt;!-- * Now, imagine instead of querying a computer for the height, you yourself were actually in that mountain range. What would you do? Well, you&#39;d look for the peak, but what if you were in a forest? You&#39;d go up. Specifically, you would look at the area around you for *local* guidance. --&gt;
&lt;/div&gt;
&lt;div id=&#34;locality-in-optimization&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Locality in Optimization&lt;/h1&gt;
&lt;p&gt;Imagine if we bite into a warm cookie, and the text the texture is just right, but there’s not enough chocolate.(more flowery language here?) We wouldn’t just throw up our hands and start from scratch… we’d add more chocolate! It’s hard to look at a cookie recipe, and imagine what the result will taste like. But we &lt;em&gt;can&lt;/em&gt; taste a recipe, and imagine what a small tweak would taste like.&lt;/p&gt;
&lt;p&gt;The “locality” in optimization refers to “closeness” in the input space. In latitude and longitude, this would be literal distance, while the definitions are a bit less cookie in the space of cookie ingredients and subway commutes. Broadly, we are pretty good at understanding. If we increase the amount of chocolate, we now they’ll taste a bit more chocolate-y than they do now. If we increase the amounts of chocolate, butter, baking soda, reduce the amount of flour, and tweak the balance between brown and cane sugar, are we confident we know what the result will taste like?&lt;/p&gt;
&lt;p&gt;In the London Tube experiment, the researchers place some of the blame on the “stylized nature” of the Tube map displayed to commuters. Its spatial distortions make it difficult to spot major inefficiencies in their route, without some external push. By comparison, we can usually estimate the impact of these “local” changes by ourselves. If we get off one stop earlier, maybe our walk to work increases by a minute but we avoid the wait at a crowded station. These slight changes are a bit like increasing just a single ingredient in a cookie.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;london_tube_map.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Personally, this describes my relationship with my own food. TO ADD MORE…&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In my own life, I find this with food all the time. I have a set rotation of dishes I love, that fit my criteria (hassle-free ingredients, saves well for leftoers, etc). I’m happy to make small chaanes to what I eat. I recently realized how well a lone, unadorned sweet potato went with some of my staples. That’s a one step addition, and I can evaluate its impact as I walk the aisles of the store. Sure enough, it quickly became a staple.&lt;/li&gt;
&lt;li&gt;However, I’m sure there are many full dishes out there that would be just as good as my current rotation. But to find a new dish is a risk. There’s no way for me to see all the the links in the chain. Will the ingredients be easy to find? Will I find the cooking burdensome? What will the end result taste like? Are my cooking supplies well suited to the task?&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- * We provide some new rules of the game. We know not only the value at these points, we know the shape/slope/etc of the area around it. This is a reasonable relaxation, because we can estimate this shape by querying our computer for the values of the areas near our point (are we going up or down). The rule is that we can&#39;t look far away to see where the peak is (perhaps we&#39;re stuck in the Appalachian forests, and not the sheer granite of the Sierra Nevada).  --&gt;
&lt;!-- * This isn&#39;t arbitrary, it&#39;s a natural understanding of how we actually operate. We are inherently *local* optimizers. We can easily assess the impact of small changes, not not large ones. --&gt;
&lt;!-- * On a morning commute, we can estimate whether taking a turn a few blocks early to avoid a stoplight saves us time. It&#39;s much harder to figure out in advance whether taking the bus or train is easy. --&gt;
&lt;!-- * Now, I&#39;d wager that generally, we&#39;re rather good at making small (later, we will call these &#34;local&#34;, due to their geometric interpretation) adjustments. If I realize that by taking a right a few blocks early, I can avoid the long traffic light, I&#39;ll often do it.  --&gt;
&lt;!-- * However, there&#39;s a reason these are small changes. We can estimate their impact from where we are. If we considered an entirely new route to work, taking a whole different train line, we would have no way to know what that&#39;s like. --&gt;
&lt;p&gt;Before, we imagined a challenge where we tried to find a peak of a mountain range, using a computer which could tell us the elevation of any chosen input point. Let’s imagine that it also tells us the slope of the incline at that point (“which direction is down”). This better reflects our intuition for local optimization, but it’s also not a large change to the game, because we could always simply query the elevation in a small area around our point, and gauge the slope ourselves. Imagine that we are hiking with an altimeter, and we can look around us and see the shape of the nearby slope. However, we can’t just look around and find a faraway peak, because we’re stuck in the forests of the Appalachians rather than the sheer granite ofthe Sierra Nevada. Our intuition tells us the optimization algorithm which is natural: from wherever we are, “go up”. This intuitive algorithm is called “gradient ascent”.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;gradient-ascent&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Gradient Ascent&lt;/h1&gt;
&lt;p&gt;Here’s what gradient ascent looks like as a step-by-step process (although, its simplest summary is simply “go up”).&lt;/p&gt;
&lt;p&gt;We begin at some initial point. We find the direction of steepest ascent from that point, i.e. up the hill (the magnitude and direction of steepest ascent form the “gradient” vector). Then, we walk for a period in that direction (the amount is determined by the “step size”, and how steep the ascent actually is). Once we finish walking, we are at a new point, and we repeat this process again (we determine direction of steepest ascent, and walk in that direction). Once we reach a point that is essentially flat, there is no more direction of steepest ascent to follow, and we are done (this is called “convergence”).&lt;/p&gt;
&lt;p&gt;This is the intuitive definition of what it means to make &lt;em&gt;local&lt;/em&gt; changes. Wherever we are, we think “what small change could we make to make this better?” If our cookies are too salty, we add a bit less salt, but we don’t start over from scratch.&lt;/p&gt;
&lt;div id=&#34;mathematical-definition&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Mathematical Definition&lt;/h2&gt;
&lt;!-- Some explanation of continuous vs discrete space would be nice? --&gt;
&lt;p&gt;Imagine we are at some point &lt;span class=&#34;math inline&#34;&gt;\(x \in \mathbb{R}^d\)&lt;/span&gt; (meaning, &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is a point in &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;-dimensional space).
(Maybe in English first?)&lt;/p&gt;
&lt;p&gt;Here’s a mathematical definition of a crude version of gradient descent.&lt;a href=&#34;#fn6&#34; class=&#34;footnote-ref&#34; id=&#34;fnref6&#34;&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(f: \mathbb{R}^d \to \mathbb{R}\)&lt;/span&gt; be our objective function. Let &lt;span class=&#34;math inline&#34;&gt;\(\eta &amp;gt; 0\)&lt;/span&gt; be our scaling constant. We begin at some initial point &lt;span class=&#34;math inline&#34;&gt;\(x^{(0)} \in \mathbb{R}^d\)&lt;/span&gt;. For &lt;span class=&#34;math inline&#34;&gt;\(k = 0, \ldots,\)&lt;/span&gt;, until convergence, repeat steps 2 through 4(?).&lt;/li&gt;
&lt;li&gt;Compute the gradient &lt;span class=&#34;math inline&#34;&gt;\(\nabla f(x^{(k)})\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Set &lt;span class=&#34;math inline&#34;&gt;\(x^{(k+1)} \leftarrow x^{(k)} + \eta \nabla f(x^{(k)})\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;If &lt;span class=&#34;math inline&#34;&gt;\(\nabla f(x^{(k)})\)&lt;/span&gt; is sufficiently small, halt the algorithm, and select &lt;span class=&#34;math inline&#34;&gt;\(x^{(k+1)}\)&lt;/span&gt; as our optima. Otherwise, set &lt;span class=&#34;math inline&#34;&gt;\(k \leftarrow k+1\)&lt;/span&gt;, and repeat steps 2-4.&lt;/li&gt;
&lt;/ol&gt;
&lt;!-- http://timharford.com/2016/10/big-decision-ahead-just-roll-the-dice/ --&gt;
&lt;/div&gt;
&lt;div id=&#34;visualizing-gradient-ascent&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Visualizing Gradient Ascent&lt;/h2&gt;
&lt;p&gt;So, we follow the direction of steepest ascent, but what does that look like in practice? Our mountain range elevation challenge&lt;a href=&#34;#fn7&#34; class=&#34;footnote-ref&#34; id=&#34;fnref7&#34;&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt; provides a natural visualization. Imagine we are dealing with the simplest of all mountain ranges: a single hill. This one is perfectly round, for simplicitly, but we pretend we don’t know that, and we can only see the local area around us.
&lt;!-- Need to go through and do better labels/etc for all these visualizations!  --&gt;s
&lt;!-- (Show the unimodel 2d case ) --&gt;&lt;/p&gt;
&lt;p&gt;The following is a contour plot, like what you find on a topographical map.&lt;/p&gt;
&lt;p&gt;It might be more clear if we consider viewing the map at an angle (thanks to the &lt;code&gt;rayshader&lt;/code&gt; &lt;a href=&#34;https://www.tylermw.com/3d-ggplots-with-rayshader/&#34;&gt;package&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;uni_vis_3d.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Imagine we start at that red dot in the top right. We look around, and see that the direction of steepest ascent is down and to the left.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;eta.temp &amp;lt;- 30
grad.first.step &amp;lt;- GradBivarNormal(x.uni.init)
# Manually compute second step, for visualization
x.uni.2 &amp;lt;- x.uni.init + eta.temp*grad.first.step

g.uni + uni.first.point + 
  geom_segment(aes(x = x.uni.init[1], y = x.uni.init[1],
                   xend = x.uni.2[1], yend = x.uni.2[2]),
               col = &amp;quot;red&amp;quot;, size = .25,
               arrow = arrow(length = unit(.2, &amp;quot;cm&amp;quot;), type = &amp;quot;closed&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-27-local-optimization.en/index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We move that distance up the slope, and look around once again. We see that the direction of steepest ascent is once again, down and to the left, and we move once more.&lt;/p&gt;
&lt;p&gt;We can again try and picture this movement in 3D (which is admittedly hard to draw…).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;uni_vis_3d_step2.png&#34; /&gt;
We repeat this process until we look around for that direction of steepest ascent, and see that we’re at a point which is basically flat. We can picture the whole process in the gif below. We start in the top right, and climb and climb until we reach the top of the mountain, where we stop.&lt;/p&gt;
&lt;!-- ```{r, include = FALSE} --&gt;
&lt;!-- g.uni &lt;- tb.uni %&gt;%  --&gt;
&lt;!--   ggplot(aes(x = x, y = y)) + --&gt;
&lt;!--   geom_tile(aes(fill = z)) + --&gt;
&lt;!--   geom_contour(aes(z = z), bins = 15, color = &#34;black&#34;) + --&gt;
&lt;!--   scale_fill_gradientn(&#34;z&#34;, colours = terrain.colors(10)) + --&gt;
&lt;!--   coord_fixed() --&gt;
&lt;!-- g.uni --&gt;
&lt;!-- ``` --&gt;
&lt;p&gt;&lt;img src=&#34;2d_uni_grad_ascent_anim.gif&#34; /&gt;
&lt;!-- THIS angle isn&#39;t very good, will have to adjust --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;not-all-mountains-are-friendly&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Not All Mountains Are Friendly&lt;/h1&gt;
&lt;p&gt;In this simple example, our local minimization strategy (“just go up!”) finds the highest peak without any trouble. However, not all mountain ranges are quite so friendly. Imagine there were three different peaks, of differing heights. It’s a bit harder to picture, but there’s a 3D visualization after the contour plot to help.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Basic plot of the setup
g.mix &amp;lt;- tb.grid %&amp;gt;% 
  ggplot(aes(x = x, y = y)) +
  geom_tile(aes(fill = z)) +
  geom_contour(aes(z = z), bins = 15, color = &amp;quot;black&amp;quot;) +
  ylim(-7.5,7.5) + xlim(-7.5, 7.5) +
  xlab(&amp;quot;East/West&amp;quot;) + ylab(&amp;quot;North/South&amp;quot;) +  
  scale_fill_gradientn(&amp;quot;Elevation&amp;quot;, colours = terrain.colors(10)) +
  coord_fixed()
g.mix&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 17500 rows containing non-finite values (stat_contour).&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 17500 rows containing missing values (geom_tile).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-27-local-optimization.en/index_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;mix_vis_3d.png&#34; /&gt;&lt;/p&gt;
&lt;!-- * 3d view, to help get a sense of it. --&gt;
&lt;!-- ```{r, include = FALSE} --&gt;
&lt;!-- #par(mfrow = c(1, 2)) --&gt;
&lt;!-- # plot_gg(g, width = 7, height = 4, raytrace = FALSE, preview = TRUE) --&gt;
&lt;!-- # plot_gg(g, multicore = TRUE, raytrace = TRUE, width = 7, height = 4,  --&gt;
&lt;!-- #         scale = 300, windowsize = c(1400, 866),  --&gt;
&lt;!-- #         zoom = 0.6, phi = 30, theta = 30) --&gt;
&lt;!-- # plot_gg(g,  --&gt;
&lt;!-- #         raytrace = FALSE,  --&gt;
&lt;!-- #         preview = TRUE) --&gt;
&lt;!-- # plot_gg(g,  --&gt;
&lt;!-- #         multicore = TRUE,  --&gt;
&lt;!-- #         raytrace = TRUE, --&gt;
&lt;!-- #         zoom = 0.6,  --&gt;
&lt;!-- #         phi = 30,  --&gt;
&lt;!-- #         theta = 30) --&gt;
&lt;!-- # render_camera(zoom=0.5,theta=-30,phi=30) --&gt;
&lt;!-- # render_snapshot(clear = TRUE) --&gt;
&lt;!-- # Sys.sleep(0.2) --&gt;
&lt;!-- # render_snapshot(clear = TRUE) --&gt;
&lt;!-- ``` --&gt;
&lt;p&gt;This poses an obvious problem for our algorithm. We hike up until we reach the top of a peak, look around, see that we can’t go up any further, and stop. But this could be true of &lt;em&gt;any&lt;/em&gt; of the three peaks, and only the peak in the southeast is actually highest.&lt;/p&gt;
&lt;p&gt;Let’s say we start near the center of this region, and follow our algorithm. We climb up south and to the east, and reach that highest peak, same as we did in the case of a single hill.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;steps &amp;lt;- 130
x.asc.seq &amp;lt;- matrix(rep(NA, 2*steps), ncol=2)
# This is the starting position which takes us to the optima
x.asc.seq[1,] &amp;lt;- c(1, -0.5)
grad.seq &amp;lt;- matrix(rep(NA, 2*steps), ncol=2)
eta &amp;lt;- 20


for (i in 1:(steps-1)) {
  grad.seq[i,] &amp;lt;- GradMixtureNormal(x.asc.seq[i,])
  x.asc.seq[i+1,] &amp;lt;-  x.asc.seq[i,] + grad.seq[i,]*eta
}

tb.mix.asc &amp;lt;- tibble(x = x.asc.seq[,1], 
                     y = x.asc.seq[,2],
                     grad.x = grad.seq[,1],
                     grad.y = grad.seq[,2],
                     iter = 1:steps) %&amp;gt;% 
  mutate(x.next = x + grad.x*eta,
         y.next = y + grad.y*eta)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2d_mix_grad_ascent_optima_anim.gif&#34; /&gt;&lt;/p&gt;
&lt;p&gt;However, what if our starting position was just a bit further north? Well, we’d head off in the opposite direction, and end up at the northmost peak. We’d halt at the top, unable to climb any further, and be forever stuck at a lower point than the peak in the southeast. And by the rules of our game, we can only see the area around us, and we’ll never know that there’s a higher peak elsewhere. And most crucially, we couldn’t possibly tell the difference between the two starting points, without prior knowledge of the shape of the region.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;2d_mix_grad_ascent_anim.gif&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;the-perils-of-nonconvexity&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Perils of “Nonconvexity”&lt;/h2&gt;
&lt;p&gt;This simplified example illustrates a fundamental divide in mathematical optimization. If a shape is “convex”, we can easily compute its optima. If it is “nonconvex”, then it may be extremely difficult. Intuitively, we call a region “convex” if the line between any two points within that region stays &lt;em&gt;within&lt;/em&gt; that region. Thus, the rectangular Wyoming is convex, while Cape Cod makes Massachussetts nonconvex. Then, we call a &lt;em&gt;function&lt;/em&gt; “convex” if the region that lies above the surface of the function forms a convex set.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Which optimization challenges are hard? Math draws a fairly simple distinction. &lt;a href=&#34;#fn8&#34; class=&#34;footnote-ref&#34; id=&#34;fnref8&#34;&gt;&lt;sup&gt;8&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In short, in the convex realm, local information provides global guidance. The local slope is pointing us in a direction&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Description of convex vs non-convex optimization, why they are two fundamentally different challenges.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;embracing-random-noise&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Embracing Random Noise&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;The explanation of the noise step.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;sgd-and-neural-nets&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;SGD, and Neural Nets&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;We have seen randomization introduced quite literally, by a random movement. But randomization is often introduced less explicitly into popular algorithms.&lt;/li&gt;
&lt;li&gt;Neural networks are the much heralded staple of the rise of machine learning. Their optimization, perhaps consistent of millions or billions of dimensions (we considered a &lt;em&gt;single&lt;/em&gt; dimension for visualization purposes above)&lt;/li&gt;
&lt;li&gt;Microsoft boasted of a neural network with &lt;a href=&#34;https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/&#34;&gt;17 billion(!)&lt;/a&gt; parameters, each of which are a dimension to be optimized.&lt;/li&gt;
&lt;li&gt;The result is an enormous amount of potential spurious optima.&lt;/li&gt;
&lt;li&gt;What we’ve found over the past decade, SGD works “unreasonably well”. It’s explicitly an algorithm for convex optimization, and yet it optimizes these deep networks.&lt;/li&gt;
&lt;li&gt;There are a variety of complex answers, depending on who you ask, but the staple reason which dates back &lt;a href=&#34;https://leon.bottou.org/publications/pdf/nimes-1991.pdf&#34;&gt;decades&lt;/a&gt; is that its inherent noisiness allows it to escape spurious local optima.&lt;a href=&#34;#fn9&#34; class=&#34;footnote-ref&#34; id=&#34;fnref9&#34;&gt;&lt;sup&gt;9&lt;/sup&gt;&lt;/a&gt; The noisiness comes not fro our explicit introduction, but from the its approximation error.&lt;/li&gt;
&lt;li&gt;The noise of SGD has little analogy in our own lives, but the way that noise becomes a tool, not a hindrance, in non-convex optimization, is very real.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;SGD is the staple way to optimize neural networks.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;simulated-annealingthe-other-random-optimization-approaches&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Simulated Annealing/the other random optimization approaches?&lt;/h1&gt;
&lt;/div&gt;
&lt;div id=&#34;wrapping-up&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Wrapping Up&lt;/h1&gt;
&lt;p&gt;Of course, this is &lt;em&gt;hard&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;A pointed self help message would say to fine and embrace randomness. But with no such agenda, I can say that’s true, but also incredibly hard to get right&lt;/p&gt;
&lt;p&gt;just look at the gif! this is all powerful theory, but hard to chose the right way to introduce noise. in a clean, mathematical scenario, we can study the sorts of noise that are optimal, without any cost. when it comes to our own personal life, we experience this noise as disruption.&lt;/p&gt;
&lt;p&gt;The famous computer scientist Alan Perlis said “Optimization hinders evolution”, and it’s true. This is the fundamental “explore vs exploit” trade-off. We could spend our energy searching for better strategies (“explore”), or we could focus on deriving the benefit from the currently known optimal strategy (“exploit”). In almost every application, there is a trade-off, and it’s very hard to get right. I think our status quo bias and inherently “local” perspective means we’ll typically err on the side of spending too little time exploring, but of course it’s hard to know how to do that.&lt;/p&gt;
&lt;p&gt;But the takeaway could be something as simple as welcoming disruption, rather than resisting it. There’s no point in getting mad at a change to your commute, the world won’t notice (cite quote?), but maybe you can use the simple platitude that this is the necessary phase of exploration.&lt;/p&gt;
&lt;p&gt;Scrambled eggs are a wonderful way to start my morning. I doubt there’s much more room to optimize the way I enjoy them. But I’d be foolish to think that there weren’t wildly different options that were just as rewarding. [rw]&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;test_grad_ascent_anim.gif&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;test_noisy_ascent_anim.gif&#34; /&gt;&lt;/p&gt;
&lt;!-- ```{r, include = FALSE} --&gt;
&lt;!-- # f &lt;- function(x) { --&gt;
&lt;!-- #   (-x^2 + 3*x -2)*(x^2 -.5*x + 6) --&gt;
&lt;!-- # } --&gt;
&lt;!-- # x.seq &lt;- seq(-10, 10, .1) --&gt;
&lt;!-- # plot(x.seq, f(x.seq)) --&gt;
&lt;!-- ``` --&gt;
&lt;/div&gt;
&lt;div id=&#34;sources&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Sources&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://timharford.com/books/messy/&#34;&gt;“Messy”&lt;/a&gt;, by Tim Harford.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://users.ox.ac.uk/~econ0360/FerdinandRauch/Tube.pdf&#34;&gt;“The Benefits of Forced Experimentation: Striking
Evidence from the London Underground Network”&lt;/a&gt;, by Larcom, Rauch, &amp;amp; Willems (2017).&lt;/li&gt;
&lt;li&gt;Short &lt;a href=&#34;http://cep.lse.ac.uk/pubs/download/cp455.pdf&#34;&gt;write-up&lt;/a&gt; of the London Tube Strike research.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46507.pdf&#34;&gt;“Bayesian Optimization for a Better Dessert”&lt;/a&gt;, by Kochanski et al. (2017).&lt;/li&gt;
&lt;li&gt;The &lt;a href=&#34;https://www.rayshader.com/&#34;&gt;&lt;code&gt;rayshader&lt;/code&gt; package&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Albeit, this seems much harder to rigorously prove, than to simply cite the study as a neat thought experiment, like I’m doing here.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;I have no personal insight into the rigor of the experiment itself, it’s being used just as a framing device here.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;A rare, and extremely tenuous connection between areas of my research, and reality.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;Or you could add in other factors, like cost and pleasantness.&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;I’ve tasted the resulting cookies, not bad for a cafeteria batch.&lt;a href=&#34;#fnref5&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn6&#34;&gt;&lt;p&gt;This is for an extremely crude version with fixed step size &lt;span class=&#34;math inline&#34;&gt;\(\eta\)&lt;/span&gt;. There is no reason to use fixed step size in practice, but that adjustment isn’t relevant to the demonstration.&lt;a href=&#34;#fnref6&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn7&#34;&gt;&lt;p&gt;name this somehow?&lt;a href=&#34;#fnref7&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn8&#34;&gt;&lt;p&gt;Like maximization and minimization, convex and concave will be used interchangeably. The multiplying a convex function by minus &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; makes it concave. Concave functions are easy to maximize, but we can switch between the two interchangeably.&lt;a href=&#34;#fnref8&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn9&#34;&gt;&lt;p&gt;And “saddle points”, which I am skirting past here, as they are a similar concern for this high level explanation.&lt;a href=&#34;#fnref9&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Winning a Debate: Insights from &#34;Intelligence Squared&#34;</title>
      <link>/post/winning-debate/</link>
      <pubDate>Sun, 29 Jul 2018 00:00:00 +0000</pubDate>
      <guid>/post/winning-debate/</guid>
      <description>


&lt;p&gt;How does one measure the winner of a debate? If the debate itself is the focus, it’s wholly unsatisfying to just tally up the opinions of the viewers, as that solely reflects the prior beliefs of the audience. “Intelligence Squared” is an excellent podcast which teaches policy through moderated debate. A resolution is proposed (e.g. “Affirmative Action does more harm than good”), and two teams of two experts argue the issue in a structured setting. Many other programs aim for balance with a halfhearted attempt to pay lip service to the opposing side (without truly believing it), so it’s refreshing to hear passionate and informed argument from both sides of the aisle. If one side cites a misleading statistic, their opponents are ready to actually call them out on it, penalizing the lazy rhetorical tactics that plague discourse in an echo chamber.&lt;/p&gt;
&lt;p&gt;The focus of the program is the debate itself, but some nominal winner must still be declared. This is determined using a simple formula. The audience is polled before and after the debate (For, Against, or Undecided on the issue), and the side which gains the most votes is the winner. To be clear, I do not think that this method of declaring a winner is a particularly important to the show, nor do I think any listeners actually consider the swings of audience opinion to be the final say on the matter. But it’s still interesting to consider whether or not this is a reasonable way to declare a debate winner. And more broadly, what can the results of a debate tell us about the way that people make up their minds on polarizing issues? Much has been written about the politically charged climate of the present, and the ways in which the media we consume is entrenching us in our positions, leading to static and polarized political views.&lt;/p&gt;
&lt;p&gt;It turns out that Intelligence Squared doesn’t just measure the change in support for each side before and after the debate, for events since 2012 it also measured the individual shifts between each category (i.e. “Undecided before and For after”, or “For before and Against after”). This provides a small but insightful dataset looking at the ways in which minds can change on a polarizing issue in the course of an hour.&lt;/p&gt;
&lt;div id=&#34;what-does-it-mean-to-win-a-debate&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;What does it mean to “win” a debate?&lt;/h3&gt;
&lt;p&gt;Intelligence Squared declares their nominal winner with a simple metric, measuring whether the For or Against side had the largest absolute percentage point shift in support from the pre debate to the post debate audience poll. Thus, if the For side originally had 40% support and the Against side 35% support before the debate, and after the debate the For side had 45% support and the Against side had 43% support, the 8 percentage point increase of the Against side trumps the 5 percentage point increase of the For side, and the Against side wins the night. We will refer to this method for determining a victory as the ISM (Intelligence Squared Metric).&lt;/p&gt;
&lt;p&gt;The problem with this approach stems from the fact that &lt;em&gt;there are a variety of ways that opinions could shift between the three camps&lt;/em&gt;. This is related to both its usage of absolute percentage point change (with no reference to the relative switch), as well as the presence of the undecided voters.&lt;/p&gt;
&lt;p&gt;I stumbled upon &lt;a href=&#34;https://stats.stackexchange.com/a/94742&#34;&gt;this&lt;/a&gt; excellent post by &lt;em&gt;whuber&lt;/em&gt; on StackExchange. I have the pleasure of knowing &lt;em&gt;whuber&lt;/em&gt; in person, and in characteristic fashion he gives a thorough and insightful analysis of the ways in which we can have different shifts in support (among the different groups) for the same absolute result, in ways that make determining the winner quite difficult. We cite a simple example proposed by &lt;em&gt;whuber&lt;/em&gt; in this post.&lt;/p&gt;
&lt;p&gt;Consider a situation where originally 20% are For, 60% are Against, and 20% are undecided (we will write this as a vector &lt;span class=&#34;math inline&#34;&gt;\((.2, .6, .2)\)&lt;/span&gt;). After the debate, the vector of support is &lt;span class=&#34;math inline&#34;&gt;\((.3, .4, .3)\)&lt;/span&gt;. Under the ISM, this is a clear and decisive win for the For side, as they gained 10 percentage points of support, while the Against side lost 20 percentage points. However, there are a variety of between-group opinion switches that could lead to this result. &lt;em&gt;whuber&lt;/em&gt; suggests that we write these between-group switches in a 3x3 transition matrix, where the &lt;span class=&#34;math inline&#34;&gt;\(ij\)&lt;/span&gt;th element represents the percent of the original supporters for the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;th camp before the debate are supporters of the &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;th camp after the debate (with For as 1, Against as 2, and Undecided as 3). Then, this represents a plausible transition matrix for our final result.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathbb{A} = \left(
\begin{array}{ccc}
 0.32 &amp;amp; 0.29 &amp;amp; 0.32 \\
 0.36 &amp;amp; 0.42 &amp;amp; 0.36 \\
 0.32 &amp;amp; 0.29 &amp;amp; 0.32 \\
\end{array}
\right).
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;As &lt;em&gt;whuber&lt;/em&gt; explains,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Here, 36% of the “Fors” changed to the other side while only 29% of the “Against” changed to the opposite opinion. Moreover, slightly more of the undecideds (36%) vs 32%) came out “against” rather than for. Although their numbers in this audience decreased, we have a situation (reminiscent of Simpson’s Paradox) in which the “Against” faction clearly won the debate!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We have a possible outcome where Intelligence Squared would declare the “For” side a decisive winner, but in terms of percentage shifts, the “Against” side was more persuasive at both convincing Undecideds to join their side, &lt;em&gt;and&lt;/em&gt; convincing those with a prior opinion to change their viewpoint. This clarifies the challenge inherent in determining the winner of a debate. When one side is initially quite unpopular, they have a larger population of possible voters that they can woo, which makes their proportional gains much higher in absolute terms. There’s certainly a plausible argument that absolute shifts in support should be the most important factor in determining a winner, but this does make the situation more complex. And it gets even messier in cases where the sub-group switches don’t necessarily agree. We might see one side sway a much larger proportion of the Undecided voters, while being less successful at convincing those from the other side to switch. This could leave us without a conclusive winner.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-has-this-worked-in-practice&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;How has this worked in practice?&lt;/h3&gt;
&lt;p&gt;Looking at the results of the Intelligence Squared debates in reality, we see that the majority of results are uncontroversial. and there is little doubt which side was more convincing. In total, we gather the results from 88 debates from the Intelligence Squared &lt;a href=&#34;https://www.intelligencesquaredus.org/debates&#34;&gt;website&lt;/a&gt;, with the process described &lt;a href=&#34;https://dylanpotteroconnell.github.io/debateresults2/&#34;&gt;here&lt;/a&gt;, and the unpolished scraped dataset &lt;a href=&#34;https://github.com/dylanpotteroconnell/IntelSquaredProject/blob/master/votingresultsfinal.csv&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The first winning metric we consider is the Intelligence Squared Metric (ISM), which measures shift in absolute percentage of support. We next consider the Undisputed Metric (UDM), which uses the proportional switching outlined above, and only assigns a winner when one side &lt;em&gt;both&lt;/em&gt; convinces a greater proportion of Undecided voters to join their side, as well as a greater proportion of the other side to switch their opinion. In cases when these do not agree, no winner is declared.&lt;/p&gt;
&lt;p&gt;Thus the first question is whether there are cases where the UDM declares a winner which is the reverse of the ISM, like the toy example outlined above by &lt;em&gt;whuber&lt;/em&gt;. It turns out that this is &lt;em&gt;not&lt;/em&gt; an idle concern, and in fact there have been six debates with this conflicting result.&lt;/p&gt;
&lt;!-- \begin{table}[h] --&gt;
&lt;!--     \centering --&gt;
&lt;!--     \begin{tabular}{|l|l|l|l|l|l|l|l|l|} --&gt;
&lt;!--     \hline --&gt;
&lt;!--         Title &amp; Date &amp; Pre \%: For &amp; Pre \%: Against &amp; Pre \%: Undecided &amp; Abs. \% \$$\backslash$Delta\$: For &amp; Abs. \% \$$\backslash$Delta\$: Against &amp; ISM Winner &amp; UDM Winner \\ \hline --&gt;
&lt;!--         Trigger Warning: Safe Spaces Are Dangerous &amp; 06/23/2018 &amp; 57 &amp; 25 &amp; 18 &amp; -1 &amp; 10 &amp; Against &amp; For \\ \hline --&gt;
&lt;!--         Preserve Net Neutrality: All Data is Created Equal &amp; 04/17/2018 &amp; 60 &amp; 23 &amp; 17 &amp; 0 &amp; 8 &amp; Against &amp; For \\ \hline --&gt;
&lt;!--         Legalize Assisted Suicide &amp; 11/03/2014 &amp; 63 &amp; 11 &amp; 26 &amp; 4 &amp; 11 &amp; Against &amp; For \\ \hline --&gt;
&lt;!--         Break up the Big Banks &amp; 10/16/2013 &amp; 35 &amp; 19 &amp; 46 &amp; 13 &amp; 20 &amp; Against &amp; For \\ \hline --&gt;
&lt;!--         Cutting the Pentagon&#39;s Budget is a Gift to our Enemies &amp; 06/19/2013 &amp; 20 &amp; 58 &amp; 22 &amp; 9 &amp; 8 &amp; For &amp; Against \\ \hline --&gt;
&lt;!--         The GOP must Seize the Center or Die &amp; 04/17/2013 &amp; 63 &amp; 14 &amp; 23 &amp; 2 &amp; 14 &amp; Against &amp; For \\ \hline --&gt;
&lt;!--     \end{tabular} --&gt;
&lt;!-- \end{table} --&gt;
&lt;style type=&#34;text/css&#34;&gt;
.tg  {border-collapse:collapse;border-spacing:0;border-color:#ccc;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:#ccc;color:#333;background-color:#fff;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:#ccc;color:#333;background-color:#f0f0f0;}
.tg .tg-yp4a{border-color:#333333;vertical-align:top}
.tg .tg-us36{border-color:inherit;vertical-align:top}
.tg .tg-fraq{color:#333333;border-color:#000000;vertical-align:top}
.tg .tg-dvpl{border-color:inherit;text-align:right;vertical-align:top}
&lt;/style&gt;
&lt;table class=&#34;tg&#34;&gt;
&lt;tr&gt;
&lt;th class=&#34;tg-us36&#34;&gt;
Title
&lt;/th&gt;
&lt;th class=&#34;tg-us36&#34;&gt;
Date
&lt;/th&gt;
&lt;th class=&#34;tg-yp4a&#34;&gt;
Pre %: For
&lt;/th&gt;
&lt;th class=&#34;tg-fraq&#34;&gt;
Pre %: Against
&lt;/th&gt;
&lt;th class=&#34;tg-yp4a&#34;&gt;
Pre %: Undecided
&lt;/th&gt;
&lt;th class=&#34;tg-dvpl&#34;&gt;
Abs. % &lt;span class=&#34;math inline&#34;&gt;\(\Delta\)&lt;/span&gt;: For
&lt;/th&gt;
&lt;th class=&#34;tg-dvpl&#34;&gt;
Abs. % &lt;span class=&#34;math inline&#34;&gt;\(\Delta\)&lt;/span&gt;: Against
&lt;/th&gt;
&lt;th class=&#34;tg-us36&#34;&gt;
ISM Winner
&lt;/th&gt;
&lt;th class=&#34;tg-us36&#34;&gt;
UDM Winner
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-us36&#34;&gt;
Trigger Warning: Safe Spaces Are Dangerous
&lt;/td&gt;
&lt;td class=&#34;tg-us36&#34;&gt;
06/23/2018
&lt;/td&gt;
&lt;td class=&#34;tg-yp4a&#34;&gt;
57
&lt;/td&gt;
&lt;td class=&#34;tg-fraq&#34;&gt;
25
&lt;/td&gt;
&lt;td class=&#34;tg-yp4a&#34;&gt;
18
&lt;/td&gt;
&lt;td class=&#34;tg-dvpl&#34;&gt;
-1
&lt;/td&gt;
&lt;td class=&#34;tg-dvpl&#34;&gt;
10
&lt;/td&gt;
&lt;td class=&#34;tg-us36&#34;&gt;
Against
&lt;/td&gt;
&lt;td class=&#34;tg-us36&#34;&gt;
For
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-us36&#34;&gt;
Preserve Net Neutrality: All Data is Created Equal
&lt;/td&gt;
&lt;td class=&#34;tg-us36&#34;&gt;
04/17/2018
&lt;/td&gt;
&lt;td class=&#34;tg-yp4a&#34;&gt;
60
&lt;/td&gt;
&lt;td class=&#34;tg-fraq&#34;&gt;
23
&lt;/td&gt;
&lt;td class=&#34;tg-yp4a&#34;&gt;
17
&lt;/td&gt;
&lt;td class=&#34;tg-dvpl&#34;&gt;
0
&lt;/td&gt;
&lt;td class=&#34;tg-dvpl&#34;&gt;
8
&lt;/td&gt;
&lt;td class=&#34;tg-us36&#34;&gt;
Against
&lt;/td&gt;
&lt;td class=&#34;tg-us36&#34;&gt;
For
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-us36&#34;&gt;
Legalize Assisted Suicide
&lt;/td&gt;
&lt;td class=&#34;tg-us36&#34;&gt;
11/03/2014
&lt;/td&gt;
&lt;td class=&#34;tg-yp4a&#34;&gt;
63
&lt;/td&gt;
&lt;td class=&#34;tg-fraq&#34;&gt;
11
&lt;/td&gt;
&lt;td class=&#34;tg-yp4a&#34;&gt;
26
&lt;/td&gt;
&lt;td class=&#34;tg-dvpl&#34;&gt;
4
&lt;/td&gt;
&lt;td class=&#34;tg-dvpl&#34;&gt;
11
&lt;/td&gt;
&lt;td class=&#34;tg-us36&#34;&gt;
Against
&lt;/td&gt;
&lt;td class=&#34;tg-us36&#34;&gt;
For
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-us36&#34;&gt;
Break up the Big Banks
&lt;/td&gt;
&lt;td class=&#34;tg-us36&#34;&gt;
10/16/2013
&lt;/td&gt;
&lt;td class=&#34;tg-yp4a&#34;&gt;
35
&lt;/td&gt;
&lt;td class=&#34;tg-fraq&#34;&gt;
19
&lt;/td&gt;
&lt;td class=&#34;tg-yp4a&#34;&gt;
46
&lt;/td&gt;
&lt;td class=&#34;tg-dvpl&#34;&gt;
13
&lt;/td&gt;
&lt;td class=&#34;tg-dvpl&#34;&gt;
20
&lt;/td&gt;
&lt;td class=&#34;tg-us36&#34;&gt;
Against
&lt;/td&gt;
&lt;td class=&#34;tg-us36&#34;&gt;
For
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-us36&#34;&gt;
Cutting the Pentagon’s Budget is a Gift to our Enemies
&lt;/td&gt;
&lt;td class=&#34;tg-us36&#34;&gt;
06/19/2013
&lt;/td&gt;
&lt;td class=&#34;tg-yp4a&#34;&gt;
20
&lt;/td&gt;
&lt;td class=&#34;tg-fraq&#34;&gt;
58
&lt;/td&gt;
&lt;td class=&#34;tg-yp4a&#34;&gt;
22
&lt;/td&gt;
&lt;td class=&#34;tg-dvpl&#34;&gt;
9
&lt;/td&gt;
&lt;td class=&#34;tg-dvpl&#34;&gt;
8
&lt;/td&gt;
&lt;td class=&#34;tg-us36&#34;&gt;
For
&lt;/td&gt;
&lt;td class=&#34;tg-us36&#34;&gt;
Against
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-us36&#34;&gt;
The GOP must Seize the Center or Die
&lt;/td&gt;
&lt;td class=&#34;tg-us36&#34;&gt;
04/17/2013
&lt;/td&gt;
&lt;td class=&#34;tg-yp4a&#34;&gt;
63
&lt;/td&gt;
&lt;td class=&#34;tg-fraq&#34;&gt;
14
&lt;/td&gt;
&lt;td class=&#34;tg-yp4a&#34;&gt;
23
&lt;/td&gt;
&lt;td class=&#34;tg-dvpl&#34;&gt;
2
&lt;/td&gt;
&lt;td class=&#34;tg-dvpl&#34;&gt;
14
&lt;/td&gt;
&lt;td class=&#34;tg-us36&#34;&gt;
Against
&lt;/td&gt;
&lt;td class=&#34;tg-us36&#34;&gt;
For
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;This is not surprising when the absolute percentage changes are so close (such as in “Cutting the Pentagon’s budget…”), but the other debates show some wildly divergent results. We even see an example of &lt;em&gt;whuber&lt;/em&gt;’s hypothesized situation. In the “Trigger Warnings” debate on 06/23/2018, we see the For camp lose support, the Against camp gain support, and yet we see that the For camp was declared the winner by the UDM. The transition matrix for this debate is shown here&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Trigger Warnings Debate: }\left(
\begin{array}{ccc}
0.67 &amp;amp; 0.44 &amp;amp; 0.39 \\ 
0.26 &amp;amp; 0.56 &amp;amp; 0.33 \\ 
0.07 &amp;amp; 0.00 &amp;amp; 0.28 \\ 
\end{array}
\right).
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The For camp was able to lure 39% of the Undecided voters, compared to only 33% by the Against camp. And the For side was able to convince 44% of the Against camp to change their minds, while the Against camp convinced only 26% of the For camp to swap. And yet the For camp &lt;em&gt;lost&lt;/em&gt; support, while the Against camp gained support. This can easily be explained by the initial disparity in popularity. The For side began with 57% of the vote, so while they convinced a larger proportion of Undecided and Against voters to change their view, they had a minority of the audience who were even available to sway.&lt;/p&gt;
&lt;p&gt;This is a perfect example of the issue, because it feels fundamentally wrong to declare the winner of the debate to be a side that &lt;em&gt;lost&lt;/em&gt; popularity during the course of the night. This cousin of Simpson’s Paradox defies our intuition, where we want relative and absolute changes to point in the same direction. I think there’s an argument for both sides. The issue with using the UDM (with subgroup proportional shifts) is that there’s an inherent expectation that a debate will shift opinions towards the center. If one group starts overwhelmingly popular, and both sides sway the proportional amount of people to their side, then that side will tend to lose share. It doesn’t make much sense that we would expect any evenly matched debate to bring public opinion towards a 50/50 split, but a series of dead even debates (as measured by the UDM) would push the audience towards that equilibrium point. And yet, we think of the goal of debates as being one of persuasion. And so it makes little sense to declare one side the winner because they had a larger possible audience to persuade, even if their marginal rate of persuasion was inferior.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;are-there-obvious-issues-in-the-original-metric&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Are there obvious issues in the original metric?&lt;/h3&gt;
&lt;p&gt;There is no singular way to measure the efficacy of these metrics, and it ultimately comes down to our subjective intuition about what it means to “win” a debate. However, we can reasonably agree on some criteria that we intuitively find “fair”. Generally, we want a debate to be a plausibly even playing field from the start. An even more intuitive metric would simply be the popularity of each motion after the debate (which we refer to as the Final Popularity Metric [FPM]), but we reject this approach because it so obviously favors the side that is initially popular with the audience. It’s then worth it to consider whether the ISM is biased in ways that clash with our intuition.&lt;/p&gt;
&lt;p&gt;The most obvious plausible bias is that of the initial popularity of each position. In fact, this is why we do not simply use the FPM , so it is perhaps instructive to consider what such bias looks like. We plot the initial popularity of the For and Against positions below, and use color to denote the final winner by the FPM (i.e. which position was ultimately more popular with the audience). Exactly as we’d expect, there is a heavy bias to the initial starting positions. There are a few instances where a particularly large swing allows for the initially unfavorable position to pull out a win, but generally the winner is determined simply by initial popularity, and the processs of the debate itself is largely irrelevant.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;IQ2_FPresults.png&#34; /&gt;&lt;/p&gt;
&lt;!-- ![OriginalResults]({{ &#34;/images/IQ2_FPresults.png&#34;}}) --&gt;
&lt;p&gt;When we consider the same plot colored by the standard ISM results, we see a much more promising picture. On first glance, this adjustment does a plausible job of removing the bias from initial popularity. We theorized that the ISM might be punishing to sides that begin with overwhelming popularity, becuase they have so many fewer people that they can convert to their side (and an increase from 90 to 95 percent popularity might only be 5 percentage points, but represents a massive 50% decrease in those who don’t support that side). However, in practice, most positions start at a popularity between 20% and 50%, so those extreme examples just aren’t as concerning. And within that range, the naked eye doesn’t detect any overwhelming patterns of bias.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;IQ2_winnerorigresults.png&#34; /&gt;&lt;/p&gt;
&lt;!-- ![OriginalResults]({{ &#34;/images/IQ2_winnerorigresults.png&#34;}}) --&gt;
&lt;p&gt;One plausible trend that might stand out lies in the bottom right of the chart, where we do note that debates with an initially extremely high For share tended to be won by the Against side. Of the seven debates where the For side began with at least 60% vote share, the Against side won six contests and For won only one. This is a very small sample, but it does fit our intuition, and thus the lack of bias might simply stem from the fact that it is only observed in the extreme starting positions, which occur rarely. We do note that the reverse extreme (where the Against side has initally dominant popularity) does not show any such obvious trend, which makes this slightly less troubling.&lt;/p&gt;
&lt;p&gt;The prior plot only considers the binary outcome under the ISM, but we are not solely concerned with the effect on the binary result, but rather the effect on percentage point change, which can manifest in a shift of binary result. Thus, it must be more illsutrative to break down the effect further. We create 4 bins of debates based on a partition of the initial popularity of the For camp. For each bin, we consider a box plot of the percentage change that the debate creates for the For camp along with the percentage change for the Against camp, which provides a more precise analysis. As before, the picture is muddled until we consider the final bin (debates where the For side enjoyed a 55% or more share of the support), where almost all of the changes to the For side were lesser than the changes for the Against side. This is a much more persuasively troubling result than the simple results observed above, but the sample remains small (only 11).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;IQ2_partitionvotechange.png&#34; /&gt;&lt;/p&gt;
&lt;!-- ![PartitionVoteShare]({{ &#34;/images/IQ2_partitionvotechange.png&#34;}}) --&gt;
&lt;p&gt;We observe a peculiar result, but are unsure whether it could simply result from random chance. This question of statistical significance is at the heart of the study of statistics. However, it is not immediately clear to me which tool is suited to the situation. A simple comparison of the mean changes of these different bins seems deeply problematic. It is unclear that we can make any plausible assumptions about the distribution of the changes of votes. Crucially, the changes to the For vote share are &lt;em&gt;not&lt;/em&gt; independent to the changes to the Against vote share. And there is a clear confounding variable, where change to For vote share is going to be effected not simply by initial For vote, but by the initial number of undecideds (we certainly have not proven this, but common sense says that an undecided voter is easier to persuade than one who believes in the opposing viewpoint). More broadly, any such test from here is problematic in the way that Exploratory Data Analysis tends to be. We have not rigorously predefined our research question based on a scientific hypothesis, we are simply confirming our vague intuition that starting vote share introduces bias. Thus, a test based on this particular binning creates some form of bias. If we explored deep enough into the data, eventualy we must have found something similarly intriguing.&lt;/p&gt;
&lt;p&gt;We can use a nonparametric test to examine the implausibility of the observed trend, but we must keep the bias mentioned above closely in mind. Exploratory data analysis minimizes this problem by relying on standardized statistical tests which answer broad questions. A nonparametric test that examines the statistical implausibility of the difference observed in the arbitrary binning we have chosen above is far less robust. An example statistical test is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test&#34;&gt;Kruskal-Wallis Rank test&lt;/a&gt;. This nonparametric test simply considers the ranked values of the observed data, and considers the statistical probability that we might observe such an extreme disparity as what occurs in our sample. This provides a statistically rigorous result without reliance on assumptions, but the issue is that it provides such an answer to a deeply narrow &lt;em&gt;question&lt;/em&gt;. In our case, we could consider the Kruskal-Wallis test with the changes to the For vote for two groups: those with initial share of 55+%, and those with lesser initial share. In this case, we observe a test statistic of 8.1929, and a p-value of 0.42%. Phrased in statistical language, this means that if our two groups generated changes to the For vote share with the same distribution, there would be less than a one in two hundred chance that we observe at least as large a disaparity in the ranks of the observed values.&lt;/p&gt;
&lt;p&gt;The problem is that the question this answers is deceptively narrow. It cannot make any bold claims about &lt;em&gt;any&lt;/em&gt; debates that begin with 55+% vote share or fewer, but only the ones which we observe. If we had an ideal sample of such debates, and considered those above and below this threshold, then such a statement would be extremely powerful. Instead, our initial sample size of 11 debates means it is quite plausible that there are other fundamental differences in these samples.&lt;/p&gt;
&lt;p&gt;In short, the Kruskal-Wallis test demonstrates the statistical significance in the difference of the distribution of these samples, but makes no claim as to why. And we have little conclusive proof that it is the prior vote share of the For side that creates this disparity, it could be any number of differences between the populations that provides the root cause.&lt;/p&gt;
&lt;!--
We can stick to a nonparametric test to sidestep of our lack of reasonable assumptions, but we must keep the above concerns closely in mind during this work. This will provide a useful benchmark for the improbability of the observed result, but there will be subjective choices made in the construction of our test (as there is no singular way to formulate our hypothesis). We must simply make reasonable choices, and keep this issue in mind (an issue at the heart of any statistical test conducted after exploratory data analysis, albeit one that is significantly less concerning when the test being used is cookie cutter and not subjectively defined by the examiner). Such 
--&gt;
&lt;p&gt;This is an obviously unsatisfying result, but in the name of intellectual honesty I hesitate to any bolder claim. Qualitatively, I do think that the sum of this analysis makes it very likely that our intuition that those with an initially high For vote share experience notable disadvantage for winning the debate under the metric used by Intelligence Squared. However, these extreme cases come up very rarely, and the ISM seems quite defensible for the purposes of the podcast.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;alternative-metrics&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Alternative Metrics&lt;/h3&gt;
&lt;p&gt;It is of course interesting to consider other metrics that one could use. Outlined above, we consider the Undisputed Metric (UDM), which instead considers the percentage that switch between each camp during the debate. The UDM declares a winner if that side converts both a greater percentage of the other side to swap, as well a greater percentage of the Undecided camp to join theirs. In cases where there is disagreement (e.g. the For side convinces a greater proportion of Against voters to join their side than vice versa, but the Against side convinces a greater proportion of Undecided voters), the UDM simply declares that there is no clear winner. The UDM operates under a fundamentally different mindset: that what matters is not the percentage point change, but the proportion of voters who you sway. As noted above, it is perfectly possible for the UDM to declare the opposite winner to the ISM (this is particularly likely to happen in those cases where one side has initially extreme vote share). We first consider another plot of the debate result under the UDM compared to the initial For and Against vote shares.&lt;/p&gt;
&lt;!-- ![UDM Box Plot](IQ2_winnerUDMresults.png) --&gt;
&lt;p&gt;&lt;img src=&#34;IQ2_winnerUDMresults.png&#34; /&gt;&lt;/p&gt;
&lt;!-- ![]({{ &#34;/images/IQ2_winnerUDMresults.png&#34;}}) --&gt;
&lt;p&gt;Similar to the ISM, under the UDM it seems the initial popularities are overly determinative of the final result. But we do observe something of a reverse trend: if an initial side has at least 50% share of the starting vote, now they nearly always win. We again break down this effect by considering the voting shifts for debates binned by their initial For vote share. Now, our object of interest is not simply the percentage point change, but a variety of changes (in this plot, we consider the proportion of initially Against voters who shift to For, Undecided persuaded to join the For camp, and initially For voters who defect to the against side). Note that the coloring is done using our initial For popularity partition, unlike above.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;IQ2_UDMboxplots.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This reflects the effects expected from our initial results. Initially high For popularity tends to imply that they will persuade a higher proportion of Against voters, and that a lower proportion of For voters will defect to Against, while the results for Undecided voters are decidedly muddled. This is what we would expect. A high initial For vote share implies a lower Against vote share, so converting a small number of Against voters shows a lower percentage point change, but a large proportional change. The movement of undecided voters does not follow this pattern.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Summary&lt;/h3&gt;
&lt;p&gt;The question of which side is the winner in a debate is far from straightforward. Our intuition tells us that it should depend on the performance of the debaters, and not simply reflect the initial position of the audience. This is why we do not conduct a simple exit poll on the popularity of each side, which the data shows is massively biased towards the initial popularity of the sides. Unsurprisingly, audience members have extremely strong priors on these hotly contested issues, and an hour of debate does not ovewhelm a lifetime of thought on the matter. Intelligence Squared adjusts for this bias by polling the audience before the debate, and considering the percentage point change that each side observe. We hypothesize that this may instead be biased to disfavor initially popular sides. This appears to occur in the observed data, but extreme initial popularity is relatively rare, so it is hard to precisely prove the magnitude of this effect. In fact, the data shows that this metric seems to be a reasonably fair benchmark in practice. We outline one alternative for determining the winner, which is to consider the proportional changes among each sides. Counterintuitively, it is perfectly possible for one side in a debate to lose under the Intelligence Squared system, and yet persuade a greater proportion of both Undecideds and the other side to defect. In fact, we observe this scenario in a number of debates that occur. In practice, following this metric appears to bias results in the opposite manner, where initially popular sides are again favored (although in a similarly plausibly fair way in all but extreme cases). It is worth keeping in mind that the metric used by Intelligence Squared is but one reasonable choice, and while it serves their purposes well, it should not be considered the ultimate determination of efficacy in debate. Luckily, no one would doubt that the experience of the debate is what matters, and the mark of a winner is simply a superficial afterthought.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Scraping the &#34;Intelligence Squared&#34; Debate Results</title>
      <link>/post/scraping-is2/</link>
      <pubDate>Thu, 26 Jul 2018 00:00:00 +0000</pubDate>
      <guid>/post/scraping-is2/</guid>
      <description>&lt;p&gt;This is a brief companion to the 
&lt;a href=&#34;https://dylanpotteroconnell.github.io/debatefinalsummary/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;post&lt;/a&gt; analyzing the methods of assigning a winner to a debate, using the Intelligence Squared dataset. I will briefly outline here how I assemble that dataset, for trasparency.&lt;/p&gt;
&lt;h3 id=&#34;compiling-the-pages&#34;&gt;Compiling the Pages&lt;/h3&gt;
&lt;p&gt;The results from each Intelligence Squared debate are posted online in pages such as 
&lt;a href=&#34;https://www.intelligencesquaredus.org/debates/globalization-has-undermined-americas-working-class&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this&lt;/a&gt;, including video of the debate, a description of the major positions of each side, the qualifications of the debaters, and most importantly, the results of the audience polling. Unfortunately, there doesn’t seem to be a central hub page that neatly lists all the URLs. However, the desired dataset isn’t huge (about 90 total debates), so there’s no substitute for the occasional work simply manually trawling through the website, and recording the date, name, and URL of each debate in question.&lt;/p&gt;
&lt;h3 id=&#34;scraping-the-numbers&#34;&gt;Scraping the numbers&lt;/h3&gt;
&lt;p&gt;Once we have a full list of all the relevant URLs, luckily, the results themselves are generally presented in a consistent format. Thus,  some simple work with regular expressions gathers the data that we need. One such example.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;debate_vote_results={&amp;quot;live&amp;quot;:{&amp;quot;pre&amp;quot;:{&amp;quot;f&amp;quot;:36,&amp;quot;a&amp;quot;:45,&amp;quot;u&amp;quot;:19,&amp;quot;w&amp;quot;:&amp;quot;a&amp;quot;},&amp;quot;post&amp;quot;:{&amp;quot;f&amp;quot;:32,&amp;quot;a&amp;quot;:61,&amp;quot;u&amp;quot;:7,&amp;quot;w&amp;quot;:&amp;quot;a&amp;quot;},&amp;quot;s&amp;quot;:41,&amp;quot;t&amp;quot;:100,&amp;quot;f&amp;quot;:{&amp;quot;f&amp;quot;:18,&amp;quot;a&amp;quot;:15,&amp;quot;u&amp;quot;:3},&amp;quot;a&amp;quot;:{&amp;quot;f&amp;quot;:6,&amp;quot;a&amp;quot;:38,&amp;quot;u&amp;quot;:1},&amp;quot;u&amp;quot;:{&amp;quot;f&amp;quot;:8,&amp;quot;a&amp;quot;:8,&amp;quot;u&amp;quot;:3}},&amp;quot;online&amp;quot;:{&amp;quot;t&amp;quot;:100,&amp;quot;pre&amp;quot;:{&amp;quot;f&amp;quot;:50,&amp;quot;a&amp;quot;:35,&amp;quot;u&amp;quot;:15,&amp;quot;w&amp;quot;:&amp;quot;&amp;quot;},&amp;quot;post&amp;quot;:{&amp;quot;f&amp;quot;:44,&amp;quot;a&amp;quot;:50,&amp;quot;u&amp;quot;:6,&amp;quot;w&amp;quot;:&amp;quot;&amp;quot;},&amp;quot;f&amp;quot;:{&amp;quot;f&amp;quot;:35,&amp;quot;a&amp;quot;:13,&amp;quot;u&amp;quot;:2},&amp;quot;a&amp;quot;:{&amp;quot;f&amp;quot;:4,&amp;quot;a&amp;quot;:27,&amp;quot;u&amp;quot;:4},&amp;quot;u&amp;quot;:{&amp;quot;f&amp;quot;:6,&amp;quot;a&amp;quot;:10,&amp;quot;u&amp;quot;:0}}};
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In case anyone wants to borrow this sort of simple scrape for their own projects, you can find the code 
&lt;a href=&#34;https://github.com/dylanpotteroconnell/IntelSquaredProject/blob/master/intelsquareddata.R&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;, although the approach is extremely messy. Luckily, with R it’s more important to be fast than it is to be clean, and you can use very awkward code as long as you find it readable and clear. Regular expressions like this can grab the relevant numbers that we need, and we store it in one large data frame.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;gsub(&amp;quot;.*\&amp;quot;f\&amp;quot;:\\{\&amp;quot;f\&amp;quot;:(\\d+\\.*\\d*),\&amp;quot;a\&amp;quot;:(\\d+\\.*\\d*),\&amp;quot;u\&amp;quot;:(\\d+\\.*\\d*).*&amp;quot;, &amp;quot;\\1 \\2 \\3&amp;quot;, post)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where we identify the numbers that we’re interested in. With the numbers compiled into one large data frame (which can be viewed in raw form 
&lt;a href=&#34;https://github.com/dylanpotteroconnell/IntelSquaredProject/blob/master/votingresultsfinal.csv&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;, for those interested in examining the data themselves). In total, there are 88 debates stretching back to 2012 which have all the information needed. The program itself stretches back further, but they only began tracking the subgroup movements more recently.&lt;/p&gt;
&lt;p&gt;In the next post, we can actually dive into the data itself.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
