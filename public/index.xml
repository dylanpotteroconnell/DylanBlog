<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Dylan&#39;s Blog</title>
    <link>/</link>
      <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <description>Dylan&#39;s Blog</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sat, 01 Jun 2030 13:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Dylan&#39;s Blog</title>
      <link>/</link>
    </image>
    
    <item>
      <title>Example Page 1</title>
      <link>/courses/example/example1/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>/courses/example/example1/</guid>
      <description>&lt;p&gt;In this tutorial, I&amp;rsquo;ll share my top 10 tips for getting started with Academic:&lt;/p&gt;
&lt;h2 id=&#34;tip-1&#34;&gt;Tip 1&lt;/h2&gt;
&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
&lt;h2 id=&#34;tip-2&#34;&gt;Tip 2&lt;/h2&gt;
&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Example Page 2</title>
      <link>/courses/example/example2/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>/courses/example/example2/</guid>
      <description>&lt;p&gt;Here are some more tips for getting started with Academic:&lt;/p&gt;
&lt;h2 id=&#34;tip-3&#34;&gt;Tip 3&lt;/h2&gt;
&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
&lt;h2 id=&#34;tip-4&#34;&gt;Tip 4&lt;/h2&gt;
&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Example Talk</title>
      <link>/talk/example/</link>
      <pubDate>Sat, 01 Jun 2030 13:00:00 +0000</pubDate>
      <guid>/talk/example/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click on the &lt;strong&gt;Slides&lt;/strong&gt; button above to view the built-in slides feature.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Slides can be added in a few ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Create&lt;/strong&gt; slides using Academic&amp;rsquo;s 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Slides&lt;/em&gt;&lt;/a&gt; feature and link using &lt;code&gt;slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Upload&lt;/strong&gt; an existing slide deck to &lt;code&gt;static/&lt;/code&gt; and link using &lt;code&gt;url_slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embed&lt;/strong&gt; your slides (e.g. Google Slides) or presentation video on this page using 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;shortcodes&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Further talk details can easily be added to this page using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Perils of Overly Local Optimization</title>
      <link>/2020/05/29/local-optimization/</link>
      <pubDate>Fri, 29 May 2020 00:00:00 +0000</pubDate>
      <guid>/2020/05/29/local-optimization/</guid>
      <description>


&lt;div id=&#34;a-clockwork-egg&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A Clockwork Egg&lt;/h1&gt;
&lt;!-- (or &#34;our optimized lives&#34;) --&gt;
&lt;p&gt;I find great comfort in routine. Years of repetition have refined my morning scrambled eggs into a precise and rehearsed ritual. If I turn on the burner before I walk to the fridge, the butter will be just starting to bubble when I finish beating the eggs, and I can clean the mixing bowl in the brief pause before they’re ready to be stirred. I never set out to find the right timing, but if you do the same task a thousand times, you learn from the trial and error of random variation which tiny choices make it go just a little smoother. Anyone with a morning commute can relate to the same familiar, iterative optimization. That is, until some external shock &lt;em&gt;forces&lt;/em&gt; them to change.&lt;/p&gt;
&lt;!-- # Disruption, and the London Subway Strike --&gt;
&lt;/div&gt;
&lt;div id=&#34;a-disrupted-subway&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A Disrupted Subway&lt;/h1&gt;
&lt;p&gt;On &lt;a href=&#34;https://www.independent.co.uk/news/uk/home-news/tube-strikes-february-2014-how-they-will-affect-your-journey-and-stress-levels-9099641.html&#34;&gt;February 4th, 2014&lt;/a&gt;, the workers of the London Tube (the underground subway) went on strike, forcing a number of station closures throughout the city. For three days, some (but not all) commuters were unable to take their usual route to work, and were forced to find alternatives. After the strike, the stations reopened, and life could go back to normal.&lt;/p&gt;
&lt;p&gt;Three economists saw the opportunity for a natural experiment: only certain commuters were disrupted by the closure, and using the subway’s tracking data, their movements could be compared to the unaffected group. And surprisingly, after the strike ended, life &lt;em&gt;didn’t&lt;/em&gt; quite go back to normal. Instead, some of the disrupted group stuck with their newly discovered routes, and their commuting time decreased as a result.&lt;/p&gt;
&lt;p&gt;The station closures forced commuters to break from their finely tuned routines, but a surprising number found themselves dragged unwillingly along a route that turned out to be &lt;em&gt;faster&lt;/em&gt; than the one they had been taking for years. The researchers even claim that the time savings from these discoveries were so significant that the strike caused a net &lt;em&gt;decrease&lt;/em&gt; in time spent commuting, despite the initial disruptions.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; Even if only a small fraction found new routes, the benefits from their new routine would accrue over years to come, while discovering it only required a disruption for a few days.&lt;/p&gt;
&lt;p&gt;This neat case study provides an opportunity to frame the mathematical challenge of optimization in real world terms, and it gives particular insight into the counterintuitive fact that many optimization algorithms involve &lt;em&gt;random noise&lt;/em&gt;. I’ll begin by describing mathematical optimization in intuitive terms, and then I’ll examine how the London Tube Strike provides a real world analogy for this theoretical challenge.&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;!-- This story provides a neat way to frame the mathematical challenge of optimization, and in particular, why so many optimization algorithms involve *random noise*, a choice which might initially seem bizarre. I&#39;ll begin by describing mathematical optimization in intuitive terms, and then show how the results of the London Tube Strike provides an analogy to very real mathematical properties. --&gt;
&lt;!-- ^[I have no personal insight into the rigor of the experiment itself, it&#39;s being used just as a framing device here.] --&gt;
&lt;/div&gt;
&lt;div id=&#34;the-framework-of-optimization&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The Framework of “Optimization”&lt;/h1&gt;
&lt;p&gt;Math always starts with a definition—so, what is “mathematical optimization”? The &lt;a href=&#34;https://deepai.org/machine-learning-glossary-and-terms/mathematical-optimization&#34;&gt;DeepAI website&lt;/a&gt; writes the following.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Mathematical optimization is the process of maximizing or minimizing an objective function by finding the best available values across a set of inputs.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It’s easiest to explain these terms by example.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For London commuters, their “objective function” might be the duration of the commute,&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; and the “available values across a set of inputs” might be the route they take.&lt;/li&gt;
&lt;li&gt;If we want to find the highest peak in a mountain range (an example we’ll discuss in detail in the next section), the input would the latitude and longitude of a point on the map, and the objective function would be the elevation at that point.&lt;/li&gt;
&lt;li&gt;When baking chocolate chip cookies, the possible inputs might be the amount of each ingredient and the order in which they are combined, and the objective function might be the tastiness of the resulting cookie. This isn’t quite a joke, and Google once ran a &lt;a href=&#34;https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46507.pdf&#34;&gt;cookie optimization experiment&lt;/a&gt; along these lines.&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Optimization is a central challenge in applied mathematics and statistics, but finding solutions can be hard. The vast increase in computational power has broadened our horizons for what optimization challenges are feasible, but many important problems remain out of our reach.&lt;/p&gt;
&lt;p&gt;In the challenge of optimization, we generally assume that we are able to check the value of the objective function at some chosen input (often called “queries” to the objective function, or perhaps, the “oracle”). For example…&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We can take a certain route on the subway and use our stopwatch record the duration of the trip.&lt;/li&gt;
&lt;li&gt;We can bake a batch of cookies following a certain recipe, and taste them.&lt;/li&gt;
&lt;li&gt;We can buy ElevationBot 1.0, a rather simple computer which takes in a pair of latitude and longitude coordinates, and spits out the altitude of that point. (This example is most cleanly understood using such a computer, but to guide our intuition we’ll often imagine ourselves walking around the slopes with an altimeter in hand.)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Our naive optimization strategy might be to just try a bunch of inputs, and hope that we find a good output. For instance, we define a grid of latitude and longitude coordinates, feed these coordinates into ElevationBot 1.0, and use the highest point we find as our answer. But this naive approach only works when the range of potential inputs is sufficiently narrow. In particular, we need to consider the “dimension” of the input space. The number of points within an exhaustive “grid” of inputs grows exponentially with the dimension of the input. Latitude and longitude represent a mere 2 dimensions, so the number of points in our grid is just the square of the number of points on a side. However, the famous &lt;a href=&#34;https://cooking.nytimes.com/recipes/1015819-chocolate-chip-cookies&#34;&gt;Jacques Torres cookie recipe&lt;/a&gt; has &lt;span class=&#34;math inline&#34;&gt;\(12\)&lt;/span&gt; ingredients. If we wanted to try all combinations at a mere five different amounts of each ingredient, that would be over two hundred thousand possibilities, which is a bit much for even the hungriest baker.&lt;/p&gt;
&lt;p&gt;In our own lives, we constantly confront similar optimization challenges, but we rarely take this naive approach. Instead, we typically to look around us for small &lt;em&gt;local&lt;/em&gt; optimizations, whose benefits we can easily identify.&lt;/p&gt;
&lt;!-- * Finding the best route on the subway is something we optimize. While often algorithmic services can do the work for us, in other cases (or the past), we have to figure it out ourselves, and we somehow pick our starting point. --&gt;
&lt;!-- * For an example, let&#39;s imagine we want to find the highest point in a mountain range. We can check an individual point, and see how high it is. How do we find the highest point? The analogy would be something like, instead of latitude and longitude in a mountain range, in our morning routine it would be that &#34;series of choices for how we get to work&#34;.  --&gt;
&lt;!-- * Now, imagine instead of querying a computer for the height, you yourself were actually in that mountain range. What would you do? Well, you&#39;d look for the peak, but what if you were in a forest? You&#39;d go up. Specifically, you would look at the area around you for *local* guidance. --&gt;
&lt;/div&gt;
&lt;div id=&#34;locality-in-optimization&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Locality in Optimization&lt;/h1&gt;
&lt;p&gt;Imagine biting into a warm cookie, fresh out of the oven. The rich buttery center has just the right chew, while the edges are lightly crisp. You taste the faintest hints of toffee, butterscotch, and vanilla, and the sprinkle of sea salt on top adds an extra kick. It’s almost perfect, but there’s not &lt;em&gt;quite&lt;/em&gt; enough chocolate.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;choc_chip_cookies.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We wouldn’t just throw up our hands and start from scratch… we’d just add more chocolate! The average baker can’t look at a cookie recipe, and imagine exactly what the result will taste like from afar. But we can &lt;em&gt;taste&lt;/em&gt; a recipe, and imagine what a small specific tweak would do to the result.&lt;/p&gt;
&lt;p&gt;“Locality” in optimization refers to “closeness” in the input space. In latitude and longitude, this would be the “as the crow flies” distance, while in the input space of “cookie ingredients” and “subway commuting choices”, the definition is a bit less rigorous, but we can still imagine what constitutes a “small” change. In general, we can estimate the impacts of a &lt;em&gt;limited single change&lt;/em&gt; to the input, but not multiple big adjustments If we increase the amount of chocolate, we know they’ll taste a bit like before, but more chocolate-y. If we increase the amounts of chocolate, butter, baking soda, cut some flour, and tweak the balance between brown and cane sugar, can the average home cook really guess what the result will taste like?&lt;/p&gt;
&lt;p&gt;In the London Tube experiment, the researchers lay some of the blame on the “stylized nature” of the Tube map displayed to commuters. Its spatial distortions make it difficult to spot major inefficiencies in their route, without some external push. By comparison, we can usually estimate the impact of these “local” changes by ourselves. If we get off one stop earlier, maybe our walk to work increases by a minute, but we avoid the wait at a crowded station. These slight changes are a bit like tweaking just a single ingredient in a cookie, and we might be able to tell whether or not they are worth trying.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;london_tube_map.jpg&#34; /&gt;&lt;/p&gt;
&lt;!-- Personally, this describes my relationship with my own food. TO ADD MORE... --&gt;
&lt;!-- * In my own life, I find this with food all the time. I have a set rotation of dishes I love, that fit my criteria (hassle-free ingredients, saves well for leftoers, etc). I&#39;m happy to make small chaanes to what I eat. I recently realized how well a lone, unadorned sweet potato went with some of my staples. That&#39;s a one step addition, and I can evaluate its impact as I walk the aisles of the store. Sure enough, it quickly became a staple. --&gt;
&lt;!-- * However, I&#39;m sure there are many full dishes out there that would be just as good as my current rotation. But to find a new dish is a risk. There&#39;s no way for me to see all the the links in the chain. Will the ingredients be easy to find? Will I find the cooking burdensome? What will the end result taste like? Are my cooking supplies well suited to the task? --&gt;
&lt;!-- * We provide some new rules of the game. We know not only the value at these points, we know the shape/slope/etc of the area around it. This is a reasonable relaxation, because we can estimate this shape by querying our computer for the values of the areas near our point (are we going up or down). The rule is that we can&#39;t look far away to see where the peak is (perhaps we&#39;re stuck in the Appalachian forests, and not the sheer granite of the Sierra Nevada).  --&gt;
&lt;!-- * This isn&#39;t arbitrary, it&#39;s a natural understanding of how we actually operate. We are inherently *local* optimizers. We can easily assess the impact of small changes, not not large ones. --&gt;
&lt;!-- * On a morning commute, we can estimate whether taking a turn a few blocks early to avoid a stoplight saves us time. It&#39;s much harder to figure out in advance whether taking the bus or train is easy. --&gt;
&lt;!-- * Now, I&#39;d wager that generally, we&#39;re rather good at making small (later, we will call these &#34;local&#34;, due to their geometric interpretation) adjustments. If I realize that by taking a right a few blocks early, I can avoid the long traffic light, I&#39;ll often do it.  --&gt;
&lt;!-- * However, there&#39;s a reason these are small changes. We can estimate their impact from where we are. If we considered an entirely new route to work, taking a whole different train line, we would have no way to know what that&#39;s like. --&gt;
&lt;p&gt;Before, we imagined a challenge where we tried to find a peak of a mountain range, using ElevationBot 1.0, which could tell us the elevation of any point defined by its latitude and longitude. Now, let’s imagine that there was an upgrade to ElevationBot 2.0, which &lt;em&gt;also&lt;/em&gt; tells us the slope of the incline of this point (that is, “which direction slopes downward, and how steep is it”, often referred to as the gradient). This better reflects our intuition for local optimization in other settings, but it’s also not a terribly difficult task for the ElevationBot, as the diligent machine could always query the elevation at nearby points, and use those to form an estimate of the local slope.&lt;/p&gt;
&lt;p&gt;Now, imagine that we were hiking with an altimeter, and we can look around and see the direction of the hillside slope around us. But, our information is strictly “local”—we’re stuck in the foggy forests of the Appalachians, not the wide open granite of the Sierra Nevada, so we can’t look miles away to spy what peak might be highest. How might we find the highest peak? Well, our intuition is simple: from wherever we are, “go up”. This “algorithm” is called “gradient ascent”.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;gradient-ascent&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Gradient Ascent&lt;/h1&gt;
&lt;p&gt;Gradient ascent is a canonical optimization algorithm, and the step by step process can be described as follows (its simplest summary is simply “go up, as fast as you can”).&lt;/p&gt;
&lt;p&gt;We begin at some initial point. We find the direction of steepest ascent from that point, i.e. “if you took a few steps in any direction, which would make you gain the most elevation?” (the magnitude and direction of steepest ascent form the “gradient”, and the direction is the &lt;em&gt;opposite&lt;/em&gt; of where a marble would roll). Then, we walk for some distance in that direction (this amount is referred to as the “step size”, meaning a step of the algorithm, not a step with your feet). Once we finish walking, we look around our new location, and we repeat this process again (i.e. determine the direction of steepest ascent, and walk in that direction). Once we reach a point that is essentially flat, there is no direction we can follow which takes us any higher, and we are done (this is called “convergence”). To see this written in basic mathematical notation, see &lt;a href=&#34;#appendix-gradient-ascent-algorithm&#34;&gt;Appendix: Gradient Ascent Algorithm&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This isn’t some arbitrary example, it’s a natural definition of what it means to improve our situation through &lt;em&gt;local&lt;/em&gt; changes. Wherever we are, we think “what small change could we take to make to make this better?” If our cookies are too salty, we add a bit less salt, but we don’t start over from scratch.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;visualizing-gradient-ascent&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Visualizing Gradient Ascent&lt;/h1&gt;
&lt;p&gt;So, we follow the direction of steepest ascent, but what does that look like in practice? We can visualize our challenge of finding the highest point in a very simple mountain range. In fact, simplest of all, imagine a single hill. This one is perfectly round, but we pretend we don’t know that, and we can only see the local area around us.&lt;/p&gt;
&lt;p&gt;The following is a contour plot, like what you find on a topographical map. At the center is the highest point, and it slopes down as you get further away. Our current position is the red dot, in the southeast.&lt;/p&gt;
&lt;p&gt;It might be more clear if we consider viewing the map at an angle (thanks to the &lt;code&gt;rayshader&lt;/code&gt; &lt;a href=&#34;https://www.tylermw.com/3d-ggplots-with-rayshader/&#34;&gt;package&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;uni_vis_3d.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Let’s imagine gradient ascent in action. We look around, and see that the direction of steepest ascent is to the northwest, shown by the red arrow.&lt;/p&gt;
&lt;p&gt;We move that distance up the slope, and look around once again. We see that the direction of steepest ascent is still northwest, so we move once more.&lt;/p&gt;
&lt;p&gt;Or, shown in 3D (which is admittedly hard to draw…).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;uni_vis_3d_step2.png&#34; /&gt;
We repeat this process until we look around for that direction of steepest ascent, and see that we’re at a point which is basically flat. You can watch the whole process in the gif below. We start in the top right, and climb and climb until we reach the top of the mountain, where we stop.&lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;2d_uni_grad_ascent_anim.gif&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;not-all-mountains-are-friendly&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Not All Mountains Are Friendly&lt;/h1&gt;
&lt;p&gt;In this simple example, our local minimization strategy (“just go up!”) finds the highest peak without any trouble. However, imagine there were three different peaks, of differing heights.&lt;/p&gt;
&lt;p&gt;It’s a bit harder to picture, but a 3D visualization might help picture its shape.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;mix_vis_3d.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This poses an obvious problem for our algorithm. We hike up until we reach the top of a peak, look around, see that we can’t go up any further, and stop. But this could be true of &lt;em&gt;any&lt;/em&gt; of the three peaks, and only the peak in the southeast is actually highest.&lt;/p&gt;
&lt;p&gt;Let’s say we start near the center of this region, and follow our algorithm. We climb up south and to the east, and reach that highest peak, just like as we did in the case of a single hill.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;2d_mix_grad_ascent_optima_anim.gif&#34; /&gt;&lt;/p&gt;
&lt;p&gt;However, what if our starting position was just a bit further north? Well, in the gif below, we see that we’d head off in the opposite direction, and end up at the northmost peak. We’d halt at the top, unable to climb any further, and be forever stuck at a lower point than the peak in the southeast. And by the rules of our game, we can only see the area around us, and we’ll never know that there’s a higher peak elsewhere! And most crucially, we couldn’t possibly tell the difference between the two starting points, without prior knowledge of the area.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;2d_mix_grad_ascent_anim.gif&#34; /&gt;
This illustrates a fundamental divide in mathematical optimization. Some surfaces are “convex”, which ensures that gradient ascent will march us directly to the highest point (the “global maxima”), no matter where we begin.&lt;a href=&#34;#fn6&#34; class=&#34;footnote-ref&#34; id=&#34;fnref6&#34;&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt; However, “nonconvex” surfaces lack such guarantees, and might prove computationally intractable (any “mountain range”, by definition, falls into this category, as it has multiple peaks). Informal definitions of these terms can be found in the appendix (&lt;a href=&#34;#appendix-the-perils-of-nonconvexity&#34;&gt;Appendix: The Perils of “Nonconvexity”&lt;/a&gt;), but the most intuitive takeaway is the issue of “spurious optima”. To clarify the jargon…&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Optima: either maxima or minima (these challenges are equivalent).&lt;/li&gt;
&lt;li&gt;Global optimum (or maximum/minimum): the optimum among the &lt;em&gt;entire&lt;/em&gt; input space&lt;/li&gt;
&lt;li&gt;Local optima (or maxima/minima): these points are optima in a small neighborhood around themselves, but not if you consider points far away. There is no local direction which takes you higher (or lower, if minimizing), but none are the global optimum.&lt;/li&gt;
&lt;li&gt;In the context of optimization, local optima are also called &lt;em&gt;spurious&lt;/em&gt; optima, because they have the same local properties as the global optimum, but they are not the solution to the optimization challenge.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the convex realm, local information provides us with global guidance, and we can follow the path of steepest ascent all the way to the maxima. In the nonconvex setting, we might instead find ourselves trapped in a spurious optima.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;not-settling-for-second-best&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Not Settling for Second Best&lt;/h1&gt;
&lt;p&gt;If we are wary of getting stuck at a spurious optima, how can we modify our approach to ensure we find the highest peak?&lt;/p&gt;
&lt;p&gt;Well, we saw above, that certain starting positions were successful, and others lead us astray. What if we simply used our gradient ascent algorithm multiple times, starting from randomly chosen starting points? Then we could check the results of each of our attempts, and pick the highest point we find. The gif below shows what that might look like.&lt;a href=&#34;#fn7&#34; class=&#34;footnote-ref&#34; id=&#34;fnref7&#34;&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt; If we try a number of randomly chosen points, we are highly likely to find one that leads to the highest peak.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;2d_mix_grad_restarts_anim.gif&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In a sense, the challenge becomes one of &lt;em&gt;initialization&lt;/em&gt;. This is common in the optimization literature. Even if the optimization surface is nonconvex, we just need to find a starting point that’s reasonably close to the optima (within its “basin of attraction”), and our naive gradient ascent algorithm can do the rest. Countless papers have been written on the topic.&lt;/p&gt;
&lt;p&gt;It’s also a natural fit for our intuition—if you set out to find the perfect chocolate chip cookie, I bet you’d do something similar. You might start with a few different recipes that look promising, and then try and find small improvements based on the results (a bit less salt, or a bit more chocolate). It’s the equivalent of trying different starting points, and then following gradient ascent (albeit, random initialization would be a poor choice when we have such rich domain knowledge to lean on).&lt;/p&gt;
&lt;p&gt;Thus, the challenge boils down to how hard it is to find the right starting point. Are any of the popular cookie recipes out there reasonably close to your own personal, optimal cookie? Hard to say. A large region of the mountain range above lead to the best peak, but what if there were dozens of mountains and the tallest one was steep with a narrow base? The difficulty of optimization always depends on the unique shape of the setting.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;embracing-random-noise&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Embracing Random Noise&lt;/h1&gt;
&lt;p&gt;Or, we could view the challenge from a different perspective. Instead of trying to optimize different starting points using their local information, what if we were a bit more flexible in our climbing path, and were willing to experiment along the way?&lt;/p&gt;
&lt;p&gt;Of course, there’s a reason that gradient ascent is so rigid in its path—it always goes in the direction that gives you the largest gain in the shortest amount of time. Any other direction appears inferior. If the cookies aren’t chocolate-y enough, you’d feel a little silly trying any other fix besides adding more chocolate chips to the dough. But as we’ve seen, local improvements will only take you so far. My ideal chocolate chip cookie is thick and chewy, but imagine that I had only ever tried recipes which were thin and crispy. I could try and improve them by tinkering with the balance of butter and sugar, or adding more chocolate, which might make them taste a bit better. But it would require the simultaneous change of many ingredients to make the huge leap from one type of cookie to the other.&lt;a href=&#34;#fn8&#34; class=&#34;footnote-ref&#34; id=&#34;fnref8&#34;&gt;&lt;sup&gt;8&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;What if we were a little more open to experimentation, even if we can’t easily see how it would help? Let’s recall the example of the London subway. These commuters had settled into their preferred morning routes, and surely if they could spot any tiny tweak which would save them time, they would have already tried it. The three day strike represented an external shock to the system, which pushed them out of their local equilibrium. It wasn’t designed to improve their commute (if anything, it was the opposite). And yet, by random chance, there was a group of commuters who found themselves pushed into a surprisingly superior path, which they stuck with. Likely, that route didn’t seem faster from afar, and they would be loathe to waste their time on any route that looks inferior. Only through this forced random experimentation did they escape their local optima.&lt;/p&gt;
&lt;p&gt;What might this look like mathematically? Well, we could introduce random “noise” into our movement. Let’s imagine gradient ascent once more. From our starting point, we walk in the direction of greatest ascent, and then, &lt;em&gt;we walk a bit further in a random direction&lt;/em&gt;. This won’t feel very rewarding in the moment… we want to go “up”, we can see which direction takes us “up”, and yet, we go in a random direction? But maybe this extra experimentation will give us a chance to escape our deterministic march towards a spurious optima, and we will eventually find the highest peak. We will still &lt;em&gt;tend&lt;/em&gt; to go “up”, but we might not follow such a direct path.&lt;/p&gt;
&lt;p&gt;To be mathematically precise (again, you can just skip the mathematical part if you’d like), we simply add the random noise to our gradient ascent step from before. That is, now we set &lt;span class=&#34;math inline&#34;&gt;\(x^{(k+1)} \leftarrow x^{(k)} + \eta \nabla f(x^{(k)}) + \sigma^2 \epsilon_k\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_k \sim N(0, 1)\)&lt;/span&gt;. That is, we add a normally distributed random noise term to each gradient ascent step.&lt;/p&gt;
&lt;p&gt;To visualize this, we turn to the one dimensional case. A one-dimensional mountain range might not be quite as intuitive, but it’s a lot easier to see on your screen, and the same rules apply. Below, we consider a mountain range where the middle peak is the highest. Following gradient ascent, if we start on the far right, we quickly find the nearby peak, but are stuck there, unable to find the highest point.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;test_grad_ascent_anim.gif&#34; /&gt;&lt;/p&gt;
&lt;p&gt;However, if we start from the same place, but add a bit of noise with every step, our path isn’t quite so direct. We take a bit longer to reach the top of the low peak, but from there, the random noise gives us a chance to “escape”. That is, with each move, we feel a pull back towards the low peak (as that is the direction of ascent). But if we get lucky, and find a sequence of random noise terms that “push” us far enough to the the left, we escape this valley and feel the pull of the highest peak. The addition of random noise allows us to escape eternal mediocrity, stuck at the top of the low peak.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;test_noisy_ascent_anim.gif&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Of course, this gif takes a &lt;em&gt;long&lt;/em&gt; time to run—it requires a very lucky sequence of steps for us to escape the local optima! Thus, this is far from an easy fix. In fact, this highlights the importance of precisely tuning this approach. Blindly adding random noise will &lt;em&gt;eventually&lt;/em&gt; let you find the summit, but along the way you’ll be stuck with long stretches of experimentation before you get anywhere.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;estimation-error-as-noise&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Estimation Error as Noise&lt;/h1&gt;
&lt;p&gt;Simply adding random noise to gradient ascent isn’t a “real” algorithm that would be ever used in practice, but it helps illustrate the powerful premise of “stochastic optimization” (i.e. optimization techniques which leverage random chance). Many varieties of stochastic optimization see widespread use,&lt;a href=&#34;#fn9&#34; class=&#34;footnote-ref&#34; id=&#34;fnref9&#34;&gt;&lt;sup&gt;9&lt;/sup&gt;&lt;/a&gt; but here, I’ll focus on one particularly relevant example, Stochastic Gradient Descent (SGD).&lt;/p&gt;
&lt;p&gt;The premise of Stochastic Gradient Descent is a natural extension of the gradient ascent algorithm we studied above. First, we note again “gradient &lt;em&gt;ascent&lt;/em&gt;” and “gradient &lt;em&gt;descent&lt;/em&gt;” are equivalent—you simply take the negative of the gradient to switch between maximization and minimization. Rather, it’s the “stochastic” that’s new.&lt;a href=&#34;#fn10&#34; class=&#34;footnote-ref&#34; id=&#34;fnref10&#34;&gt;&lt;sup&gt;10&lt;/sup&gt;&lt;/a&gt; In gradient ascent, we needed to compute the slope of the local area (i.e. the gradient) with each step. Let’s consider again our ElevationBot 2.0 which takes in latitude and longitude coordinates, and tells us the elevation and local slope of that point. But perhaps we’re not the only ones trying to play this addictive “find the highest peak” game, so we have to wait in a long line before each query. However, nearby we see a rusted old ElevationBot 1.5 terminal. It’s an intermediate model, between version 1.0 (which just computes the altitude) and 2.0 (which also computes the direction of ascent), because it can only &lt;em&gt;estimate&lt;/em&gt; the direction of ascent, and it’s occasionally a bit off. However, there’s no one in line to use it, so we can rattle off our queries much faster. Should we skip the line, and use the faster but less reliable estimates of the 1.5 model?&lt;/p&gt;
&lt;p&gt;This silly story might make the setting sound overly contrived, but it’s actually a very foundational tradeoff. It is often much faster to compute an &lt;em&gt;approximate&lt;/em&gt; slope rather than an &lt;em&gt;exact&lt;/em&gt; one.&lt;a href=&#34;#fn11&#34; class=&#34;footnote-ref&#34; id=&#34;fnref11&#34;&gt;&lt;sup&gt;11&lt;/sup&gt;&lt;/a&gt; This is the premise of Stochastic Gradient Descent—the exact gradient is replaced by a stochastic gradient, which is much faster to compute but is only an approximation of the truth.&lt;/p&gt;
&lt;p&gt;And this is no niche use case. Stochastic Gradient Descent has been the canonical industry standard technique for optimizing artificial neural nets for decades. When you hear about the rise of “deep learning” in the news, Stochastic Gradient Descent is the most reliable and versatile computational tool in the field. The challenge of neural networks is that their optimization space might consist of millions of dimensions (above, we visualized optimizing over a &lt;em&gt;single&lt;/em&gt; dimension!). In fact, Microsoft recently boasted of a neural network with &lt;a href=&#34;https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/&#34;&gt;17 billion (!)&lt;/a&gt; dimensions. The inevitable result of these optimization structures is that there are an enormous number of spurious local optima.&lt;/p&gt;
&lt;p&gt;Which begs the question—if neural networks are loaded with spurious local optima, and following the gradient will often lead to those spurious points, then in the words of neural nets pioneer Geoffrey Hinton, why does SGD work so &lt;a href=&#34;https://twitter.com/samcharrington/status/917816482242990080&#34;&gt;“unreasonably well”&lt;/a&gt;?&lt;/p&gt;
&lt;p&gt;There are a variety of answers, depending on who you ask, but the most common reason researchers point to (an explanation dating back &lt;a href=&#34;https://leon.bottou.org/publications/pdf/nimes-1991.pdf&#34;&gt;decades&lt;/a&gt;), is that the inherent noise in the imprecise stochastic approximation allows it to escape spurious local optima.&lt;a href=&#34;#fn12&#34; class=&#34;footnote-ref&#34; id=&#34;fnref12&#34;&gt;&lt;sup&gt;12&lt;/sup&gt;&lt;/a&gt; This is not noise that we added explicitly, it’s just the approximation error that arises from not computing the exact gradient. And yet, these random shocks turn out to be an advantage, rather than a hindrance. Even at the computational cutting edge of the field, random noise can be a powerfully helpful force, pushing us away from spurious optima and further towards our goal.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;wrapping-up&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Wrapping Up&lt;/h1&gt;
&lt;p&gt;The informal parallels between mathematical theory and our lived experience are illustrative, but not definitive. I think there is a kernel of a genuine lesson to be found here—we tend towards overly local optimization strategies in our daily lives, and external random shocks can push us to a better solution. And yet, it’s important not to take this too far. In the world of pure mathematical theory, we can carefully study the effects of any potential noise, and calibrate it to fit the situation. In our daily lives, we can’t easily assess the impact of this noise, and it could just as easily provide a pointless disruption (indeed, recall the example of the one-dimensional noisy ascent gif, and the long circuitous route it took to reach the top!).&lt;/p&gt;
&lt;p&gt;Famed computer scientist Alan Perlis once said “Optimization hinders evolution”. In our case, this means finding a balance between the fundamental “explore vs exploit” trade-off. That is, we could spend our energy searching for better strategies (“explore”), or we could focus on deriving the benefit from the currently known optimal strategy (“exploit”). This trade-off is omnipresent in the decisions we make, and it’s very hard to get right. My instincts are that our &lt;a href=&#34;https://link.springer.com/article/10.1007/BF00055564&#34;&gt;status quo bias&lt;/a&gt; and inherently “local” perspective means we’ll typically err on the side of “too much exploitation”, but there’s no universal rule.&lt;/p&gt;
&lt;!-- Perhaps the takeaway is simply one of perspective---rather than resisting every disruption outside of our control, we should welcome it as an opportunity to explore something new. After all, there&#39;s no point getting mad at a road closure on your commute, the world won&#39;t notice.^[&#34;Why should we feel anger at the world? As if the world would notice?&#34; is often cited to Marcus Aurelius, although I can&#39;t personally find the source.]  --&gt;
&lt;!-- It&#39;s natural to be wary of the unfamiliar---more often than not, it will be a downgrade from your well rehearsed routine. But the potential upsides are enormous. All it takes is acceptance of your limited local perspective,  and a leap of faith towards that hidden, higher peak. --&gt;
&lt;!-- check whether this edit changed anything --&gt;
&lt;p&gt;Perhaps the takeaway is simply one of perspective—rather than resisting every disruption outside of our control, we should welcome it as an opportunity to explore something new. After all, there’s no point getting mad at a road closure on your commute, the world won’t notice.&lt;a href=&#34;#fn13&#34; class=&#34;footnote-ref&#34; id=&#34;fnref13&#34;&gt;&lt;sup&gt;13&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Scrambled eggs are a wonderful way to start the day, and I doubt I could do much more to optimize the way I make them to my own unique tastes. But I’d be foolish to think that there weren’t other breakfast options I might enjoy just as much, even if the first few times I make them they don’t go as smoothly as my current ritual. All it takes is accepting the limits of your local perspective, and taking a leap of faith towards that hidden higher peak.&lt;a href=&#34;#fn14&#34; class=&#34;footnote-ref&#34; id=&#34;fnref14&#34;&gt;&lt;sup&gt;14&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sources&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Sources&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://timharford.com/books/messy/&#34;&gt;“Messy”&lt;/a&gt;, by Tim Harford. Broadly, this whole post was heavily inspired by Tim Harford’s writings on the London Tube experiment. I first read his description when I was studying Langevin diffusions, and I have wanted to show some visualizations of the premise ever since. I think this post is sufficiently far from his writings to add something new (he uses the example to make a point about how external shocks pushing us out of equilibrium prompt greater creativity, whereas I’m trying to focus simply on the viewpoint on mathematical optimization), but the original idea is certainly his.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://users.ox.ac.uk/~econ0360/FerdinandRauch/Tube.pdf&#34;&gt;“The Benefits of Forced Experimentation: Striking
Evidence from the London Underground Network”&lt;/a&gt;, by Larcom, Rauch, &amp;amp; Willems (2017). Further, a short &lt;a href=&#34;http://cep.lse.ac.uk/pubs/download/cp455.pdf&#34;&gt;write-up&lt;/a&gt; of the London Tube Strike research.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46507.pdf&#34;&gt;“Bayesian Optimization for a Better Dessert”&lt;/a&gt;, by Kochanski et al. (2017).&lt;/li&gt;
&lt;li&gt;The &lt;a href=&#34;https://www.rayshader.com/&#34;&gt;&lt;code&gt;rayshader&lt;/code&gt; package&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://leon.bottou.org/publications/pdf/nimes-1991.pdf&#34;&gt;“Stochastic Gradient Learning
in Neural Networks”&lt;/a&gt;, by L&#39;eon Bottou.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;appendix-gradient-ascent-algorithm&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Appendix: Gradient Ascent Algorithm&lt;/h1&gt;
&lt;p&gt;The gradient ascent algorithm, written informally in mathematical notation.&lt;a href=&#34;#fn15&#34; class=&#34;footnote-ref&#34; id=&#34;fnref15&#34;&gt;&lt;sup&gt;15&lt;/sup&gt;&lt;/a&gt; Imagine we are at some point &lt;span class=&#34;math inline&#34;&gt;\(x \in \mathbb{R}^d\)&lt;/span&gt; (meaning, &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is a point in &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;-dimensional space).&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(f: \mathbb{R}^d \to \mathbb{R}\)&lt;/span&gt; be our objective function. Let &lt;span class=&#34;math inline&#34;&gt;\(\eta &amp;gt; 0\)&lt;/span&gt; be our step size constant. We begin at some initial point &lt;span class=&#34;math inline&#34;&gt;\(x^{(0)} \in \mathbb{R}^d\)&lt;/span&gt;. For &lt;span class=&#34;math inline&#34;&gt;\(k = 0, \ldots,\)&lt;/span&gt;, until convergence, repeat steps 2 through 4.&lt;/li&gt;
&lt;li&gt;Compute the gradient &lt;span class=&#34;math inline&#34;&gt;\(\nabla f(x^{(k)})\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Set &lt;span class=&#34;math inline&#34;&gt;\(x^{(k+1)} \leftarrow x^{(k)} + \eta \nabla f(x^{(k)})\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;If &lt;span class=&#34;math inline&#34;&gt;\(\nabla f(x^{(k)})\)&lt;/span&gt; is sufficiently small, halt the algorithm, and select &lt;span class=&#34;math inline&#34;&gt;\(x^{(k+1)}\)&lt;/span&gt; as our optima. Otherwise, set &lt;span class=&#34;math inline&#34;&gt;\(k \leftarrow k+1\)&lt;/span&gt;, and repeat steps 2-4.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;appendix-the-perils-of-nonconvexity&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Appendix: The Perils of “Nonconvexity”&lt;/h1&gt;
&lt;p&gt;Intuitively, we call a region a “convex set” if the line between any two points within that region stays &lt;em&gt;within&lt;/em&gt; that region. Thus, the rectangular Wyoming is convex, while Cape Cod makes Massachussetts nonconvex. Then, we call a &lt;em&gt;function&lt;/em&gt; “convex” if the region that lies above the surface of the function forms a “convex set”.&lt;/p&gt;
&lt;p&gt;In general, convex functions are easy to optimize, because *local information provides global guidance&amp;quot;. That is, if we follow the path upward, we know that we’ll eventually reach the true optimal value.&lt;/p&gt;
&lt;p&gt;However, one further note to make this correct. The mountain range elevation objective function we study is actually “concave”, which is just a convex function flipped upside down. In this colloquial description of optimization challenges, we use the two terms interchangeably, because they are equally easy to solve.&lt;a href=&#34;#fn16&#34; class=&#34;footnote-ref&#34; id=&#34;fnref16&#34;&gt;&lt;sup&gt;16&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;This seems much harder to rigorously prove, but this is meant simiply as an informal framing device.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;A rare, and extremely tenuous connection between areas of my research, and reality.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;Or they could add in other factors, like cost or “pleasantness”.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;I’ve tasted the resulting cookies, not bad for a cafeteria batch.&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;The above images took huge step sizes for visual clarity, this shows a more realistic sequence.&lt;a href=&#34;#fnref5&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn6&#34;&gt;&lt;p&gt;This post is informal, so we omit regularity conditions that would not add to the explanation (but technically, this guarantee would also require the gradient of the objective to be Lipschitz, and for a suitable adaptive step-size).&lt;a href=&#34;#fnref6&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn7&#34;&gt;&lt;p&gt;Only a few starting points shown, to keep the gif short.&lt;a href=&#34;#fnref7&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn8&#34;&gt;&lt;p&gt;Of course, the question is “Would a sequence of small, incrementally beneficial changes take you all the way from ‘thin and crispy’ to ‘thick and chewey’?” In mathematics, this is the matter of convexity, but in the real world, it can be very hard to say. My assumption is that this often &lt;em&gt;doesn’t&lt;/em&gt; apply in cooking. If you take the middle point between two excellent dishes, there’s no reason to think that it would be tasty! But it’s hard to say with confidence what the optimization space of chocolate chip cookies looks like. &lt;a href=&#34;#fnref8&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn9&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://science.sciencemag.org/content/220/4598/671&#34;&gt;Simulated annealing&lt;/a&gt; and &lt;a href=&#34;https://www.researchgate.net/publication/13255490_Replica_Monte_Carlo_Simulation_of_Spin-Glasses&#34;&gt;parallel tempering&lt;/a&gt; are two particular favorites of mine.&lt;a href=&#34;#fnref9&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn10&#34;&gt;&lt;p&gt;What follows is a bit of a simplification of SGD, but I think it’s true to the essence of the algorithm.&lt;a href=&#34;#fnref10&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn11&#34;&gt;&lt;p&gt;In the case of SGD, the reason this happens is that it estimates the gradient by only using a &lt;em&gt;subset&lt;/em&gt; of the whole dataset.&lt;a href=&#34;#fnref11&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn12&#34;&gt;&lt;p&gt;And “saddle points”, which I am skirting past here, as they are a similar concern for this high level explanation.&lt;a href=&#34;#fnref12&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn13&#34;&gt;&lt;p&gt;“Why should we feel anger at the world? As if the world would notice?” is often cited to Marcus Aurelius, although I can’t personally find the source.&lt;a href=&#34;#fnref13&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn14&#34;&gt;&lt;p&gt;For full transparency, a few months ago I switched to fried eggs on top of frozen vegetables as my breakfast of choice, so maybe I took this lesson to heart.&lt;a href=&#34;#fnref14&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn15&#34;&gt;&lt;p&gt;This is for an extremely crude version with fixed step size &lt;span class=&#34;math inline&#34;&gt;\(\eta\)&lt;/span&gt;, and an imprecise definition of convergence. There is no reason to use fixed step size in practice, but that adjustment isn’t relevant to the demonstration.&lt;a href=&#34;#fnref15&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn16&#34;&gt;&lt;p&gt;You can turn a concave function into a convex one by simply multiplying it by &lt;span class=&#34;math inline&#34;&gt;\(-1\)&lt;/span&gt;, and minimizing rather than optimizing.&lt;a href=&#34;#fnref16&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian Structural Time Series, and Movie Ticket Sales Sales</title>
      <link>/2020/05/28/bsts-boxofice/</link>
      <pubDate>Thu, 28 May 2020 00:00:00 +0000</pubDate>
      <guid>/2020/05/28/bsts-boxofice/</guid>
      <description>


&lt;div id=&#34;disclaimer&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Disclaimer&lt;/h1&gt;
&lt;p&gt;This is something of a rough draft, written to introduce myself to BSTS models, and not exactly ready for public presentation.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;In this post, I’ll describe my introductory exploration of Bayesian structural time series (BSTS) models, using Steve Scott’s &lt;code&gt;bsts&lt;/code&gt; &lt;a href=&#34;https://cran.r-project.org/web/packages/bsts/bsts.pdf&#34;&gt;package&lt;/a&gt;, applied to the total weekly box office gross (i.e. ticket sales) of American movie theaters.&lt;/p&gt;
&lt;p&gt;This is intended to help me think through and explain the concepts and methods, and is not intended as a case study with a clear goal. In fact, I’ll analyze the ways in which these models do not necessarily fit the data setting, which should highlight potential pitfalls for projects where a powerfully predictive end result is key. My aim is simply to talk through the concepts, start to flesh out my own understanding of a topic I know almost nothing about.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; Thus, this is geared at those who are already comfortable with statistics and R. The explanations are for my own benefit, but others might find the explanation of the BSTS models helpful (as well as my assorted musings on theg oals of interest).&lt;/p&gt;
&lt;p&gt;The outcome of this project is not a tidy and powerfully predictive model for weekly box office gross, the point is the process where we think through the limits of our inference. In fact, I purposefully do not set out a specific goal (be it long term forecasting, or interpreting the industry trends), because I want to consider how these flexible models can provide insight for many different purposes, and how that flexibility can also present subtle difficulties.&lt;/p&gt;
&lt;p&gt;In particular, we will show how a model with a reasonable short term forecast can provide a disastrously useless long term forecast, and that these are two very distinct challenges. This is just the tip of the iceberg for a much broader idea, but the distinction is not discussed enough. We will see that time series models must be built with their ultimate goal in mind, and different models may serve different purposes.&lt;/p&gt;
&lt;!-- As we&#39;ll see, the class of models are likely not a particularly apt fit to the domain in question. This doesn&#39;t mean that there&#39;s no inferential value in the final result, but it does mean we have to be very conservative about what inferences we draw. BSTS models are exactly the sort of flexible structure that can provide alluringly useful accurate fits in certain contexts, only for the model to totally fall apart when stretched too far. This result is mostly due to my choice of data set (which I picked without careful consideration for its final use), but I think it&#39;s an illustrative example, because it can feel very natural to apply these models in ways which may not be theoretically sound (and they still might provide some insight in those cases, if used with appropriate caution). Above all, this is simply an opportunity for me to dip my toes into an unfamiliar topic, and I&#39;m using this post as an excuse to articulate my own understanding of the topic, and make sure I&#39;m comfortable with the basics. --&gt;
&lt;/div&gt;
&lt;div id=&#34;the-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The Data&lt;/h1&gt;
&lt;p&gt;The response variable is the weekly total of American theater movie ticket sales, in dollars, as tallied by &lt;a href=&#34;%22https://www.boxofficemojo.com/weekly/%22&#34;&gt;Box Office Mojo&lt;/a&gt;. We consider all weeks dating from 2019 back to 1993.&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Box Office Mojo also lists a few added goodies, like “Releases”,^{The total number of distinct movies which grossed money that week.}, the name of the “Top Release”, and whether or not it is a “Long Weekend”. For this initial exploration, we ignore these covariates, and stick to the date and the weekly gross.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;weekly.tb &amp;lt;- read_csv(&amp;quot;weekly_gross.csv&amp;quot;)
select &amp;lt;- dplyr::select
weekly.tb %&amp;gt;% names()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;year&amp;quot;         &amp;quot;week&amp;quot;         &amp;quot;gross&amp;quot;        &amp;quot;top_release&amp;quot;  &amp;quot;releases&amp;quot;    
## [6] &amp;quot;long_weekend&amp;quot; &amp;quot;dates&amp;quot;        &amp;quot;start_date&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;weekly.tb &amp;lt;- weekly.tb %&amp;gt;%
  as_tibble() %&amp;gt;%
  filter(year &amp;gt;= 1993) 
weekly.tb %&amp;gt;% head(3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 8
##    year  week     gross top_release    releases long_weekend dates    start_date
##   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;             &amp;lt;dbl&amp;gt; &amp;lt;lgl&amp;gt;        &amp;lt;chr&amp;gt;    &amp;lt;date&amp;gt;    
## 1  1993     1 104886500 Aladdin              20 FALSE        Jan 1-7  1993-01-01
## 2  1993     2  78995731 A Few Good Men       23 FALSE        Jan 8-14 1993-01-08
## 3  1993     3  97233042 Aladdin              26 FALSE        Jan 15-… 1993-01-15&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;cleaning-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Cleaning the Data&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;weekly.tb &amp;lt;- weekly.tb %&amp;gt;%
  dplyr::select(start_date, gross, releases, everything())
weekly.tb %&amp;gt;%
  ggplot(aes(x = start_date, y = gross)) + 
  geom_line() + 
  xlab(&amp;quot;Year&amp;quot;) + 
  ylab(&amp;quot;Gross ($)&amp;quot;) + 
  ggtitle(&amp;quot;Weekly (American) Box Office Gross&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-28-bsts-boxofice.en/index_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;
Looking at the time series, we first note that there are enormous week-to-week spikes. As we will discuss in greater detail, this reflects the release of new blockbusters, and it reflects unobserved data (of course, we have some useful approximations, such as the fact that a major blockbuster is often released around Christmas). Intuitively, our lack of access shows that there is probably a great deal of irreducible uncertainty for us in this setting.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;inflation-adjustment&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Inflation Adjustment&lt;/h2&gt;
&lt;p&gt;Next, we note that there’s an obvious upward trend in the time series, but that doesn’t tell us much, due to the steady effects of inflation. Of course, we could simply ignore this, and rely on the model to track the effects of inflation. However, this tends to cause problems, because it conflates the trends we might care about, with those that are relatively well understood. Broadly, the interesting temporal trends will be easier to detect if we first control for the &lt;em&gt;uninteresting&lt;/em&gt; trends, like inflation. We cite the &lt;code&gt;blscrapeR&lt;/code&gt; package, and our response variable is instead the weekly gross in terms of adjusted “2019 Dollars”.&lt;/p&gt;
&lt;!-- but any time series in nominal dollars is bound  but that will be true for almost there&#39;s an obvious upward trend in our time series, just about everything has gotten more expensive in the past 30 years, due to inflation. Our first step is to work with the inflation adjusted dollar weekly gross. If this cleaning step is not taken, and a time series is fit blind to the data, you can waste your time chasing after some temporal trend that simply reflects this natural increase in prices. We cite the priceR package, which can download the relevant CPI data and do the work for us.  --&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(blscrapeR)
inf_adj &amp;lt;- inflation_adjust(2019)
weekly.tb &amp;lt;- weekly.tb %&amp;gt;% left_join(inf_adj %&amp;gt;%
                                       select(year, 
                                              adj_value) %&amp;gt;%
                                       mutate(year = as.numeric(year)),
                                     by = &amp;quot;year&amp;quot;) %&amp;gt;%
  mutate(gross_ia = gross/adj_value) %&amp;gt;%
  select(-adj_value)

weekly.tb %&amp;gt;% 
  group_by(year) %&amp;gt;%
  summarize(`Nominal` = mean(gross),
            `Inflation Adjusted` = mean(gross_ia)) %&amp;gt;%
  ungroup() %&amp;gt;%
  pivot_longer(-year, names_to = &amp;quot;Type&amp;quot;, values_to = &amp;quot;Mean Yearly Gross ($)&amp;quot;) %&amp;gt;%
  ggplot(aes(x = year)) +
  geom_line(aes(y = `Mean Yearly Gross ($)`, 
                col = Type)) + 
  xlab(&amp;quot;Year&amp;quot;) + ylab(&amp;quot;Yearly Mean Gross&amp;quot;) +
  ggtitle(&amp;quot;Yearly Mean Gross: Nominal vs Inflation Adjusted.&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-28-bsts-boxofice.en/index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- # library(priceR) --&gt;
&lt;!-- #  --&gt;
&lt;!-- # weekly.tb &lt;- weekly.tb %&gt;% --&gt;
&lt;!-- #   mutate(gross_ia = adjust_for_inflation(gross,  --&gt;
&lt;!-- #                                          year,  --&gt;
&lt;!-- #                                          country = &#34;US&#34;,  --&gt;
&lt;!-- #                                          to_date = 2019)) --&gt;
&lt;!-- ``` --&gt;
&lt;p&gt;Thus, the underlying trend is still real, and the surge in box office gross from the 90s through the early 2000s appears quite meaningful. But the natural inclusion of inflation does help reduce some of the impact of the trend, which may influence our modeling.&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-scale-of-the-response-variable&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Scale of the Response Variable&lt;/h2&gt;
&lt;p&gt;We will fit our model onto the log of the box office gross (using base 10, rather than the typical &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt;, as this has a more intuitive numerical conversion for these large decimals). Intuitively, box office cannot be negative, so if we are fitting models which assume some sort of normality, this conversion is natural. Of course, we will see that while such a conversion fixes the most glaring violation of the assumptions, that does not automatically mean that the model fits! In fact, in this exploration we will see just how difficult it is to find a tidy model for box office gross.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;weekly.tb &amp;lt;- weekly.tb %&amp;gt;% 
  mutate(log_gross_ia = log10(gross_ia),
         log_gross = log10(gross))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, we will plan to &lt;em&gt;evaluate&lt;/em&gt; the model on the regular scale, as we could imagine that “dollars” tend to mean more to any interested party than “log dollars” (a rather made-up measurement). I’m not sure if there is a rigorous choice here, surely it depends on the setting. But we are choosing the log transformation not because it captures the trend we care about, but because it seems to more plausibly fit the &lt;em&gt;model&lt;/em&gt; we plan to use. In this case, the model should be evaluated based on the target of interest.&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt; Broadly, this provides good practice with how to convert from a model fit on a certain scale, to inferences drawn on another scale.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;bayesian-structural-time-series-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Bayesian Structural Time Series Model&lt;/h1&gt;
&lt;p&gt;For this introduction, we will cite a Bayesian structural time series (BSTS) model. We can take this piece by piece. A “time series” is a sequence of data points which are indexed by time. It’s a particular category of statistics, because it fundamentally violates the core “independent and identically distributed” assumption that underpins much of statistical inference. While that renders many classical statistical procedures infeasible, the temporal relationship amongst the data is the whole point! We want to understand this evolution, which means that we must rely on a model which leverages this index on time. For notation, we write &lt;span class=&#34;math inline&#34;&gt;\(y_1, \ldots, y_T\)&lt;/span&gt; for the response variables of interest (in this case, the log gross).&lt;/p&gt;
&lt;p&gt;“Bayesian” follows its standard applied definition, as our objective is not a hypothesis test of point estimate with some desirable properties, but rather a posterior distribution on the parameters of interest (the parameters which describe our time series model). These parameters are determined by the “Structural” part of the model, whcih requires a careful definition.&lt;/p&gt;
&lt;div id=&#34;the-structural-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The “Structural” Model&lt;/h2&gt;
&lt;p&gt;A “structural time series” is a model of response data indexed by time, whose temporal relationship is defined by the interplay of two equations. First, we have the &lt;em&gt;transition&lt;/em&gt; equation, which models how a latent (i.e. unobserved state) &lt;span class=&#34;math inline&#34;&gt;\(\alpha_t\)&lt;/span&gt; evolves over time.
&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
\alpha_{t+1} &amp;amp;= T_t \alpha_t + R_t \eta_t, ~~~~~~ \eta_t \sim N(0, Q_t)\label{eqn:bsts_transition}
\end{align}\]&lt;/span&gt;
Before we explain any of these pieces, it’s best to introduce the second part, the  equation, which models how the observed response &lt;span class=&#34;math inline&#34;&gt;\(y_t\)&lt;/span&gt; is generated using the state variable, &lt;span class=&#34;math inline&#34;&gt;\(\alpha_t\)&lt;/span&gt;.
&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
y_{t} &amp;amp;= Z_t^T \alpha_t + \epsilon_t, ~~~~~~ \epsilon_t \sim N(0, H_t).\label{eqn:bsts_observation}
\end{align}\]&lt;/span&gt;
Thus, while a classic time series might model the relationship between &lt;span class=&#34;math inline&#34;&gt;\(y_t\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y_{t+1}\)&lt;/span&gt; directly, a &lt;em&gt;structural&lt;/em&gt; time series model instead links them through some latent, unobserved state, &lt;span class=&#34;math inline&#34;&gt;\(\alpha_t\)&lt;/span&gt;, which evolves underneath unseen, and influences the observed &lt;span class=&#34;math inline&#34;&gt;\(y_t\)&lt;/span&gt; values.&lt;/p&gt;
&lt;p&gt;This is a sensible setup because often time series are understood to be random observations of some underlying “state” in our system, which we cannot observe directly. If you conduct monthly opinion polling with a relatively small sample, this can readily be understood as a structural time series, because the target of interest is the underlying latent state (the public’s opinion on a particular issue), while the observations are noisy datum which are heavily influenced by that latent state. Thus, it makes sense for us to model our understanding of the latent state itself, rather than the latest noisy observation.&lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;With this understanding, the rest of the terms fall into place. &lt;span class=&#34;math inline&#34;&gt;\(Z_t\)&lt;/span&gt; is the &lt;em&gt;observation&lt;/em&gt; vector, whose coefficients define the relationship between the latent state and the response (subject to the noise, &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_t\)&lt;/span&gt;). &lt;span class=&#34;math inline&#34;&gt;\(T_t\)&lt;/span&gt; is the &lt;em&gt;state transition matrix&lt;/em&gt;, which determines the evolution of the latent state, along with the noise &lt;span class=&#34;math inline&#34;&gt;\(\eta_t\)&lt;/span&gt; (with state-diffusion matrix &lt;span class=&#34;math inline&#34;&gt;\(Q_t\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;This is the most general statement, and it covers a vast array of potential models. In particular, the allure of this structure is that these models are naturally &lt;em&gt;additive&lt;/em&gt;, which makes for easy modular construction. In this brief introduction, I won’t go into much detail about its potential.&lt;/p&gt;
&lt;p&gt;Before we proceed to fitting the data itself, a brief note about the normal errors, &lt;span class=&#34;math inline&#34;&gt;\(\eta_t\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_t\)&lt;/span&gt;. The particular computational appeal of this model is that the additive composition of these normal errors allows the posterior distributions to be normal themselves. This structure allows for the use of the Kalman filter introduction quite clear.] to compute the estimate the distributions of &lt;span class=&#34;math inline&#34;&gt;\(\alpha_t\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y_t\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The construction of the Kalman filter (and smoother) is a bit tricky to explain, so it is ommitted here,&lt;a href=&#34;#fn6&#34; class=&#34;footnote-ref&#34; id=&#34;fnref6&#34;&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt; but the important punchline is that this model requires normally distributed errors. There is no particular reason to assume that the errors of weekly box office gross should be normally distributed! We will investigate this further, but in the use of these models, often far too little attention is paid to the model specification (as we will see that we obtain some sensible results, even with a poorly fitting model).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;computation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Computation&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;bsts&lt;/code&gt; &lt;a href=&#34;https://cran.r-project.org/web/packages/bsts/bsts.pdf&#34;&gt;package&lt;/a&gt; is developed by Steve Scott, and provides a host of tools to make ABDDF. His specific approach, combining the Kalman filter with a spike-and-slab prior, was introduced in a &lt;a href=&#34;https://www.nber.org/chapters/c12995&#34;&gt;neat paper&lt;/a&gt; showing how search trends could be used to “Nowcast”.&lt;a href=&#34;#fn7&#34; class=&#34;footnote-ref&#34; id=&#34;fnref7&#34;&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt; The bsts package allows for the quick construction of common BSTS models with minimal effort, and provides a host of tools for their analysis. It’s worth noting that Tensorflow has its own package for Bayesian structural time series modeling (&lt;a href=&#34;https://www.tensorflow.org/probability/api_docs/python/tfp/sts&#34;&gt;tfp.sts&lt;/a&gt;), which provides an alternative, for those comfortable with Tensorflow, and the additional features it offers.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;our-theoretical-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Our Theoretical Model&lt;/h1&gt;
&lt;p&gt;From here, we could simply fit the time series itself, and look at the results. First, we consider the insights of Tassone &amp;amp; Rohani on the &lt;a href=&#34;http://www.unofficialgoogledatascience.com/2017/04/our-quest-for-robust-time-series.html&#34;&gt;Google Unofficial Data Science Blog&lt;/a&gt;. Their excellent post covers a range of topics regarding the application of time series, with a particular focus on robust systems that can be applied robustly at scale (that is, applied in a wide range of unknown situations, and provide reaonable outcomes). That is, at scale, you often do not have the luxury of considering the underlying mechanism of the time series. They cite the power of aggregating a wide range of models, whose fit onto individual components provides the desired robustness to unknown inputs.&lt;/p&gt;
&lt;p&gt;However, their goal is purely predictive, rather than inferential (i.e. “What happens?” not “Why it happens?”).&lt;a href=&#34;#fn8&#34; class=&#34;footnote-ref&#34; id=&#34;fnref8&#34;&gt;&lt;sup&gt;8&lt;/sup&gt;&lt;/a&gt; In our case, we have the luxury of thinking about the data setting, and considering what types of models make any sense. Thus, we begin with a discussion of the limits of our inference, before considering several of the tools available, and planning which models we can fit. As we will see, our goals are not necessarily a good fit for a BSTS model! The goal of this post is simply to introduce myself to these tools, and part of that is recognizing these limits.&lt;/p&gt;
&lt;div id=&#34;the-limits-of-prediction-and-inference&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Limits of Prediction and Inference&lt;/h2&gt;
&lt;p&gt;It’s important to start with the &lt;em&gt;limits&lt;/em&gt; of the domain. If the goal of our time series was to model the Cosmic Microwave Background noise over time, we can stop right away, because it consists of entirely irreducible uncertainty, and our model has nothing to offer. More commonly, we might not have a &lt;em&gt;firm&lt;/em&gt; theory of the data, but we have a rough idea of connections which might exist, and we wish to fit some models and see which have predictive and inferential merit.&lt;/p&gt;
&lt;p&gt;There are clear limits to our capabilities given this dataset. Intuitively, the most important factor in determining the weekly box office gross has nothing to do with the general “state of the movie business”, and it has everything to do with &lt;em&gt;what movie was just released&lt;/em&gt;. This presents a significant impediment to our modeling. We have an intuitive grasp of the theory, but it’s not something that can easily be modeled! Of course, you &lt;em&gt;can&lt;/em&gt; model the ticket sales of the biggest new release, but that’s a separate challenge, and has little to do with this time series.&lt;/p&gt;
&lt;!-- In an ideal world, every modeling challenge would aim to perfectly capture the theory of the setting. In practice, we often work in situations like these. This post aims to analyze the *limits* of such an appraoch. How &#34;far&#34; can a BSTS model get us, even when we know that it is not a perfect fit for the setting? Reading about people practicing these models online, it&#39;s common to find settings with such a theoretical mismatch. --&gt;
&lt;p&gt;Now, assuming that we don’t have prescience about the release of “Avengers: Endgame”, what are some reasonable theoretical assumptions we can make? Well, first, we might expect the continuity of which movies are in theaters to provide us with a natural form of autocorrelation. The rate of decline in a movie’s second week at the same theater is a &lt;a href=&#34;https://en.wikipedia.org/wiki/Second_weekend_in_box_office_performance&#34;&gt;widely studied&lt;/a&gt; phenomenom, and it is consistently between 40 and 60 percent. Now, there are dozens of movies contributing to the total weekly gross, so the relationship won’t be nearly so tidy, but this still provides a clear form of autocorrelation.&lt;/p&gt;
&lt;p&gt;More broadly, we might intuitively expect there to be “trends” in the movie industry. The end of the 70s saw the rise of the blockbuster, the grosses of the biggest hits has steadily risen,&lt;a href=&#34;#fn9&#34; class=&#34;footnote-ref&#34; id=&#34;fnref9&#34;&gt;&lt;sup&gt;9&lt;/sup&gt;&lt;/a&gt;, and the relative popularity of different genres is always in flux. We might attempt to capture this with some sort of local slope for the latent state.&lt;/p&gt;
&lt;p&gt;Further, we might expect there to be seasonal &lt;em&gt;weekly&lt;/em&gt; effects. Box office grosses are commonly compared to the same weeks in previous years, because of the assumption of consistent patterns. It is understood that over Christmas, many people go to the movies, whereas the theaters are comparatively dry throughout February.&lt;/p&gt;
&lt;p&gt;Finally, we could find a more nuanced understanding of the autocorrelation. By its nature, a BSTS model introduces autocorrelation, because response data which are close in time are linked by the underlying latent state. Given that we expect there to be some degree of continuity movie theaters from week to week (as movies phase in and out slowly, in addition to the aforementioned trends), this is fitting, but the point is that it is possible to be much more precise with how the autocorrelation is modeled (for instance, perhaps a sharp increase in ticket sales in one week implies a sharp drop the next, due to a single huge movie release). The point of this post is for my own education, not to actually produce the most accurate model, so I proceed with a more straightforward BSTS model here, and I will critique it if it appears to be a poor fit.&lt;a href=&#34;#fn10&#34; class=&#34;footnote-ref&#34; id=&#34;fnref10&#34;&gt;&lt;sup&gt;10&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;initial-models&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Initial Models&lt;/h1&gt;
&lt;p&gt;We begin by fitting a few starter models, which we can use to demonstrate the tools to analyze these models. While we will follow our insight above, we will see that without more care, there is little reason to think that these models are a precise fit.&lt;/p&gt;
&lt;p&gt;We can start with a local linear trend, which provides a common and flexible state space model (this is a useful starting point because of its generality, but that does not guarantee that it reflects the situation). In short, a local linear trend assumes that both the mean, the trend (i.e. the slope) of the latent state evolution are drawn from a random walk. That is, if &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; is the mean of the latent state.&lt;a href=&#34;#fn11&#34; class=&#34;footnote-ref&#34; id=&#34;fnref11&#34;&gt;&lt;sup&gt;11&lt;/sup&gt;&lt;/a&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
\mu_{t+1} = \mu_t + \delta_t + \epsilon_{\mu, t},
\end{align*}\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_{\mu,t} \sim N(0, \sigma_\mu^2)\)&lt;/span&gt; is our random walk. Then, the evolution of the slope is similarly given by
&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
\delta_{t+1} = \delta_t + \epsilon_{\delta, t},
\end{align*}\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_{\delta,t} \sim N(0, \sigma_\mu^2)\)&lt;/span&gt; is another random walk. How these terms relate to our overall transition and observation equations will be clear once they are combined.&lt;/p&gt;
&lt;p&gt;Next, we consider the “seasonal” model, that is, any fixed time cycle with &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; possible states in the cycle). For these &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; seasons, we create an &lt;span class=&#34;math inline&#34;&gt;\(S-1\)&lt;/span&gt; dimension state vector &lt;span class=&#34;math inline&#34;&gt;\(\gamma_t\)&lt;/span&gt; (as we constrain the coefficients to sum to &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;) and in essence our model performs regression using these coefficients and dummy variables corresponding to the current season for the given time &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;. Then, the first element of the season state vector &lt;span class=&#34;math inline&#34;&gt;\(\gamma_t\)&lt;/span&gt; evolves as
&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
\gamma_{t+1, 1} = -1*\sum_{s=2}^S \gamma_{t, s} + \epsilon_{\gamma},
\end{align*}\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_(\gamma) \sim N(0, \sigma_\gamma^2)\)&lt;/span&gt;.&lt;/p&gt;
&lt;!-- &#34;If predictor variables are present, the regression coefficients are fixed (as opposed to time varying, --&gt;
&lt;!-- though time varying coefficients might be added as state component). The predictors and response --&gt;
&lt;!-- in the formula are contemporaneous, so if you want lags and differences you need to put them in --&gt;
&lt;!-- the predictor matrix yourself.&#34; -bsts Docs --&gt;
&lt;p&gt;With these two commponents, we can fit our twin initial models. Our “season” model consists of just the seasonal component, with &lt;span class=&#34;math inline&#34;&gt;\(S=52\)&lt;/span&gt; allowing a regression on every single unique week of the year, and the “LLT” model adding to that a local linear trend. When just using the seasonal model, we need to add to it a static intercept term, so that the mean isn’t wildly off. While the purpose of this is more educational than anything else, it’s useful to be clear on what this could show. Comparing these models essentially takes for granted the folk wisdom that the precise week of the year influences the “state” of ticket sales. Then, comparing these models can help understand if there is some basis for tracing local trends in the movie business, or if that is just overfitting to noise.&lt;a href=&#34;#fn12&#34; class=&#34;footnote-ref&#34; id=&#34;fnref12&#34;&gt;&lt;sup&gt;12&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;bsts&lt;/code&gt; package makes fitting these models extremely easy. The “state specification” is a list of state components. We construct a seasonal state space, add a static intercept, and fit the model. This post is meant as a reminder of the underlying theory, so it won’t go into the technical details of what the model produces (which are covered in the documentation), but broadly the model object which results from &lt;code&gt;bsts()&lt;/code&gt; contains the relevant parameters and samples from the posterior distribution which we want (with the one technical caveat that it returns all iterations of its posterior sampler, so if you want to access those samples yourself, you need to remove the burn-in segment manually).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ss.season &amp;lt;- AddSeasonal(list(), 
                         weekly.tb$log_gross_ia, 
                         nseasons = 52)
ss.season.intercept &amp;lt;- AddStaticIntercept(ss.season,
                                          weekly.tb$log_gross_ia)
model.season &amp;lt;- bsts(weekly.tb$log_gross_ia, 
                     state.specification = ss.season.intercept,
                     niter = 500, 
                     ping = 0, 
                     seed = 21)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As the models are modular, to create our local linear trend state specification, we simply add the local linear trend to our already specified seasonal component.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ss.llt &amp;lt;- AddLocalLinearTrend(ss.season,
                              weekly.tb$log_gross_ia)
model.llt &amp;lt;- bsts(weekly.tb$log_gross_ia, 
                  state.specification = ss.llt,
                  niter = 500, 
                  ping = 0, 
                  seed = 21)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;comparing-the-models&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Comparing the Models&lt;/h1&gt;
&lt;p&gt;It should first be said that models cannot be analyzed in isolation. In practice, we should begin by specifying our goals, and the costs associated with poor results. Here, we sketch out some relevant tools to analyze these forecasts, recognizing that their actual application is dependent on the specific context.&lt;/p&gt;
&lt;div id=&#34;the-bsts-package-tools&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The &lt;code&gt;bsts&lt;/code&gt; Package Tools&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;bsts&lt;/code&gt; package comes preloaded with useful functions for analysis of the results. We start with &lt;code&gt;CompareBstsModels()&lt;/code&gt;, which charts the cumulative absolute error of our one-step predictions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;CompareBstsModels(list(Seasonal = model.season, 
                       LLT = model.llt))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-28-bsts-boxofice.en/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;
With a single line of code, we can see how the additional flexibility of the local linear trend does indeed make our predictions more accurate.&lt;/p&gt;
&lt;p&gt;However, if you’re unfamiliar with BSTS models, it’s not immediately clear what exactly this is measuring, and it’s important to be very precise. “Cumulative absolute error” is clear enough, but what errors is it referring to? These are the “one-step-ahead” prediction errors, where the data points are drawn from the posterior computed by the Kalman filter. Unfortunately, we can’t be too rigorous about this description without going into the theoretical details of the filter, but the gist is that the Kalman filter computes the posterior distribution for the next response data point, &lt;span class=&#34;math inline&#34;&gt;\(p(y_t | y_{1:t-1})\)&lt;/span&gt;, recursively through the posterior on the latent state posterior, &lt;span class=&#34;math inline&#34;&gt;\(p(\alpha_t | y_{1:t-1})\)&lt;/span&gt;.&lt;a href=&#34;#fn13&#34; class=&#34;footnote-ref&#34; id=&#34;fnref13&#34;&gt;&lt;sup&gt;13&lt;/sup&gt;&lt;/a&gt; Our model object contains posterior draws (in our case, &lt;span class=&#34;math inline&#34;&gt;\(500\)&lt;/span&gt; of them) for each posterior distribution &lt;span class=&#34;math inline&#34;&gt;\(p(y_t | y_{1:t-1})\)&lt;/span&gt;. The “errors” above are the difference between these posterior samples, and the actual observed data point.&lt;/p&gt;
&lt;p&gt;This poses a problem for applying this function directly to our setting. The mean absolute prediction errors are computed on the response data, which are logarithms, whereas we are most likely interested in errors on the regular scale. The fix is simply to grab the relevant parts of the &lt;code&gt;CompareBstsModels()&lt;/code&gt; &lt;a href=&#34;https://rdrr.io/cran/bsts/src/R/compare.bsts.models.R&#34;&gt;source code&lt;/a&gt;, and do a bit of surgery until it suits our purpose. We take the one-step errors directly from the model object, and remove the burn-in. Then, we convert from posterior errors in the log scale, to posterior errors on the regular scale. If such a log-scale posterior error is &lt;span class=&#34;math inline&#34;&gt;\(\hat{e}\)&lt;/span&gt;, our response (in regular scale, not log) as &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, and our predicted value as &lt;span class=&#34;math inline&#34;&gt;\(\hat{y}\)&lt;/span&gt;, we have &lt;span class=&#34;math inline&#34;&gt;\(\hat{e} = \log_{10} \hat{y} - \log_{10}y\)&lt;/span&gt;. Thus, the errors we want are given by
&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
\hat{e} &amp;amp;= \log_{10} \hat{y} - \log_{10}y\\
\hat{y} - y &amp;amp;= 10^{\hat{e}}y -y = (10^{\hat{e}}-1)y.
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# This is copied directly from the source code, and then with
# the noted modifications (and some added features ommitted)
# to fit our purposes.
# It returns a similar graph to CompareBstsModels(), but once it
# transforms the observed and fitted values back to the original scale
# (rather than the log10 scale of the model).
CompareBstsModelsLog &amp;lt;- function(model.list) {
  main &amp;lt;- &amp;quot;Cum. One-Step Error&amp;quot;
  burn &amp;lt;- SuggestBurn(.1, model.list[[1]])
  model.names &amp;lt;- names(model.list)
  if (is.null(model.names)) {
    model.names &amp;lt;- paste(&amp;quot;Model&amp;quot;, 1:length(model.list))
  }
  number.of.models &amp;lt;- length(model.list)
  opar &amp;lt;- par(mfrow = c(2, 1))
  original.margins &amp;lt;- c(5.1, 4.1, 4.1, 2.1)
  margins &amp;lt;- original.margins
  opar$mar &amp;lt;- original.margins
  margins[1] &amp;lt;- 0
  par(mar = margins)
  cumulative.errors &amp;lt;- matrix(nrow = number.of.models, 
                              ncol = length(model.list[[1]]$original.series))
  for (i in 1:number.of.models) {
    # The prediction errors are in the log scale, and we 
    # grab them directly from the model itself.
    e.hat.log &amp;lt;- model.list[[i]]$one.step.prediction.errors
    e.hat.log &amp;lt;- e.hat.log[-(1:burn), , drop = FALSE]
    # We compute our prediction error as \hat{y} - y = (10^{\hat{e}}-1)y
    prediction.errors &amp;lt;- (10^e.hat.log - 1)*
      10^model.list[[1]]$original.series
    cumulative.errors[i, ] &amp;lt;- cumsum(abs(colMeans(prediction.errors)))
  }
  colors &amp;lt;- c(&amp;quot;black&amp;quot;, 
              rainbow(number.of.models - 1))
  
  idx &amp;lt;- model.list[[1]]$timestamp.info$timestamps
  plot(zoo(cumulative.errors[1, ], 
           order.by = idx),
       ylim = range(cumulative.errors),
       ylab = &amp;quot;cumulative absolute error&amp;quot;,
       xaxt = &amp;quot;n&amp;quot;,
       lwd = 2,
       yaxs = &amp;quot;i&amp;quot;)
  axis(2)
  for (i in 2:number.of.models) {
    lines(zoo(cumulative.errors[i, ], order.by = idx),
          lty = i,
          col = colors[i],
          lwd = 2)
  }
  grid()
  legend(&amp;quot;topleft&amp;quot;,
         model.names,
         lty = 1:number.of.models,
         col = colors,
         bg  = &amp;quot;white&amp;quot;,
         lwd = 2)
  
  margins &amp;lt;- original.margins
  margins[3] &amp;lt;- 0
  par(mar = margins)
  plot(model.list[[1]]$original.series,
       main = &amp;quot;&amp;quot;,
       ylab = &amp;quot;scaled values&amp;quot;,
       xlab = &amp;quot;Date&amp;quot;,
       yaxs = &amp;quot;i&amp;quot;)
  grid()
  par(opar)
}

CompareBstsModelsLog(list(Season = model.season, 
                          LLT = model.llt))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-28-bsts-boxofice.en/index_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;
Of course, this transformation doesn’t radically change the picture, it’s still measuring the same errors. But the units of absolute error are now interpretable, the difference between the two is more clear, and there could be cases where computing the errors based on the logs would lead to unintuitive results.&lt;/p&gt;
&lt;p&gt;Further, as we keep repeating, one should think carefully about the goals of the analysis. The cumulative error is an intuitive metric, but it’s not particularly Bayesian (as it is computed using a simple point estimate on the posterior). If the ultimate output of the model is a sequence of these point estimates, then it is a sensible place to start. If the goal is to construct a full posterior probability that captures the theoretical situation, it won’t be very meaningful.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;bsts&lt;/code&gt; package offers a &lt;a href=&#34;https://rdrr.io/cran/bsts/man/plot.bsts.html&#34;&gt;variety of other plotting tools&lt;/a&gt; (such as &lt;code&gt;PlotBstsComponents()&lt;/code&gt;, &lt;code&gt;PlotBstsForecastDistribution()&lt;/code&gt;, &lt;code&gt;PlotSeasonalEffect()&lt;/code&gt;, and more), but they didn’t appear very informative when applied directly out of the box to this data set. Which isn’t surprising, it’s hard to write general purpose plotting functions. In almost all cases, it’s better to construct your own, to tackle the task at hand.
&lt;!-- # ```{r} --&gt;
&lt;!-- # PlotBstsComponents(model.season) --&gt;
&lt;!-- # ``` --&gt;&lt;/p&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- ss.slt &lt;- AddSemilocalLinearTrend(ss.season, --&gt;
&lt;!--                                   weekly.tb$log_gross_ia) --&gt;
&lt;!-- model.slt &lt;- bsts(weekly.tb$log_gross_ia,  --&gt;
&lt;!--                   state.specification = ss.slt, --&gt;
&lt;!--                   niter = 500,  --&gt;
&lt;!--                   ping = 0,  --&gt;
&lt;!--                   seed = 21) --&gt;
&lt;!-- CompareBstsModels(list(Season = model.season,  --&gt;
&lt;!--                        LLT = model.llt, --&gt;
&lt;!--                        SLT = model.slt)) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- ss.slt &lt;- AddSeasonal(list(), weekly.tb$log_gross_ia,  --&gt;
&lt;!--                       nseasons = 52) --&gt;
&lt;!-- ss.llt &lt;- AddSemilocalLinearTrend(ss.slt, --&gt;
&lt;!--                                   weekly.tb$log_gross_ia) --&gt;
&lt;!-- model.llt &lt;- bsts(weekly.tb$log_gross_ia,  --&gt;
&lt;!--                   state.specification = ss.slt, --&gt;
&lt;!--                   niter = 500,  --&gt;
&lt;!--                   ping = 0,  --&gt;
&lt;!--                   seed = 21) --&gt;
&lt;!-- CompareBstsModels(list(Season = model.season, --&gt;
&lt;!--                        Season2 = model.season)) --&gt;
&lt;!-- ``` --&gt;
&lt;/div&gt;
&lt;div id=&#34;more-specific-tools&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;More Specific Tools&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;CompareBstsModels()&lt;/code&gt; function helps compare the cumulative growth of our one-step predictive error, but we might be curious to see what the actual predictive &lt;em&gt;fit&lt;/em&gt; looks lie, visually. We construct a function to create that plot, with much of this code taken directly (with modifications) from Kim Larsen’s &lt;a href=&#34;https://multithreaded.stitchfix.com/blog/2016/04/21/forget-arima/&#34;&gt;excellent blog post&lt;/a&gt;.&lt;/p&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- burn &lt;- SuggestBurn(.1, model.season) --&gt;
&lt;!-- 10^(model.season$one.step.prediction.errors) --&gt;
&lt;!-- Actual_Log &lt;- weekly.tb$log_gross_ia --&gt;
&lt;!-- predictions &lt;- 10^(-model.season$one.step.prediction.errors[-(1:burn),] + Actual_Log)  --&gt;
&lt;!-- pred.quantiles &lt;- as_tibble(t(apply(predictions,  --&gt;
&lt;!--                                     2,  --&gt;
&lt;!--                                     function(x) quantile(x,  --&gt;
&lt;!--                                                          c(.025, --&gt;
&lt;!--                                                            .975))))) %&gt;% --&gt;
&lt;!--   mutate(Date = weekly.tb$start_date) --&gt;
&lt;!-- names(pred.quantiles) &lt;- c(&#34;Date&#34;,&#34;Lower&#34;, &#34;Upper&#34;) --&gt;
&lt;!-- p.season &lt;- predict(mod) --&gt;
&lt;!-- predict.bsts(model.season.train,  --&gt;
&lt;!--                          horizon = weekly.tb %&gt;% nrow() -  --&gt;
&lt;!--                            train.tb %&gt;% nrow()) --&gt;
&lt;!-- tb %&gt;%  --&gt;
&lt;!--   posterior.interval &lt;- cbind.data.frame( --&gt;
&lt;!--     10^as.numeric(p.season$interval[1,]), --&gt;
&lt;!--     10^as.numeric(p.season$interval[2,]),  --&gt;
&lt;!--     subset(tb, year(Date) &gt; cutoff)$Date) --&gt;
&lt;!-- names(posterior.interval) &lt;- c(&#34;LL&#34;, &#34;UL&#34;, &#34;Date&#34;) --&gt;
&lt;!-- tb &lt;- left_join(tb,  --&gt;
&lt;!--                 posterior.interval,  --&gt;
&lt;!--                 by = &#34;Date&#34;) --&gt;
&lt;!-- ``` --&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# For a given model, the returned tibble has three columns.
# Fitted: the mean of the one-step posterior predictions, transformed
#   back into our original scale.
# Actual: the original response values.
# Date: the date.
ComputeOneStepFitted &amp;lt;- function(model, dat, response_var) {
  burn &amp;lt;- SuggestBurn(0.1, model)
  response_var &amp;lt;- enquo(response_var)
  Actual_Log &amp;lt;- dat %&amp;gt;% pull(!! response_var)
  tb &amp;lt;- tibble(Fitted = 10^(-colMeans(
    model$one.step.prediction.errors[-(1:burn),])
    + Actual_Log),
    Actual = 10^(Actual_Log),
    Date = dat$start_date)
  Actual_Log &amp;lt;- weekly.tb$log_gross_ia
  
  predictions &amp;lt;- 10^(-model$one.step.prediction.errors[-(1:burn),] + Actual_Log) 
  pred.quantiles &amp;lt;- as_tibble(t(apply(predictions, 
                                      2, 
                                      function(x) 
                                        quantile(x, c(.025, .975))))) %&amp;gt;%
    transmute(Date = dat$start_date,
              LL = `2.5%`,
              UU = `97.5%`)
  tb &amp;lt;- tb %&amp;gt;% left_join(pred.quantiles, by = &amp;quot;Date&amp;quot;)
  return(tb)
}

# Given a tibble computed from ComputeOneStepFitted(), this 
# computes the Mean Absolute Percentage Error, based on these
# Actual and Fitted results.
ComputeMAPE &amp;lt;- function(tb) {
  tb %&amp;gt;% 
    summarize(MAPE = mean(abs(Actual - Fitted)/Actual))
}

# Given a tibble computed from ComputeOneStepFitted(), we compute the 
# 
PlotOneStepFitted &amp;lt;- function(tb, 
                              plot_quantiles = FALSE,
                              Response_Label = 
                                &amp;quot;Log Inf. Adj. Gross ($)&amp;quot;) {
  MAPE &amp;lt;- ComputeMAPE(tb)
  plot.fitted &amp;lt;- tb %&amp;gt;% 
    ggplot(aes(x = Date)) +
    geom_line(aes(y = Actual, color = &amp;quot;Actual&amp;quot;), 
              size = .3) +
    geom_line(aes(y = Fitted, color = &amp;quot;Fitted&amp;quot;), 
              size = .3, linetype = 2) +
    theme_bw() + 
    theme(legend.title = element_blank()) + 
    ylab(Response_Label) + xlab(&amp;quot;&amp;quot;) +
    geom_vline(xintercept = as.numeric(as.Date(&amp;quot;1993-01-01&amp;quot;)), 
               linetype = 2) + 
    ggtitle(paste0(&amp;quot;BSTS -- One-Step MAPE = &amp;quot;, round(100*MAPE,2), &amp;quot;%&amp;quot;)) +
    theme(axis.text.x=element_text(angle = -90, hjust = 0)) + 
    ylim(c(.9*min(tb$Fitted, tb$Actual), 1.1*max(tb$Fitted, tb$Actual)))
  if (plot_quantiles) {
    plot.fitted &amp;lt;- plot.fitted + 
      geom_ribbon(aes(ymin = LL, 
                      ymax = UU), 
                  fill=&amp;quot;grey&amp;quot;, 
                  alpha=0.5) 
  }
  return(plot.fitted)
}

CheckOneStepCalibration &amp;lt;- function(tb) {
  tb %&amp;gt;% 
    mutate(is_below_UU = Actual &amp;lt; UU,
           is_above_LL = Actual &amp;gt; LL,
           is_within_interval = is_above_LL &amp;amp; is_below_UU) %&amp;gt;%
    summarize(`Prop. Above LB` = mean(is_above_LL),
              `Prop. Below UB` = mean(is_below_UU),
              `Prop. Within Interval` = mean(is_within_interval)) %&amp;gt;%
    return()
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tb.season &amp;lt;- ComputeOneStepFitted(model.season, weekly.tb, log_gross_ia)
PlotOneStepFitted(tb.season, plot_quantiles = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-28-bsts-boxofice.en/index_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tb.llt &amp;lt;- ComputeOneStepFitted(model.llt, weekly.tb, log_gross_ia)
PlotOneStepFitted(tb.llt, plot_quantiles = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-28-bsts-boxofice.en/index_files/figure-html/unnamed-chunk-11-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Broadly, we can see that the seasonal model is more conservative, and sticks with the broadly known weekly trends. It predicts spikes at major holidays, but otherwise misses on the unexpected micro trends. The LLT model &lt;em&gt;does&lt;/em&gt; make an attempt to predict these large spikes. It’s a very difficult task given the data, so we only see a moderate improvement in the MAPE. They represent two different philosophies, and the choice might depend on the specific goals.&lt;/p&gt;
&lt;p&gt;However, these are simply fitting point estimates, the means of the one-step posterior predictive distributions. In the tutorials I saw online, this was the the typical example used (with posterior distributions cited for the coefficients themselves), but we do have a full distribution to work with. We consider the rate of coverage for the 95% credible intervals for both distributions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;CheckOneStepCalibration(tb.season)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 3
##   `Prop. Above LB` `Prop. Below UB` `Prop. Within Interval`
##              &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;                   &amp;lt;dbl&amp;gt;
## 1            0.897            0.855                   0.752&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;CheckOneStepCalibration(tb.llt)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 3
##   `Prop. Above LB` `Prop. Below UB` `Prop. Within Interval`
##              &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;                   &amp;lt;dbl&amp;gt;
## 1            0.839            0.884                   0.722&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that neither one-step calibration is what we would hope, with only about 75% and 72%, respectively, for the seasonal and LLN model predictions capturing the observed data within their credible interval. We also see that the slight advantage in MAPE for the more flexible model evaporates under this perspective, which makes sense: the more aggressive model sometimes tries to “predict” a release that doesn’t occur. This helps slightly with its MAPE, but means it is more likely to whiff entirely in the credible interval.&lt;/p&gt;
&lt;p&gt;These credible intervals are not well calibrated, and it is not particularly surprising. We know there is a large amount of irreducible uncertainty in the system that our model does not capture. Ultimately, for a properly calibratd model, we would need to restructure it from these composition of normal models, to something that represents the theory of the system itself. It’s certainly notable that the seasonal effect alone seems to capture most of the variability, and that the addition of a local trend doesn’t appear to be significantly superior (depending on our goals).&lt;/p&gt;
&lt;p&gt;So, is there any use to these models at all? We might consider the argument by a skeptic, who thinks that the additional assumptions we pose are misguided, and the simplest of models is just as apt. Such a model would consist solely of a static intercept (note, it is not simply a flat model, it still has our composition of normal components, just without a trend).&lt;/p&gt;
&lt;p&gt;If we follow the same basic evaluative steps as above, we can see that the static intercept model has about 25% MAPE (compared to 16% before), and about 10 percentage point drop in coverage by the credible interval. Clearly, the flexibility afforded by our seasonal modeling adds to the predictive power. However, as we’d expect, the models shown above are quite flawed, and the fact that they are not too many miles ahead of the static model is cause for further scrutiny of our theoretical setup.&lt;/p&gt;
&lt;!-- I&#39;m still a little uncertain on this part, I actually thought that the mean one-step posterior predictive value of a static model might be fixed, should investigate further. --&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ss.skeptic &amp;lt;- AddStaticIntercept(list(), 
                                 weekly.tb$log_gross_ia)
model.skeptic &amp;lt;- bsts(weekly.tb$log_gross_ia, 
                      state.specification = ss.skeptic,
                      niter = 2000, 
                      ping = 0, 
                      seed = 21)
tb.skeptic &amp;lt;- ComputeOneStepFitted(model.skeptic, 
                                   weekly.tb, 
                                   log_gross_ia)
PlotOneStepFitted(tb.skeptic, plot_quantiles = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-28-bsts-boxofice.en/index_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;CheckOneStepCalibration(tb.skeptic)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 3
##   `Prop. Above LB` `Prop. Below UB` `Prop. Within Interval`
##              &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;                   &amp;lt;dbl&amp;gt;
## 1            0.862            0.799                   0.661&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;coefficients&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Coefficients&lt;/h2&gt;
&lt;p&gt;Of course, predictive power is not the only allure of constructing such a Bayesian model. While not the focus of this exploration, we can also examine the resulting model coefficients. In this case, we take a peak at the seasonal coefficients that result from the pure seasonal model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;burn &amp;lt;- SuggestBurn(.1, model.season)
components &amp;lt;- cbind.data.frame(
  colMeans(model.season$state.contributions[-(1:burn),&amp;quot;seasonal.52.1&amp;quot;,]),
  weekly.tb$start_date)  
names(components) &amp;lt;- c(&amp;quot;Seasonality&amp;quot;, &amp;quot;Date&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We look at box plots for each of the 52 weeks, showing their spread. We also consider the visual trend of the week coefficients over the past few years.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;components %&amp;gt;% mutate(week_number = row_number() %% 52) %&amp;gt;%
  ggplot() + geom_boxplot(aes(group = week_number, y = Seasonality))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-28-bsts-boxofice.en/index_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data=components %&amp;gt;% filter(year(Date) &amp;lt; 2015 &amp;amp; year(Date) &amp;gt; 2012), aes(x=Date, y=Seasonality)) + geom_line() + 
  theme_bw() + theme(legend.title = element_blank()) + ylab(&amp;quot;&amp;quot;) + xlab(&amp;quot;&amp;quot;) + guides(colour=FALSE) + 
  theme(axis.text.x=element_text(angle = -90, hjust = 0))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-28-bsts-boxofice.en/index_files/figure-html/unnamed-chunk-15-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can observe the typically expected trends. While January is a strong month (due to Oscars hits being in theaters, and the tail end of Christmas), theaters hit a low during the early months of the year, before surging at summer, lagging in the fall, and surging again around Christmas.&lt;a href=&#34;#fn14&#34; class=&#34;footnote-ref&#34; id=&#34;fnref14&#34;&gt;&lt;sup&gt;14&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;brief-mention-of-long-term-forecasting&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Brief Mention of Long Term Forecasting&lt;/h1&gt;
&lt;p&gt;Throughout this exploration, I have mentioned how I am being purposefully fuzzy about the goals of the model. It is important to emphasize that, because in any real application, these goals would have to be foregrounded in the construction of &lt;em&gt;any&lt;/em&gt; model (whereas the point of this post is to just explore the tools). One example of this is in a critical distinction between types of prediction: forecasting in the long run, and forecasting the next step (or “nowcasting”).&lt;/p&gt;
&lt;p&gt;Thomas Olavson’s &lt;a href=&#34;http://www.unofficialgoogledatascience.com/2019/12/humans-in-loop-forecasting-integrating.html&#34;&gt;blog post&lt;/a&gt; on the objective automation of forecasting helps clarify this distinction. He calls the long run objective “strategic forecasting, which has low update frequency, uncertainty which is difficult to quantify, and whose outputs are used in high stakes decision making (with significant human interpretation). By comparison,”tactical forecasting&amp;quot; is a mostly automated process, making countless micro predictions at scale, whose automation can be guided by frequent backtesting (and related strategies).&lt;/p&gt;
&lt;p&gt;In theory, it seems that Bayesian Structural Time Series models should be perfectly serviceable at either goal. The original paper by Scott &amp;amp; Varian focuses on the short term nowcasting, but the interpretability of the Bayesian framework makes longer term forecasts a reasonable application. However, the models may be built quite differently when we keep these goals in mind.&lt;/p&gt;
&lt;p&gt;Implicitly, our work this far has been geared towards tactical forecasting, using the model to construct point estimates for each subsequent time step, relying on a relatively flexible fit to the data.&lt;/p&gt;
&lt;p&gt;Our full model computed earlier uses the Kalman smoother and is fit over &lt;em&gt;all&lt;/em&gt; the data. Thus, to get a sense of how this might work for forecasting, we need to turn to a training and test set structure. We pick the cut-off arbitrarily at the end of 2016 (we would hope that the model is relatively agnostic about this choice, but we will see that in this case, it is not). In a more serious application, it would likely make sense to compute models for a sequence of forecast cut-offs, but this choice is just for a quick demonstration.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cutoff &amp;lt;- 2016
# Divide into &amp;quot;training&amp;quot; and &amp;quot;test&amp;quot; set (for forecasting)
train.tb &amp;lt;- weekly.tb %&amp;gt;% 
  filter(year &amp;lt;= cutoff)
test.tb &amp;lt;- weekly.tb %&amp;gt;% 
  filter(start_date &amp;gt; cutoff)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We again compute the same two models as before. The 52 week seasonal model, with a static intercept (just on the training data).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ss.season.train &amp;lt;- AddSeasonal(list(), 
                               train.tb$log_gross_ia, 
                               nseasons = 52)
ss.season.intercept.train &amp;lt;- AddStaticIntercept(ss.season.train,
                                                train.tb$log_gross_ia)
model.season.train &amp;lt;- bsts(train.tb$log_gross_ia, 
                           state.specification = ss.season.intercept.train,
                           niter = 500, 
                           ping = 0, 
                           seed = 21)

p.season &amp;lt;- predict.bsts(model.season.train, 
                         horizon = weekly.tb %&amp;gt;% nrow() - 
                           train.tb %&amp;gt;% nrow())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And the model where we add a local linear trend.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ss.llt.train &amp;lt;- AddLocalLinearTrend(ss.season.train,
                                    train.tb$log_gross_ia)
model.llt.train &amp;lt;- bsts(train.tb$log_gross_ia, 
                        state.specification = ss.llt.train,
                        niter = 500, 
                        ping = 0, 
                        seed = 21)
p.llt &amp;lt;- predict.bsts(model.llt.train, 
                      horizon = weekly.tb %&amp;gt;% nrow() - 
                        train.tb %&amp;gt;% nrow())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now use the &lt;code&gt;bsts&lt;/code&gt; built in feature for plotting these forecasts, showing the fitted values and credible intervals.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(p.season, plot.original = 52*2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-28-bsts-boxofice.en/index_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(p.llt, plot.original = 52*2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-28-bsts-boxofice.en/index_files/figure-html/unnamed-chunk-19-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The only real punchline here is that both models make moderately similar projections for the next few years. However, the plots aren’t too revealing yet, and we want to compare them to the observed values.
We can rework our earlier functions to now compute MAPE for both fitted and predicted. Now, in addition to the credible interval (for both the one-step predictions, and the future forecasts), we zoom in on the time period which is forecast (based on our training data, up until the end of 2016).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(gridExtra)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;gridExtra&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:dplyr&amp;#39;:
## 
##     combine&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ComputeOneStepPredicted &amp;lt;- function(model, dat, response_var, p) {
  burn &amp;lt;- SuggestBurn(0.1, model)
  response_var &amp;lt;- enquo(response_var)
  Actual_Log &amp;lt;- dat %&amp;gt;% pull(!! response_var)
  tb &amp;lt;- tibble(Fitted = c(10^as.numeric(-colMeans(
    model$one.step.prediction.errors[-(1:burn),])
    + model$original.series), 10^p$mean),
    Actual = 10^(Actual_Log),
    Date = dat$start_date)
  
  Actual_Log_Past &amp;lt;- dat %&amp;gt;% 
    head(length(Actual_Log) - length(p$mean)) %&amp;gt;% 
    pull(!! response_var)
  predictions &amp;lt;- 10^(-model$one.step.prediction.errors[-(1:burn),] + Actual_Log_Past) 
  pred.quantiles &amp;lt;- as_tibble(t(apply(predictions, 
                                      2, 
                                      function(x) 
                                        quantile(x, c(.025, .975))))) %&amp;gt;%
    bind_rows(as_tibble(t(10^p$interval))) %&amp;gt;%
    transmute(Date = dat$start_date,
              LL = `2.5%`,
              UU = `97.5%`)
  tb &amp;lt;- tb %&amp;gt;% left_join(pred.quantiles, by = &amp;quot;Date&amp;quot;)
  return(tb)
}

PlotOneStepPredicted &amp;lt;- function(tb, 
                                 p,
                                 cutoff,
                                 Response_Label = 
                                   &amp;quot;Log Inf. Adj. Gross ($)&amp;quot;) {
  MAPE &amp;lt;- ComputeMAPE(tb)
  # posterior.interval &amp;lt;- cbind.data.frame(
  #   10^as.numeric(p$interval[1,]),
  #   10^as.numeric(p$interval[2,]), 
  #   subset(tb, year(Date) &amp;gt; cutoff)$Date)
  # names(posterior.interval) &amp;lt;- c(&amp;quot;LL&amp;quot;, &amp;quot;UU&amp;quot;, &amp;quot;Date&amp;quot;)
  # tb &amp;lt;- left_join(tb, as_tibble(posterior.interval), by=&amp;quot;Date&amp;quot;)
  # 
  plot.full &amp;lt;- tb %&amp;gt;% 
    ggplot(aes(x = Date)) +
    geom_line(aes(y = Actual, color = &amp;quot;Actual&amp;quot;), 
              size = .3) +
    geom_line(aes(y = Fitted, color = &amp;quot;Fitted&amp;quot;), 
              size = .3, linetype = 2) +
    theme_bw() + 
    theme(legend.title = element_blank()) + 
    ylab(Response_Label) + xlab(&amp;quot;&amp;quot;) +
    geom_vline(xintercept = as.numeric(as.Date(paste0(cutoff, &amp;quot;-12-31&amp;quot;)), linetype=2)) + 
    geom_ribbon(aes(ymin = LL, 
                    ymax = UU), 
                fill=&amp;quot;grey&amp;quot;, 
                alpha=0.5) +
    ggtitle(paste0(&amp;quot;BSTS -- Total One-Step MAPE = &amp;quot;, round(100*MAPE,2), &amp;quot;%&amp;quot;)) +
    ylim(c(.9*min(tb$Fitted, tb$Actual), 1.1*max(tb$Fitted, tb$Actual)))

  
  plot.forecast &amp;lt;- subset(tb, year(Date) &amp;gt; cutoff) %&amp;gt;%
    ggplot(aes(x = Date)) +
    geom_line(aes(y = Actual, color = &amp;quot;Actual&amp;quot;), 
              size = .3) +
    geom_line(aes(y = Fitted, color = &amp;quot;Fitted&amp;quot;), 
              size = .3, linetype = 2) +
    theme_bw() + 
    theme(legend.title = element_blank()) + 
    ylab(Response_Label) + xlab(&amp;quot;&amp;quot;) +
    geom_vline(xintercept = as.numeric(as.Date(paste0(cutoff, &amp;quot;-12-31&amp;quot;)), linetype=2)) + 
    geom_ribbon(aes(ymin = LL, ymax = UU), fill=&amp;quot;grey&amp;quot;, alpha=0.5) + 
    ylim(c(.9*min(tb$Fitted, tb$Actual), 1.1*max(tb$Fitted, tb$Actual)))
  grid.arrange(plot.full, plot.forecast, nrow = 2)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tb.season.train &amp;lt;- ComputeOneStepPredicted(model.season.train,
                                           weekly.tb,
                                           log_gross_ia,
                                           p.season)
CheckOneStepCalibration(tb.season.train)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 3
##   `Prop. Above LB` `Prop. Below UB` `Prop. Within Interval`
##              &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;                   &amp;lt;dbl&amp;gt;
## 1            0.908            0.852                   0.760&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;PlotOneStepPredicted(tb.season.train, p.season, 2016)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-28-bsts-boxofice.en/index_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tb.llt.train &amp;lt;- ComputeOneStepPredicted(model.llt.train,
                                        weekly.tb,
                                        log_gross_ia,
                                        p.llt)
CheckOneStepCalibration(tb.llt.train)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 3
##   `Prop. Above LB` `Prop. Below UB` `Prop. Within Interval`
##              &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;                   &amp;lt;dbl&amp;gt;
## 1            0.856            0.889                   0.744&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;PlotOneStepPredicted(tb.llt.train, p.season, 2016)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-28-bsts-boxofice.en/index_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can see that both seem to offer reasonably useful forecasts, with calibration (over the entire period, combining both one-step and forecasts)\footnote[Perhaps these should be separated, I haven’t found resources discussing the best way to evaluate these models.] in a similar range as before, and the forecast range seemingly capturing much of the observed trend.&lt;/p&gt;
&lt;p&gt;However, the particular difference between the two models becomes clear in the distant forecast. The seasonal model is more consistent, with fairly static uncertainty. With the addition of the local linear trend, we become more confident in the short run, but have much less confidence once we project several years out. This is because the local linear trend is extremely flexible, which becomes particularly apparent when considering outcomes several years away. Whether the state of the movie business is sufficiently static for this to be a poor assumption will depend on who you ask.&lt;a href=&#34;#fn15&#34; class=&#34;footnote-ref&#34; id=&#34;fnref15&#34;&gt;&lt;sup&gt;15&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;not-far-from-madness&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Not Far From Madness?&lt;/h2&gt;
&lt;p&gt;While our starter models continue to perform moderately well (if unexceptionally), it is illustrative to consider how easy this modeling can go badly wrong. It only takes a few wrong choices for a model to go from simply incomplete, to totally nonsensical.&lt;/p&gt;
&lt;p&gt;For example, what happens if we remove our seasonal correction, and left the local linear trend in to capture this seasonal variation? As the movie industry heats up in the summer months, perhaps the local linear trend is sufficient to capture that growth. We certainly would expect our accuracy to decrease, we’ve established that the week of the year is a critical predictor of the weekly gross. However, what we’ll see below is that the removal of the seasonal adjustment makes the model &lt;em&gt;completely useless&lt;/em&gt; when it comes to longer term forecasting, not simply “inaccurate”.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# A model with solely a local linear trend 
ss.base &amp;lt;- AddLocalLinearTrend(list(),
                               train.tb$log_gross_ia)
model.base &amp;lt;- bsts(train.tb$log_gross_ia, 
                   state.specification = ss.base,
                   niter = 500, 
                   ping = 0, 
                   seed = 21)

p.base &amp;lt;- predict.bsts(model.base, 
                       horizon = weekly.tb %&amp;gt;% nrow() - 
                         train.tb %&amp;gt;% nrow())


tb.base &amp;lt;- ComputeOneStepPredicted(model.base, 
                                   weekly.tb, 
                                   log_gross_ia, 
                                   p.base)
PlotOneStepPredicted(tb.base, p.base, cutoff)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-28-bsts-boxofice.en/index_files/figure-html/unnamed-chunk-23-1.png&#34; width=&#34;672&#34; /&gt;
We can see that the results are completely nonsensical. It is a little more clear when using the built in prediction plots (although the dates are removed, I didn’t have time to add those back in).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(p.base, plot.original = 52*2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-28-bsts-boxofice.en/index_files/figure-html/unnamed-chunk-24-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We recall that the original series is on the &lt;em&gt;log&lt;/em&gt; scale, which helps clarify the issue.&lt;/p&gt;
&lt;p&gt;The intuitive explanation is straightforward enough. Without the seasonal adjustment, the local linear trend has learned from the data that there is great variability. The slope is constantly changing direction, multiple times per year, so it assigned a large variance in its movements. Without observing any new data, this uncertainty compounds, as the model thinks that the slope could be rapidly growing, or rapidly falling. And worse, this all happens on that log scale. So we quickly see the range of uncertainty about the state state double and triple in log units, which leads to absurd credible intervals for the future predictions.&lt;/p&gt;
&lt;p&gt;What’s interesting about this is that our short run predictions are not so impossible. We can see in the comparison below that the model without seasonality is clearly inferior, but it simply looks like simply a moderate step down. It’s only once we take stab at long term forecasting that we see that the use of a local linear trend without a seasonal adjustment fully breaks.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;CompareBstsModelsLog(list(`LLT + Seasonal` = model.llt.train,
                          `LLT Without Seasonal` = model.base))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-28-bsts-boxofice.en/index_files/figure-html/unnamed-chunk-25-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This reiterates the aforementioned discussion in Thomas Olavson’s &lt;a href=&#34;http://www.unofficialgoogledatascience.com/2019/12/humans-in-loop-forecasting-integrating.html&#34;&gt;blog post&lt;/a&gt;, distinguishing tactical and strategic forecasting. He discusses the great theoretical and practical differences between the two challenges. This frivolous example is simply a reminder of how easy theoretical models can break, when we are careless with the data. Long term forecasting requires more theoretical care, as we cannot rely on automatic processes fitting relevant patterns in the data and avoiding disaster. Poor theoretical assumptions will lead to useless forecasts.&lt;/p&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- tb.llt.train &lt;- ComputeOneStepPredicted(model.llt.train,  --&gt;
&lt;!--                                         weekly.tb,  --&gt;
&lt;!--                                         log_gross_ia,  --&gt;
&lt;!--                                         p.llt) --&gt;
&lt;!-- PlotOneStepPredicted(tb.llt.train, p.llt, 2016) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- tb.base &lt;- ComputeOneStepPredicted(model.base,  --&gt;
&lt;!--                                    weekly.tb,  --&gt;
&lt;!--                                    log_gross_ia,  --&gt;
&lt;!--                                    p.base) --&gt;
&lt;!-- tb.base %&gt;% tail() --&gt;
&lt;!-- #PlotOneStepPredicted(tb.base, p.base, 2016) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- It goes without saying that the forecasting  --&gt;
&lt;!-- The 95% prediction intervals in this case range from  --&gt;
&lt;!-- a --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- tb.temp  &lt;- ComputeOneStepFitted(model.base,  --&gt;
&lt;!--                                  train.tb,  --&gt;
&lt;!--                                  log_gross_ia) --&gt;
&lt;!-- PlotOneStepFitted(tb.temp) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- # This is for not adjusting for inflation.  --&gt;
&lt;!-- # ss.season.train.orig &lt;- AddSeasonal(list(),  --&gt;
&lt;!-- #                                train.tb$log_gross,  --&gt;
&lt;!-- #                                nseasons = 52) --&gt;
&lt;!-- # ss.llt.train.orig &lt;- AddLocalLinearTrend(ss.season.train, --&gt;
&lt;!-- #                                          train.tb$log_gross) --&gt;
&lt;!-- # model.llt.train.orig &lt;- bsts(train.tb$log_gross,  --&gt;
&lt;!-- #                              state.specification = ss.llt.train.orig, --&gt;
&lt;!-- #                              niter = 500,  --&gt;
&lt;!-- #                              ping = 0,  --&gt;
&lt;!-- #                              seed = 21) --&gt;
&lt;!-- #  --&gt;
&lt;!-- # p.llt.orig &lt;- predict.bsts(model.llt.train.orig,  --&gt;
&lt;!-- #                            horizon = weekly.tb %&gt;% nrow() -  --&gt;
&lt;!-- #                              train.tb %&gt;% nrow()) --&gt;
&lt;!-- # plot(p.llt.orig, plot.original = 52*2) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- TO BE CONTINUED  --&gt;
&lt;!-- # Unfinished  --&gt;
&lt;!-- ## Exploring the Autocorrelation --&gt;
&lt;!-- One challenge is that these effects are all closely related. The introduction of regular seasonal effects can mask effects of auto correlation. To be honest, I don&#39;t know the best way to solve this. Eventually, the model should be well equipped to sort them out, but in the exploration stage, it&#39;s not so clear. For now, I will simply resort to a crude method at de-seasoning the model, by normalizing the grosses by the mean gross for each of the 52 weeks. This should help uncover autocorrelative effects. However, the model itself will be fit on data without this adjustment, because it will apply a more delicate (and Bayesian) treatment to these seasonal terms. I&#39;m curious what the best pratice here would be.  --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- weekly.tb &lt;- weekly.tb %&gt;%  --&gt;
&lt;!--   group_by(week) %&gt;% --&gt;
&lt;!--   mutate(week_mean_ia = mean(gross_ia)) %&gt;% --&gt;
&lt;!--   ungroup() %&gt;% --&gt;
&lt;!--   mutate(gross_ia_week_adj = gross_ia - week_mean_ia + mean(gross_ia)) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- We use the Acf function from the &#34;forecast&#34; package, which computes the correlation of the data for each &#34;lag&#34; (the number of steps back). Thus, we can see here that there is a strong positive correlation between the inflation adjusted weekly gross and the previous week, but once you get to around 10 weeks or so, the correlation becomes negative (if slight).  --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- library(forecast) --&gt;
&lt;!-- Acf(weekly.tb$gross_ia) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- However, this is applied to our original data, and the presumed seasonal effects might be causing potential problems. When we instead compute an ACF plot for the week-normalized gross, we see instead that the positive autocorrelation is *consistent*. This makes a good deal of sense. The 10 to 15 weeks we were looking at above is a long enough period of time, that you essentially switch &#34;movie seasons&#34;. Thus, intuitively it fits that there might be some negative correlation, as the low point of the movie year (around February and March), are roughly 10-20 weeks offset from some of the busiest movie times (Christmas, and the summer, in the two directions). --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- Acf(weekly.tb$gross_ia_week_adj) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- This local trend is typical of most time series models, but we might want to consider a slightly more advanced version.  --&gt;
&lt;!-- Now, we apply [differencing](https://otexts.com/fpp2/stationarity.html)  --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- temp &lt;- weekly.tb %&gt;%  --&gt;
&lt;!--   transmute(start_date = start_date, --&gt;
&lt;!--             gross_ia = gross_ia, --&gt;
&lt;!--             diff_gross_ia = gross_ia - lag(gross_ia), --&gt;
&lt;!--             diff2_gross_ia = diff_gross_ia - lag(diff_gross_ia))  --&gt;
&lt;!-- Acf(temp$gross_ia) --&gt;
&lt;!-- Acf(temp$diff_gross_ia) --&gt;
&lt;!-- Acf(temp$diff2_gross_ia) --&gt;
&lt;!-- ggplot(aes(x = start_date)) + --&gt;
&lt;!--   geom_line(aes(y = diff2_gross_ia))+ --&gt;
&lt;!--   xlab(&#34;Year&#34;) + ylab(&#34;&#34;) + --&gt;
&lt;!--   ggtitle(&#34;&#34;) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- # Not sure what&#39;s goign on here, reinvestigate --&gt;
&lt;!-- weekly.tb &lt;- weekly.tb %&gt;% --&gt;
&lt;!--   mutate(diff_log_gross_ia = log_gross_ia - lag(log_gross_ia), --&gt;
&lt;!--          diff2_log_gross_ia = diff_log_gross_ia - --&gt;
&lt;!--            lag(diff_log_gross_ia)) --&gt;
&lt;!-- ss.r.diff2 &lt;- AddSeasonal(list(),  --&gt;
&lt;!--                           weekly.tb$log_gross_ia,  --&gt;
&lt;!--                           nseasons = 52) --&gt;
&lt;!-- model.rdiff2 &lt;- bsts(log_gross_ia ~ diff_log_gross_ia + --&gt;
&lt;!--                        diff2_log_gross_ia,  --&gt;
&lt;!--                      state.specification = ss.r.diff2, --&gt;
&lt;!--                      data = weekly.tb %&gt;% slice(-c(1,2)), --&gt;
&lt;!--                      niter = 1000,  --&gt;
&lt;!--                      ping = 0,  --&gt;
&lt;!--                      seed = 21) --&gt;
&lt;!-- burn &lt;-  SuggestBurn(0.1, model.rdiff2) --&gt;
&lt;!-- os.preds.r.diff2 &lt;- ComputeOneStepFitted(model.rdiff2,  --&gt;
&lt;!--                                         weekly.tb %&gt;% slice(-c(1,2)), --&gt;
&lt;!--                                         log_gross_ia) --&gt;
&lt;!-- PlotOneStepFitted(os.preds.r.diff2) --&gt;
&lt;!-- components &lt;- cbind.data.frame( --&gt;
&lt;!--   colMeans(model.regres.diff2$state.contributions[-(1:burn), &#34;trend&#34;,]),                                --&gt;
&lt;!--   colMeans(bsts.model$state.contributions[-(1:burn), &#34;seasonal.12.1&#34;,]), --&gt;
&lt;!--   as.Date(time(Y)))  --&gt;
&lt;!-- ``` --&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;final-thoughts&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Final Thoughts&lt;/h1&gt;
&lt;p&gt;Using time series for forecasting is an exceptionally tricky endeavor, and this post is just a brief introduction as I dip my toe into the field. Looking at how time series are used publicly online can often give you the sense that it’s a matter of plugging the data into a flexible model, fitting the curve, and hoping for the best. However, it’s important to be precise about the inferential goals of this analysis.&lt;/p&gt;
&lt;p&gt;The punch line from this analysis might simply be “don’t cite a Bayesian time series when the specified joint probability model doesn’t naturally reflect our best understanding of the underlying process”. And that might be fair, outside of an introductory context I would be hesitant to say much of anything about such a tenuous approaach to modeling (I’d probably tell my boss just to take a look at the movies that are coming out, and see if he recognizes the names, before I try and use past data to predict the weekly box office gross!). But these models are often applied in imperfect ways, and I’ve found it illustrative to work carefully through one such example, and try and understand the mechanics of the model under the hood.&lt;/p&gt;
&lt;p&gt;In particular, the distinction between long run and short term forecasting is critical. They are markedly different challenges, which call for different tools and perspectives.&lt;/p&gt;
&lt;!-- Eric Tassone &amp; Farzan Rohani provide an insightful [blog post](http://www.unofficialgoogledatascience.com/2017/04/our-quest-for-robust-time-series.html) examining the way that time series forecasting can and should cite ensemble averaging. --&gt;
&lt;/div&gt;
&lt;div id=&#34;sources&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Sources&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;a href=&#34;https://cran.r-project.org/web/packages/bsts/bsts.pdf&#34;&gt;bsts package&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.nber.org/chapters/c12995&#34;&gt;Scott &amp;amp; Varian (2014)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.unofficialgoogledatascience.com/2017/07/fitting-bayesian-structural-time-series.html&#34;&gt;“Fitting Bayesian structural time series with the bsts R package”&lt;/a&gt; by Scott Varian, on the Google Unoficial Data Science Blog.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://multithreaded.stitchfix.com/blog/2016/04/21/forget-arima/&#34;&gt;“Sorry ARIMA, but I’m Going Bayesian”&lt;/a&gt;, by Kim Larsen.&lt;/li&gt;
&lt;li&gt;Thomas Rothenberg’s &lt;a href=&#34;https://eml.berkeley.edu/~rothenbe/Fall2007/kalman.pdf&#34;&gt;notes on the Kalman Filter&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.unofficialgoogledatascience.com/2017/04/our-quest-for-robust-time-series.html&#34;&gt;“Our quest for robust time series at scale”&lt;/a&gt;, by Tassone &amp;amp; Rohani on the Google Unofficial Data Science Blog.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://otexts.com/fpp2/&#34;&gt;“Forecasting: Principles and Practice”&lt;/a&gt;, by Rob Hyndman and George Athanasopoulos.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.unofficialgoogledatascience.com/2019/12/humans-in-loop-forecasting-integrating.html&#34;&gt;“Humans in the Loop”&lt;/a&gt;, by Thomas Olavson, on The UnofficialGoogle Data Science Blog.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;This summer, I may work professionally on projects along these lines, but I made sure to finish writing this before I saw this in a professional setting, as at that point I might have to think about what I could share.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;I.e. my lifetime.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;While this is an incredibly obvious correction, and I left it until now mostly to make a point about an easy but damaging mistake, it’s also not quite so apparent that it is a perfect and tidy fix. Between the 90s and today, it likely is sufficient, but there are other notable societal changes that can make comparing ticket sales difficult.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;If we evaluate this transformation, the Mean Absolute Percentage Error becomes nonsensical, where being off by a factor of two leads to only a small penalty.&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;Not classical time series don’t have good ways of handling this problem too! While it would indeed be foolish to consider the prior observation, the sole piece of information denoting the current state, time series models have a rich array of tools to handle these complex forms of autocorrelation.&lt;a href=&#34;#fnref5&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn6&#34;&gt;&lt;p&gt;I found &lt;a href=&#34;https://eml.berkeley.edu/~rothenbe/Fall2007/kalman.pdf&#34;&gt;this&lt;/a&gt; quite helpful.&lt;a href=&#34;#fnref6&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn7&#34;&gt;&lt;p&gt;Forecasting an unobserved response occurring in the given moment&lt;a href=&#34;#fnref7&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn8&#34;&gt;&lt;p&gt;In this post, I use “Predictive” to refer to the task of forecasting the future response (including in the very next step), and “Inferential” to refer to understanding trends in the data (e.g. the posterior distribution of the parameters in a sensible Bayesian model should provide useful inference.&lt;a href=&#34;#fnref8&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn9&#34;&gt;&lt;p&gt;As we will see in a later section, this deserves more careful consideration.&lt;a href=&#34;#fnref9&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn10&#34;&gt;&lt;p&gt;If I return to this dataset, my next approach will focus on explicitly modeling the structure of the autocorrelation.&lt;a href=&#34;#fnref10&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn11&#34;&gt;&lt;p&gt;Most of this is grabbed right from the bsts manual, whose documentation is very helpful.&lt;a href=&#34;#fnref11&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn12&#34;&gt;&lt;p&gt;Of course, if neither model captures the situation, this inference can be complicated, but it’s still useful to explore the area.&lt;a href=&#34;#fnref12&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn13&#34;&gt;&lt;p&gt;Where &lt;span class=&#34;math inline&#34;&gt;\(y_{1:t} := (y_1, \ldots, y_t)\)&lt;/span&gt;, matching the typical Kalman filter notation.&lt;a href=&#34;#fnref13&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn14&#34;&gt;&lt;p&gt;I am still not entirely confident about the interpretation of these plots, will return soon to clear this up.&lt;a href=&#34;#fnref14&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn15&#34;&gt;&lt;p&gt;In recent decades, it has likely been too consistent for this to be sensible, but given that this is being written as the movie industry is shut down during COVID-19, greater tail uncertainty doesn’t seem unreasonable.&lt;a href=&#34;#fnref15&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Summary of the Common Philisophical Interpretations of Probability</title>
      <link>/other_posts/interpreting-probability/</link>
      <pubDate>Thu, 28 May 2020 00:00:00 +0000</pubDate>
      <guid>/other_posts/interpreting-probability/</guid>
      <description>


&lt;p&gt;Common interpretations of probability &lt;span class=&#34;math inline&#34;&gt;\(P= 1\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(P=0\)&lt;/span&gt; that’s IT.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An example preprint / working paper</title>
      <link>/publication/preprint/</link>
      <pubDate>Sun, 07 Apr 2019 00:00:00 +0000</pubDate>
      <guid>/publication/preprint/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Slides&lt;/em&gt; button above to demo Academic&amp;rsquo;s Markdown slides feature.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Supplementary notes can be added here, including 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code and math&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>/slides/example/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>/slides/example/</guid>
      <description>&lt;h1 id=&#34;create-slides-in-markdown-with-academic&#34;&gt;Create slides in Markdown with Academic&lt;/h1&gt;
&lt;p&gt;
&lt;a href=&#34;https://sourcethemes.com/academic/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Academic&lt;/a&gt; | 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/hakimel/reveal.js#pdf-export&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF Export&lt;/a&gt;: &lt;code&gt;E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;
&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Code block:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;porridge = &amp;quot;blueberry&amp;quot;
if porridge == &amp;quot;blueberry&amp;quot;:
    print(&amp;quot;Eating...&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;
&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;
&lt;p&gt;Block math:&lt;/p&gt;
&lt;p&gt;$$
f\left( x \right) = ;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;
&lt;p&gt;Make content appear incrementally&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;fragment &#34; &gt;
One
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
&lt;strong&gt;Two&lt;/strong&gt;
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
Three
&lt;/span&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;
&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% speaker_note %}}
- Only the speaker can read these notes
- Press `S` key to view
{{% /speaker_note %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/img/boards.jpg&#34;
  &gt;

&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;
&lt;p&gt;Customize the slide style and background&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; slide background-image=&amp;quot;/img/boards.jpg&amp;quot; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;quot;#0000FF&amp;quot; &amp;gt;}}
{{&amp;lt; slide class=&amp;quot;my-style&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;
&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.reveal section h1,
.reveal section h2,
.reveal section h3 {
  color: navy;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;
&lt;a href=&#34;https://spectrum.chat/academic&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Gentle Introduction to Parlays and Independence </title>
      <link>/2018/11/08/parlays-independence/</link>
      <pubDate>Thu, 08 Nov 2018 00:00:00 +0000</pubDate>
      <guid>/2018/11/08/parlays-independence/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Betting parlays of positively correlated events will generally be profitable.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This statement is far from revolutionary, and will be obvious to those familar with the fundamentals of betting. However, I&amp;rsquo;ve talked to many folks with no prior experience who wanted to learn more. I thought it would be interesting and fun to write up a basic tutorial in this concept which assumes minimal background in probability (and should provide good intuition even with no mathematical foundation). This post is intended as an exercise in explaining concepts at an introductory level, and should be ignored by those who are already comfortable with the tools of probability (although many might find the 2018 election parlay opportunities of interest).&lt;/p&gt;
&lt;h1 id=&#34;betting&#34;&gt;Betting&lt;/h1&gt;
&lt;p&gt;Most people are familiar with the concept of a bet. I might flip a coin, and the loser pays the winner a dollar, which feels like a fair deal. Implicit in our evaluation of this bet as &amp;ldquo;fair&amp;rdquo; is the concept of Expected Value (EV). While this will avoid the mathematical definition whenever possible, we tend to have an intuitive sense of EV when we confront it in our daily lives. The fact that the coin flipping game is &amp;ldquo;fair&amp;rdquo; is obvious due to its symmetry. But if I instead offered to roll a die, and said that on a one through four you pay me a dollar, and on a five or six I pay you two dollars, many people would intuitively see this bet as fair as well. I have twice the chance to win, but have to pay out twice as much. Mathematically, we simply find the sum of all the outcomes multiplied by their probability of occurrence. I have \(2/3\) chance to pay you a dollar, and \(1/3\) chance to win two dollars, and \(-1*\frac{2}{3} + 2*\frac{1}{3} = 0\).&lt;/p&gt;
&lt;p&gt;When it comes to betting, it&amp;rsquo;s worth noting that Expected Value is the &lt;em&gt;only&lt;/em&gt; viable way to consider the outcome, and any other metric is bound to be a losing strategy in the long run. That doesn&amp;rsquo;t mean it applies in every practical situation. If I offered you a deal where I would double your life&amp;rsquo;s savings with a 55% chance, and you would lose it all with 45% chance, this is clearly a positive expected value bet, and it is also one that you should demonstrably &lt;em&gt;never&lt;/em&gt; take. The marginal utility of money states that the value of doubling your life savings does not equate to the loss from losing your life&amp;rsquo;s savings. This is a reminder that betting is particularly problematic in any situation where the stakes are not so low that you can focus solely on EV. If someone offered you the same deal on each of your individual dollars in your life savings independently, then it would be a consistently profitable proposition with essentially no downside, due to the law of large numbers. (And if one&amp;rsquo;s life savings is not suitably large so that the law of large numbers applies, betting should be far from your mind). This is not the purpose of this summary, but for anyone who does plan to place bets, it is essential that they give some preliminary consideration to the marginal utility of money, bankroll management, and the law of large numbers.&lt;/p&gt;
&lt;h1 id=&#34;betting-notation&#34;&gt;Betting Notation&lt;/h1&gt;
&lt;p&gt;In America, bets are usually denoted using the Moneyline system, where odds are reported in the form of \(\pm X-[\)hundred]. A +\(200\) line says that if you bet \$100 and win, you receive back \$200 plus your original bet. A \(-250\) line says that if you bet \$250 and win, you win \$100 (and of course also win back your original bet). This system makes it intuitive to measure your payout on a specific bet, but it is somewhat opaque when newer bettors try and understand the corresponding probabilities associated with each bet. We can work in terms of &amp;ldquo;Implied Odds&amp;rdquo;, which are simply the corresponding percentage chance that you would need to win for the bet to be breakeven in expected value. A +200 Moneyline has corresponding Implied Odds of 33.3%. Abroad, odds are frequently reported in fractional terms, a +200 Moneyline corresponds to 2/1 (&amp;ldquo;two to one&amp;rdquo;) odds. We note that Moneyline odds will be in the form of plus or minus at least \(100\). American sports will also report bettling lines such as the New England Patriots being \(-7.5\) for their game. This is a &lt;em&gt;point spread&lt;/em&gt;, where there is even money to be made betting whether the Patriots will score at least \(7.5\) more points than their opponent. I personally find it nearly impossible to quickly understand Moneyline odds in terms of probabilities, so I built a light 
&lt;a href=&#34;https://chrome.google.com/webstore/detail/odds-converter/klechkhopfnjihobbcfeheooaigjjgdg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chrome extension&lt;/a&gt; that can quickly perform these conversions on the fly.&lt;/p&gt;
&lt;h1 id=&#34;parlays&#34;&gt;Parlays&lt;/h1&gt;
&lt;p&gt;Standard bets offer a payout based on a binary result. You might bet that the Golden State Warriors win the NBA title (&amp;ldquo;NBA Futures&amp;rdquo;), or that the Eagles score at least six more points than the Raiders in their upcoming game (&amp;ldquo;beating the spread&amp;rdquo;). A bettor who has fallen on hard times might have little to do for the whole weekend but watch sports, and could place separate bets that Alabama beats Oklahoma and that Clemson beats Notre Dame in the College Football Playoff. However, they could also &amp;ldquo;parlay&amp;rdquo; events, in which they &lt;em&gt;only&lt;/em&gt; get paid out if &lt;em&gt;both&lt;/em&gt; those events occur (if Clemson beats ND but Alabama loses, then they would receive nothing). This seems like a scam, but the allure is that under a parlay the payoffs are multiplicative, and much more lucrative than simply the sum of those individual payouts. This is alluring to many bettors because it promises large paydays. In the above example, Alabama and Clemson were both large favorites, at -555 and -440 respectively (i.e. you have to place a bet of \$555 or \$440 to win a prize of \$100 if you are correct). Parlays are appealing to bettors because we have poor intuition for the likelihood of a number of independent (explained below) very likely events &lt;em&gt;all&lt;/em&gt; occurring. This fallacy is ubiquitous. In college football, when halfway through the season, a dozen contenders are 6-0, analysts and fans tend to look at their upcoming schedule, see that they are favored in each of those six games, and tend to muse about the situation in which each of those individually likely events occurs. In politics, readers are perplexed when analysts predict that of some 100 house races that lean solidly to one side, there will very likely be &lt;em&gt;some&lt;/em&gt; number of shocking upsets, even if we don&amp;rsquo;t know which one. Our mind works by considering individual events. If I can&amp;rsquo;t tell you &lt;em&gt;which&lt;/em&gt; remaining football game your team is likely to lose, you may be accustomed to think that the most likely result is that you don&amp;rsquo;t lose a single one, even when that is an unlikely event (as shown by the fact that even the top contenders tend to lose a game by the end of the college football season).&lt;/p&gt;
&lt;p&gt;On the other hand, parlays are appealing to bookies because they encourage more betting, and bookies profit from the volume of bets placed. There is no &lt;em&gt;catch&lt;/em&gt; here, and the bookies give a fair price on a parlay. Instead of the Clemson &amp;amp; Alabama example, we consider one where the probabilities are intuitive, with an extremely boring bookie that allows you to bet on the results of die rolls with no rake (which is the tax they take as profit on any bet placed). In each case, you bet \$1. For Bet A, they toss two dice, and you win \$12 (including your bet) when they sum to at least 11. For Bet B, they toss a single die, and you win \$6 (including your bet) if they roll a six. Both these bets have an expected value of 0. For Bet A, this has a 1 in 12 chance of occuring, so if you have placed a \$1 bet, you have a 1 in 12 chance of winning \$12, which is fair (and the same for Bet B with a 1 in 6 chance).&lt;/p&gt;
&lt;p&gt;If you wanted to up the stakes and instead ask for a parlay on these events, you could significantly increase your possible payout. Bookies are perfectly fair when it comes to parlays. They set their own lines, and choose how much &amp;ldquo;rake&amp;rdquo; to take from the bets, but when it comes to parlays, bookies follow a simple formula which preserves the expected value of the payouts. They simply consider it the same as a single bet on the event where both happen, with that probability computed by multiplying the individual probabilities together. In the case above, a parlay of Bet A and Bet B would yield a whopping \$72 payout when it hits. This calculation is predicated on the idea that the probability that both A and B occur is the product of their individual probabilities, so \(1/6 * 1/12 = 1/72\). If we use the mathematical shorthand \(P(A)\) to denote &amp;ldquo;Probability of Bet A succeeding&amp;rdquo;, and \(P(A,B)\) for &amp;ldquo;probability of both Bet A and Bet B succeeding&amp;rdquo;, we write this as \(P(A)*P(B)=P(A,B)\). Those familiar with probability will note that this formula holds for any two events which are &lt;em&gt;independent&lt;/em&gt;, that is, where information about the result of one event tells us nothing about the result of the other.&lt;/p&gt;
&lt;p&gt;Independence is a mathematically defined concept, but we have an intuition for its meaning. If I ask you to guess the probability that a baby is born on a white Christmas (that is, on a December 25th where it snows), you would be foolish to take the probability that the baby is born on December 25th, and multiply it by the probability that it is snowing on any given day. It is significantly more likely that it snows on December 25th than the average day because it is in the middle of winter. The proper calculation would be to find the product of the probabilities that it is December 25th, and that it is snowing on any given December 25th (a conditional probability, whose  definition we sidestep here). Luckily, in the case of our dice rolling bookie, independence is pretty irrefutable. Assuming that the dice are fair and properly weighted, the results of the first pair of rolls have no effect on the outcome of some later rolls.&lt;/p&gt;
&lt;h1 id=&#34;parlays-of-dependent-events&#34;&gt;Parlays of Dependent Events&lt;/h1&gt;
&lt;p&gt;This prompts the question, how do parlays work when the events are not independent? The short answer is that bookies generally do not offer parlays on dependent events, and that parlaying events that are positively correlated (i.e. the chance of one occurring makes it more likely that the other occurs) is profitable for the bettor. Luckily, most bets offered by the bookie tend to be independent. The scores of football games on a given Sunday seem to have little relation to one another. The most common bets to show dependence in fact show a &lt;em&gt;negative&lt;/em&gt; correlation, when only one can occur. A parlay on NBA futures where both the Golden State Warriors and the Cleveland Cavaliers win the NBA title is clearly nonsensical (the fact that the probability of both occurring is 0 is a form of dependence).&lt;/p&gt;
&lt;p&gt;What happens if a bookie allows betting on positively correlated events? This is best demonstrated by example. We consider Bet A and Bet B again, but with a twist. Bet A remains the same, but now define Bet B to be whether the first die rolled &lt;em&gt;of those used for Bet A&lt;/em&gt; is a six (rather than rolling a new die. Clearly, the individual probabilities are the same. However, these events are no longer independent. Intuitively, the success of Bet B (so the first roll is a 6) greatly increases the chance that Bet A succeeds (although it does not guarantee it). More precisely, the probability that both Bet A and Bet B occur is simply 1/18, as there are two possible rolls that are valid ([6,5] and [6,6]). However, \(1/18 &amp;gt; 1/72 = 1/6*1/12\). Thus, if the bookie provided an \$18 payout on this parlay, it would be break even for the bettor. If they provided the $72 pay out prescribed by the standard parlay formula, a bettor would print money in the long run.&lt;/p&gt;
&lt;p&gt;Correlated events still have a &amp;ldquo;correct&amp;rdquo; and fair price (in the example above, it was the substantially reduced price of $18). However, this computation required knowledge of their exact correlation. This is straightforward when it comes to the rolling of two dice, but difficult when it comes to the complex real world bets that bookies profit from. We can intuitively guess that the Super Bowl prop bet &amp;ldquo;Tom Brady throws for two or more touchdowns&amp;rdquo; is likely correlated with the bet &amp;ldquo;The Patriots (his team) win the Super Bowl&amp;rdquo;. In the world where we know Brady had a prolific scoring night, his team&amp;rsquo;s chances of winning are much higher (having essentially &amp;ldquo;removed&amp;rdquo; all of the worlds in which the Patriots offense was shut out, many of which were losing scenarios). But it is very difficult to estimate precisely how correlated those events are.&lt;/p&gt;
&lt;p&gt;Bookies prefer the simple solution: they do not offer correlated parlays. You can bet separately on these events, but they offer a singular pricing formula for parlays. Either the events are independent (in which case the probabilities are multicative, and they use their standard pricing formula), or they do not offer the parlay. In the world of sports, the divide between dependence and independence is usually fairly clear. Events in separate games on the same day should be independent. Events &lt;em&gt;within the same game&lt;/em&gt; tend not to be. There are correlations between a wide variety of events within the same game, even if it isn&amp;rsquo;t as obvious as &amp;ldquo;touchdowns scored&amp;rdquo; and &amp;ldquo;final result&amp;rdquo;. If a basketball team has twin stars, there will tend to be a slight negative correlation between their points scored, because there are a limited number of possessions and each shot taken by one player is a shot the other does not take. There are of course mitigating factors: a high scoring night for one player might indicate that the opposing defense is poor, or the two stars might both pass less to their supporting cast on nights when those role players are shooting poorly. But neither effect tends to outweigh the simple fact that there are a limited number of shots in the game. Bookies feel no obligation to attempt to set the correct price on these myriad combinations, and instead allow for parlays solely in the case of obviously plausible independence.&lt;/p&gt;
&lt;h1 id=&#34;parlays-in-political-elections&#34;&gt;Parlays in Political Elections&lt;/h1&gt;
&lt;p&gt;In practice, the statement &amp;ldquo;bookies do not allow for correlated parlays&amp;rdquo; is a sweeping generalization. Bookies make mistakes, and the &amp;ldquo;rake&amp;rdquo; that they take allows for a reasonable margin of error. It is better practice for them to consistently offer bets that attract bettors, than worry about the singular case in which they make a small mistake and suffer a loss. Bookies limit the amount that can be wagered at any given time, so even when they set an inaccurate line, they are not fleeced by sharp gamblers with deep pockets. It is always worth keeping an eye out for situations where bookies might slip up and offer such a profitable parlay.&lt;/p&gt;
&lt;p&gt;One such example came in the form of the 2018 midterm elections. Bookies offered bets on the results of individual races, general trends (&amp;ldquo;How many seats do Republicans gain in the Senate?&amp;quot;), and more. Some correlations are powerful and obvious. The result that Josh Hawley (Missouri), Dean Heller (Nevada), and Mike Braun (Indiana) win their Senate races is highly correlated with the GOP retaining control of the Senate. If they win those three close races, the odds that they somehow lose enough much more safe Senate seats for the Democrats to flip control is next to 0. I did not see any bookie foolish enough to allow parlays on these events. Generally, bookies are acutely aware that vertical structures like this (where one result is an aggregate of many individual results) have clear positive correlation.&lt;/p&gt;
&lt;p&gt;However, there was at least one bookie that allowed for parlays of &lt;em&gt;individual&lt;/em&gt; Senate races. For bookies accustomed to sports, this might seem sensible, as it is superficially similar to parlaying simultaneous game results. However, separate senate races can show obvious and consistent correlation. Some may dislike the language of this claim, as philisophically, it depends on your probabilistic interpretatin of a political race. It&amp;rsquo;s hard to see the connection between the senate choice of individual voters in Arizona and Nevada. However, under that framework, it&amp;rsquo;s hard to see how random chance enters the equation at all (voters are not flipping a coin at the ballot box, by and large). This ties into a deeper issue of how we interpret probabilistic forecasts, but my short answer would be that we use probability describe level of uncertainty about complex phenomena.&lt;/p&gt;
&lt;p&gt;Polling is the base staple of an election prediction. Even in the fantastical world where polls represent a perfectly random sampling of the entire voting population, there is the uncertainty that stems from the inherent randomness of such a random sample (luckily, this form of uncertainty is easy to mathematically model, and disappears as our sample gets very large). Then there is the uncertainty comes from the practical realities of imperfect polling, which crudely violates the simple assumptions taught in an introductory probability class (consider the work 538 does to grade pollsters as part of this uncertainty). And the final nebulous level of uncertainty comes from translating the results of the poll (which studies on a specific date who people say they plan to vote for) to the results of the election itself (determined by the choice of people who actually place a vote). These are two separate questions, and even if a poll precisely answers its own question, the translation of that to answer the second question can be clouded by the uncertainty of news that breaks after the poll is conducted, or voters who think it over some more and get cold feet, or a storm which prevents those without a car from making their way to the polling station, or any number of ways that these questions can differ. Election models have to grapple with these layers of uncertainty, and as a result a site like 538 reports that Ted Cruz has a &amp;ldquo;7 in 9&amp;rdquo; chance to beat Beto O&amp;rsquo;Rourke, and not complete certainty.&lt;/p&gt;
&lt;p&gt;The claim that election results are correlated fits neatly into this framework. It&amp;rsquo;s hard to define the connection between the individual decisions of voters in two states about two different pairs of candidates. But it&amp;rsquo;s easy to see how the errors in the probabilitic forecast would be connected. This is particularly true in the heavily partisan landscape of our current political system. The senate races in Nevada and Arizona aren&amp;rsquo;t entirely separate. In each of these demographically similar states, the populace is choosing between a democrat and a republican. Thus, certain forecasting errors made in one state tend to be mirrored in the other. The mistakes that pollsers make which could underestimate latino turnout will cause a similar divergence from the election forecasts and the final result in the two states.&lt;/p&gt;
&lt;p&gt;This implies that even in a world where the election outcomes are random according to the exact probabilities prescribed by the betting markets, a parlay offers a chance for profit. The positive correlation of these two results simply needs to outweigh the cost of the rake on these bets for these bets to have a positive expected value.&lt;/p&gt;
&lt;p&gt;We can illustrate this with the example of the bets I placed on the 2018 midterms. A significantly more complex and involved strategy could be used to profit from this parlaying opportunity, but this was just a fun example of the concept in practice.&lt;/p&gt;
&lt;h1 id=&#34;post-incomplete&#34;&gt;POST INCOMPLETE&lt;/h1&gt;
&lt;p&gt;To be continued&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Winning a Debate: Insights from &#34;Intelligence Squared&#34;</title>
      <link>/2018/07/29/winning-debate/</link>
      <pubDate>Sun, 29 Jul 2018 00:00:00 +0000</pubDate>
      <guid>/2018/07/29/winning-debate/</guid>
      <description>


&lt;p&gt;How does one measure the winner of a debate? If the debate itself is the focus, it’s wholly unsatisfying to just tally up the opinions of the viewers, as that solely reflects the prior beliefs of the audience. “Intelligence Squared” is an excellent podcast which teaches policy through moderated debate. A resolution is proposed (e.g. “Affirmative Action does more harm than good”), and two teams of two experts argue the issue in a structured setting. Many other programs aim for balance with a halfhearted attempt to pay lip service to the opposing side (without truly believing it), so it’s refreshing to hear passionate and informed argument from both sides of the aisle. If one side cites a misleading statistic, their opponents are ready to actually call them out on it, penalizing the lazy rhetorical tactics that plague discourse in an echo chamber.&lt;/p&gt;
&lt;p&gt;The focus of the program is the debate itself, but some nominal winner must still be declared. This is determined using a simple formula. The audience is polled before and after the debate (For, Against, or Undecided on the issue), and the side which gains the most votes is the winner. To be clear, I do not think that this method of declaring a winner is a particularly important to the show, nor do I think any listeners actually consider the swings of audience opinion to be the final say on the matter. But it’s still interesting to consider whether or not this is a reasonable way to declare a debate winner. And more broadly, what can the results of a debate tell us about the way that people make up their minds on polarizing issues? Much has been written about the politically charged climate of the present, and the ways in which the media we consume is entrenching us in our positions, leading to static and polarized political views.&lt;/p&gt;
&lt;p&gt;It turns out that Intelligence Squared doesn’t just measure the change in support for each side before and after the debate, for events since 2012 it also measured the individual shifts between each category (i.e. “Undecided before and For after”, or “For before and Against after”). This provides a small but insightful dataset looking at the ways in which minds can change on a polarizing issue in the course of an hour.&lt;/p&gt;
&lt;div id=&#34;what-does-it-mean-to-win-a-debate&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;What does it mean to “win” a debate?&lt;/h3&gt;
&lt;p&gt;Intelligence Squared declares their nominal winner with a simple metric, measuring whether the For or Against side had the largest absolute percentage point shift in support from the pre debate to the post debate audience poll. Thus, if the For side originally had 40% support and the Against side 35% support before the debate, and after the debate the For side had 45% support and the Against side had 43% support, the 8 percentage point increase of the Against side trumps the 5 percentage point increase of the For side, and the Against side wins the night. We will refer to this method for determining a victory as the ISM (Intelligence Squared Metric).&lt;/p&gt;
&lt;p&gt;The problem with this approach stems from the fact that &lt;em&gt;there are a variety of ways that opinions could shift between the three camps&lt;/em&gt;. This is related to both its usage of absolute percentage point change (with no reference to the relative switch), as well as the presence of the undecided voters.&lt;/p&gt;
&lt;p&gt;I stumbled upon &lt;a href=&#34;https://stats.stackexchange.com/a/94742&#34;&gt;this&lt;/a&gt; excellent post by &lt;em&gt;whuber&lt;/em&gt; on StackExchange. I have the pleasure of knowing &lt;em&gt;whuber&lt;/em&gt; in person, and in characteristic fashion he gives a thorough and insightful analysis of the ways in which we can have different shifts in support (among the different groups) for the same absolute result, in ways that make determining the winner quite difficult. We cite a simple example proposed by &lt;em&gt;whuber&lt;/em&gt; in this post.&lt;/p&gt;
&lt;p&gt;Consider a situation where originally 20% are For, 60% are Against, and 20% are undecided (we will write this as a vector &lt;span class=&#34;math inline&#34;&gt;\((.2, .6, .2)\)&lt;/span&gt;). After the debate, the vector of support is &lt;span class=&#34;math inline&#34;&gt;\((.3, .4, .3)\)&lt;/span&gt;. Under the ISM, this is a clear and decisive win for the For side, as they gained 10 percentage points of support, while the Against side lost 20 percentage points. However, there are a variety of between-group opinion switches that could lead to this result. &lt;em&gt;whuber&lt;/em&gt; suggests that we write these between-group switches in a 3x3 transition matrix, where the &lt;span class=&#34;math inline&#34;&gt;\(ij\)&lt;/span&gt;th element represents the percent of the original supporters for the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;th camp before the debate are supporters of the &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;th camp after the debate (with For as 1, Against as 2, and Undecided as 3). Then, this represents a plausible transition matrix for our final result.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathbb{A} = \left(
\begin{array}{ccc}
 0.32 &amp;amp; 0.29 &amp;amp; 0.32 \\
 0.36 &amp;amp; 0.42 &amp;amp; 0.36 \\
 0.32 &amp;amp; 0.29 &amp;amp; 0.32 \\
\end{array}
\right).
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;As &lt;em&gt;whuber&lt;/em&gt; explains,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Here, 36% of the “Fors” changed to the other side while only 29% of the “Against” changed to the opposite opinion. Moreover, slightly more of the undecideds (36%) vs 32%) came out “against” rather than for. Although their numbers in this audience decreased, we have a situation (reminiscent of Simpson’s Paradox) in which the “Against” faction clearly won the debate!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We have a possible outcome where Intelligence Squared would declare the “For” side a decisive winner, but in terms of percentage shifts, the “Against” side was more persuasive at both convincing Undecideds to join their side, &lt;em&gt;and&lt;/em&gt; convincing those with a prior opinion to change their viewpoint. This clarifies the challenge inherent in determining the winner of a debate. When one side is initially quite unpopular, they have a larger population of possible voters that they can woo, which makes their proportional gains much higher in absolute terms. There’s certainly a plausible argument that absolute shifts in support should be the most important factor in determining a winner, but this does make the situation more complex. And it gets even messier in cases where the sub-group switches don’t necessarily agree. We might see one side sway a much larger proportion of the Undecided voters, while being less successful at convincing those from the other side to switch. This could leave us without a conclusive winner.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-has-this-worked-in-practice&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;How has this worked in practice?&lt;/h3&gt;
&lt;p&gt;Looking at the results of the Intelligence Squared debates in reality, we see that the majority of results are uncontroversial. and there is little doubt which side was more convincing. In total, we gather the results from 88 debates from the Intelligence Squared &lt;a href=&#34;https://www.intelligencesquaredus.org/debates&#34;&gt;website&lt;/a&gt;, with the process described &lt;a href=&#34;/2018/07/26/scraping-is2/&#34;&gt;here&lt;/a&gt;, and the unpolished scraped dataset &lt;a href=&#34;https://github.com/dylanpotteroconnell/IntelSquaredProject/blob/master/votingresultsfinal.csv&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The first winning metric we consider is the Intelligence Squared Metric (ISM), which measures shift in absolute percentage of support. We next consider the Undisputed Metric (UDM), which uses the proportional switching outlined above, and only assigns a winner when one side &lt;em&gt;both&lt;/em&gt; convinces a greater proportion of Undecided voters to join their side, as well as a greater proportion of the other side to switch their opinion. In cases when these do not agree, no winner is declared.&lt;/p&gt;
&lt;p&gt;Thus the first question is whether there are cases where the UDM declares a winner which is the reverse of the ISM, like the toy example outlined above by &lt;em&gt;whuber&lt;/em&gt;. It turns out that this is &lt;em&gt;not&lt;/em&gt; an idle concern, and in fact there have been six debates with this conflicting result.&lt;/p&gt;
&lt;!-- \begin{table}[h] --&gt;
&lt;!--     \centering --&gt;
&lt;!--     \begin{tabular}{|l|l|l|l|l|l|l|l|l|} --&gt;
&lt;!--     \hline --&gt;
&lt;!--         Title &amp; Date &amp; Pre \%: For &amp; Pre \%: Against &amp; Pre \%: Undecided &amp; Abs. \% \$$\backslash$Delta\$: For &amp; Abs. \% \$$\backslash$Delta\$: Against &amp; ISM Winner &amp; UDM Winner \\ \hline --&gt;
&lt;!--         Trigger Warning: Safe Spaces Are Dangerous &amp; 06/23/2018 &amp; 57 &amp; 25 &amp; 18 &amp; -1 &amp; 10 &amp; Against &amp; For \\ \hline --&gt;
&lt;!--         Preserve Net Neutrality: All Data is Created Equal &amp; 04/17/2018 &amp; 60 &amp; 23 &amp; 17 &amp; 0 &amp; 8 &amp; Against &amp; For \\ \hline --&gt;
&lt;!--         Legalize Assisted Suicide &amp; 11/03/2014 &amp; 63 &amp; 11 &amp; 26 &amp; 4 &amp; 11 &amp; Against &amp; For \\ \hline --&gt;
&lt;!--         Break up the Big Banks &amp; 10/16/2013 &amp; 35 &amp; 19 &amp; 46 &amp; 13 &amp; 20 &amp; Against &amp; For \\ \hline --&gt;
&lt;!--         Cutting the Pentagon&#39;s Budget is a Gift to our Enemies &amp; 06/19/2013 &amp; 20 &amp; 58 &amp; 22 &amp; 9 &amp; 8 &amp; For &amp; Against \\ \hline --&gt;
&lt;!--         The GOP must Seize the Center or Die &amp; 04/17/2013 &amp; 63 &amp; 14 &amp; 23 &amp; 2 &amp; 14 &amp; Against &amp; For \\ \hline --&gt;
&lt;!--     \end{tabular} --&gt;
&lt;!-- \end{table} --&gt;
&lt;style type=&#34;text/css&#34;&gt;
.tg  {border-collapse:collapse;border-spacing:0;border-color:#ccc;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:#ccc;color:#333;background-color:#fff;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:#ccc;color:#333;background-color:#f0f0f0;}
.tg .tg-yp4a{border-color:#333333;vertical-align:top}
.tg .tg-us36{border-color:inherit;vertical-align:top}
.tg .tg-fraq{color:#333333;border-color:#000000;vertical-align:top}
.tg .tg-dvpl{border-color:inherit;text-align:right;vertical-align:top}
&lt;/style&gt;
&lt;table class=&#34;tg&#34;&gt;
&lt;tr&gt;
&lt;th class=&#34;tg-us36&#34;&gt;
Title
&lt;/th&gt;
&lt;th class=&#34;tg-us36&#34;&gt;
Date
&lt;/th&gt;
&lt;th class=&#34;tg-yp4a&#34;&gt;
Pre %: For
&lt;/th&gt;
&lt;th class=&#34;tg-fraq&#34;&gt;
Pre %: Against
&lt;/th&gt;
&lt;th class=&#34;tg-yp4a&#34;&gt;
Pre %: Undecided
&lt;/th&gt;
&lt;th class=&#34;tg-dvpl&#34;&gt;
Abs. % &lt;span class=&#34;math inline&#34;&gt;\(\Delta\)&lt;/span&gt;: For
&lt;/th&gt;
&lt;th class=&#34;tg-dvpl&#34;&gt;
Abs. % &lt;span class=&#34;math inline&#34;&gt;\(\Delta\)&lt;/span&gt;: Against
&lt;/th&gt;
&lt;th class=&#34;tg-us36&#34;&gt;
ISM Winner
&lt;/th&gt;
&lt;th class=&#34;tg-us36&#34;&gt;
UDM Winner
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-us36&#34;&gt;
Trigger Warning: Safe Spaces Are Dangerous
&lt;/td&gt;
&lt;td class=&#34;tg-us36&#34;&gt;
06/23/2018
&lt;/td&gt;
&lt;td class=&#34;tg-yp4a&#34;&gt;
57
&lt;/td&gt;
&lt;td class=&#34;tg-fraq&#34;&gt;
25
&lt;/td&gt;
&lt;td class=&#34;tg-yp4a&#34;&gt;
18
&lt;/td&gt;
&lt;td class=&#34;tg-dvpl&#34;&gt;
-1
&lt;/td&gt;
&lt;td class=&#34;tg-dvpl&#34;&gt;
10
&lt;/td&gt;
&lt;td class=&#34;tg-us36&#34;&gt;
Against
&lt;/td&gt;
&lt;td class=&#34;tg-us36&#34;&gt;
For
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-us36&#34;&gt;
Preserve Net Neutrality: All Data is Created Equal
&lt;/td&gt;
&lt;td class=&#34;tg-us36&#34;&gt;
04/17/2018
&lt;/td&gt;
&lt;td class=&#34;tg-yp4a&#34;&gt;
60
&lt;/td&gt;
&lt;td class=&#34;tg-fraq&#34;&gt;
23
&lt;/td&gt;
&lt;td class=&#34;tg-yp4a&#34;&gt;
17
&lt;/td&gt;
&lt;td class=&#34;tg-dvpl&#34;&gt;
0
&lt;/td&gt;
&lt;td class=&#34;tg-dvpl&#34;&gt;
8
&lt;/td&gt;
&lt;td class=&#34;tg-us36&#34;&gt;
Against
&lt;/td&gt;
&lt;td class=&#34;tg-us36&#34;&gt;
For
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-us36&#34;&gt;
Legalize Assisted Suicide
&lt;/td&gt;
&lt;td class=&#34;tg-us36&#34;&gt;
11/03/2014
&lt;/td&gt;
&lt;td class=&#34;tg-yp4a&#34;&gt;
63
&lt;/td&gt;
&lt;td class=&#34;tg-fraq&#34;&gt;
11
&lt;/td&gt;
&lt;td class=&#34;tg-yp4a&#34;&gt;
26
&lt;/td&gt;
&lt;td class=&#34;tg-dvpl&#34;&gt;
4
&lt;/td&gt;
&lt;td class=&#34;tg-dvpl&#34;&gt;
11
&lt;/td&gt;
&lt;td class=&#34;tg-us36&#34;&gt;
Against
&lt;/td&gt;
&lt;td class=&#34;tg-us36&#34;&gt;
For
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-us36&#34;&gt;
Break up the Big Banks
&lt;/td&gt;
&lt;td class=&#34;tg-us36&#34;&gt;
10/16/2013
&lt;/td&gt;
&lt;td class=&#34;tg-yp4a&#34;&gt;
35
&lt;/td&gt;
&lt;td class=&#34;tg-fraq&#34;&gt;
19
&lt;/td&gt;
&lt;td class=&#34;tg-yp4a&#34;&gt;
46
&lt;/td&gt;
&lt;td class=&#34;tg-dvpl&#34;&gt;
13
&lt;/td&gt;
&lt;td class=&#34;tg-dvpl&#34;&gt;
20
&lt;/td&gt;
&lt;td class=&#34;tg-us36&#34;&gt;
Against
&lt;/td&gt;
&lt;td class=&#34;tg-us36&#34;&gt;
For
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-us36&#34;&gt;
Cutting the Pentagon’s Budget is a Gift to our Enemies
&lt;/td&gt;
&lt;td class=&#34;tg-us36&#34;&gt;
06/19/2013
&lt;/td&gt;
&lt;td class=&#34;tg-yp4a&#34;&gt;
20
&lt;/td&gt;
&lt;td class=&#34;tg-fraq&#34;&gt;
58
&lt;/td&gt;
&lt;td class=&#34;tg-yp4a&#34;&gt;
22
&lt;/td&gt;
&lt;td class=&#34;tg-dvpl&#34;&gt;
9
&lt;/td&gt;
&lt;td class=&#34;tg-dvpl&#34;&gt;
8
&lt;/td&gt;
&lt;td class=&#34;tg-us36&#34;&gt;
For
&lt;/td&gt;
&lt;td class=&#34;tg-us36&#34;&gt;
Against
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-us36&#34;&gt;
The GOP must Seize the Center or Die
&lt;/td&gt;
&lt;td class=&#34;tg-us36&#34;&gt;
04/17/2013
&lt;/td&gt;
&lt;td class=&#34;tg-yp4a&#34;&gt;
63
&lt;/td&gt;
&lt;td class=&#34;tg-fraq&#34;&gt;
14
&lt;/td&gt;
&lt;td class=&#34;tg-yp4a&#34;&gt;
23
&lt;/td&gt;
&lt;td class=&#34;tg-dvpl&#34;&gt;
2
&lt;/td&gt;
&lt;td class=&#34;tg-dvpl&#34;&gt;
14
&lt;/td&gt;
&lt;td class=&#34;tg-us36&#34;&gt;
Against
&lt;/td&gt;
&lt;td class=&#34;tg-us36&#34;&gt;
For
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;This is not surprising when the absolute percentage changes are so close (such as in “Cutting the Pentagon’s budget…”), but the other debates show some wildly divergent results. We even see an example of &lt;em&gt;whuber&lt;/em&gt;’s hypothesized situation. In the “Trigger Warnings” debate on 06/23/2018, we see the For camp lose support, the Against camp gain support, and yet we see that the For camp was declared the winner by the UDM. The transition matrix for this debate is shown here&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Trigger Warnings Debate: }\left(
\begin{array}{ccc}
0.67 &amp;amp; 0.44 &amp;amp; 0.39 \\ 
0.26 &amp;amp; 0.56 &amp;amp; 0.33 \\ 
0.07 &amp;amp; 0.00 &amp;amp; 0.28 \\ 
\end{array}
\right).
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The For camp was able to lure 39% of the Undecided voters, compared to only 33% by the Against camp. And the For side was able to convince 44% of the Against camp to change their minds, while the Against camp convinced only 26% of the For camp to swap. And yet the For camp &lt;em&gt;lost&lt;/em&gt; support, while the Against camp gained support. This can easily be explained by the initial disparity in popularity. The For side began with 57% of the vote, so while they convinced a larger proportion of Undecided and Against voters to change their view, they had a minority of the audience who were even available to sway.&lt;/p&gt;
&lt;p&gt;This is a perfect example of the issue, because it feels fundamentally wrong to declare the winner of the debate to be a side that &lt;em&gt;lost&lt;/em&gt; popularity during the course of the night. This cousin of Simpson’s Paradox defies our intuition, where we want relative and absolute changes to point in the same direction. I think there’s an argument for both sides. The issue with using the UDM (with subgroup proportional shifts) is that there’s an inherent expectation that a debate will shift opinions towards the center. If one group starts overwhelmingly popular, and both sides sway the proportional amount of people to their side, then that side will tend to lose share. It doesn’t make much sense that we would expect any evenly matched debate to bring public opinion towards a 50/50 split, but a series of dead even debates (as measured by the UDM) would push the audience towards that equilibrium point. And yet, we think of the goal of debates as being one of persuasion. And so it makes little sense to declare one side the winner because they had a larger possible audience to persuade, even if their marginal rate of persuasion was inferior.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;are-there-obvious-issues-in-the-original-metric&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Are there obvious issues in the original metric?&lt;/h3&gt;
&lt;p&gt;There is no singular way to measure the efficacy of these metrics, and it ultimately comes down to our subjective intuition about what it means to “win” a debate. However, we can reasonably agree on some criteria that we intuitively find “fair”. Generally, we want a debate to be a plausibly even playing field from the start. An even more intuitive metric would simply be the popularity of each motion after the debate (which we refer to as the Final Popularity Metric [FPM]), but we reject this approach because it so obviously favors the side that is initially popular with the audience. It’s then worth it to consider whether the ISM is biased in ways that clash with our intuition.&lt;/p&gt;
&lt;p&gt;The most obvious plausible bias is that of the initial popularity of each position. In fact, this is why we do not simply use the FPM , so it is perhaps instructive to consider what such bias looks like. We plot the initial popularity of the For and Against positions below, and use color to denote the final winner by the FPM (i.e. which position was ultimately more popular with the audience). Exactly as we’d expect, there is a heavy bias to the initial starting positions. There are a few instances where a particularly large swing allows for the initially unfavorable position to pull out a win, but generally the winner is determined simply by initial popularity, and the processs of the debate itself is largely irrelevant.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;IQ2_FPresults.png&#34; /&gt;&lt;/p&gt;
&lt;!-- ![OriginalResults]({{ &#34;/images/IQ2_FPresults.png&#34;}}) --&gt;
&lt;p&gt;When we consider the same plot colored by the standard ISM results, we see a much more promising picture. On first glance, this adjustment does a plausible job of removing the bias from initial popularity. We theorized that the ISM might be punishing to sides that begin with overwhelming popularity, becuase they have so many fewer people that they can convert to their side (and an increase from 90 to 95 percent popularity might only be 5 percentage points, but represents a massive 50% decrease in those who don’t support that side). However, in practice, most positions start at a popularity between 20% and 50%, so those extreme examples just aren’t as concerning. And within that range, the naked eye doesn’t detect any overwhelming patterns of bias.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;IQ2_winnerorigresults.png&#34; /&gt;&lt;/p&gt;
&lt;!-- ![OriginalResults]({{ &#34;/images/IQ2_winnerorigresults.png&#34;}}) --&gt;
&lt;p&gt;One plausible trend that might stand out lies in the bottom right of the chart, where we do note that debates with an initially extremely high For share tended to be won by the Against side. Of the seven debates where the For side began with at least 60% vote share, the Against side won six contests and For won only one. This is a very small sample, but it does fit our intuition, and thus the lack of bias might simply stem from the fact that it is only observed in the extreme starting positions, which occur rarely. We do note that the reverse extreme (where the Against side has initally dominant popularity) does not show any such obvious trend, which makes this slightly less troubling.&lt;/p&gt;
&lt;p&gt;The prior plot only considers the binary outcome under the ISM, but we are not solely concerned with the effect on the binary result, but rather the effect on percentage point change, which can manifest in a shift of binary result. Thus, it must be more illsutrative to break down the effect further. We create 4 bins of debates based on a partition of the initial popularity of the For camp. For each bin, we consider a box plot of the percentage change that the debate creates for the For camp along with the percentage change for the Against camp, which provides a more precise analysis. As before, the picture is muddled until we consider the final bin (debates where the For side enjoyed a 55% or more share of the support), where almost all of the changes to the For side were lesser than the changes for the Against side. This is a much more persuasively troubling result than the simple results observed above, but the sample remains small (only 11).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;IQ2_partitionvotechange.png&#34; /&gt;&lt;/p&gt;
&lt;!-- ![PartitionVoteShare]({{ &#34;/images/IQ2_partitionvotechange.png&#34;}}) --&gt;
&lt;p&gt;We observe a peculiar result, but are unsure whether it could simply result from random chance. This question of statistical significance is at the heart of the study of statistics. However, it is not immediately clear to me which tool is suited to the situation. A simple comparison of the mean changes of these different bins seems deeply problematic. It is unclear that we can make any plausible assumptions about the distribution of the changes of votes. Crucially, the changes to the For vote share are &lt;em&gt;not&lt;/em&gt; independent to the changes to the Against vote share. And there is a clear confounding variable, where change to For vote share is going to be effected not simply by initial For vote, but by the initial number of undecideds (we certainly have not proven this, but common sense says that an undecided voter is easier to persuade than one who believes in the opposing viewpoint). More broadly, any such test from here is problematic in the way that Exploratory Data Analysis tends to be. We have not rigorously predefined our research question based on a scientific hypothesis, we are simply confirming our vague intuition that starting vote share introduces bias. Thus, a test based on this particular binning creates some form of bias. If we explored deep enough into the data, eventualy we must have found something similarly intriguing.&lt;/p&gt;
&lt;p&gt;We can use a nonparametric test to examine the implausibility of the observed trend, but we must keep the bias mentioned above closely in mind. Exploratory data analysis minimizes this problem by relying on standardized statistical tests which answer broad questions. A nonparametric test that examines the statistical implausibility of the difference observed in the arbitrary binning we have chosen above is far less robust. An example statistical test is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test&#34;&gt;Kruskal-Wallis Rank test&lt;/a&gt;. This nonparametric test simply considers the ranked values of the observed data, and considers the statistical probability that we might observe such an extreme disparity as what occurs in our sample. This provides a statistically rigorous result without reliance on assumptions, but the issue is that it provides such an answer to a deeply narrow &lt;em&gt;question&lt;/em&gt;. In our case, we could consider the Kruskal-Wallis test with the changes to the For vote for two groups: those with initial share of 55+%, and those with lesser initial share. In this case, we observe a test statistic of 8.1929, and a p-value of 0.42%. Phrased in statistical language, this means that if our two groups generated changes to the For vote share with the same distribution, there would be less than a one in two hundred chance that we observe at least as large a disaparity in the ranks of the observed values.&lt;/p&gt;
&lt;p&gt;The problem is that the question this answers is deceptively narrow. It cannot make any bold claims about &lt;em&gt;any&lt;/em&gt; debates that begin with 55+% vote share or fewer, but only the ones which we observe. If we had an ideal sample of such debates, and considered those above and below this threshold, then such a statement would be extremely powerful. Instead, our initial sample size of 11 debates means it is quite plausible that there are other fundamental differences in these samples.&lt;/p&gt;
&lt;p&gt;In short, the Kruskal-Wallis test demonstrates the statistical significance in the difference of the distribution of these samples, but makes no claim as to why. And we have little conclusive proof that it is the prior vote share of the For side that creates this disparity, it could be any number of differences between the populations that provides the root cause.&lt;/p&gt;
&lt;!--
We can stick to a nonparametric test to sidestep of our lack of reasonable assumptions, but we must keep the above concerns closely in mind during this work. This will provide a useful benchmark for the improbability of the observed result, but there will be subjective choices made in the construction of our test (as there is no singular way to formulate our hypothesis). We must simply make reasonable choices, and keep this issue in mind (an issue at the heart of any statistical test conducted after exploratory data analysis, albeit one that is significantly less concerning when the test being used is cookie cutter and not subjectively defined by the examiner). Such 
--&gt;
&lt;p&gt;This is an obviously unsatisfying result, but in the name of intellectual honesty I hesitate to any bolder claim. Qualitatively, I do think that the sum of this analysis makes it very likely that our intuition that those with an initially high For vote share experience notable disadvantage for winning the debate under the metric used by Intelligence Squared. However, these extreme cases come up very rarely, and the ISM seems quite defensible for the purposes of the podcast.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;alternative-metrics&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Alternative Metrics&lt;/h3&gt;
&lt;p&gt;It is of course interesting to consider other metrics that one could use. Outlined above, we consider the Undisputed Metric (UDM), which instead considers the percentage that switch between each camp during the debate. The UDM declares a winner if that side converts both a greater percentage of the other side to swap, as well a greater percentage of the Undecided camp to join theirs. In cases where there is disagreement (e.g. the For side convinces a greater proportion of Against voters to join their side than vice versa, but the Against side convinces a greater proportion of Undecided voters), the UDM simply declares that there is no clear winner. The UDM operates under a fundamentally different mindset: that what matters is not the percentage point change, but the proportion of voters who you sway. As noted above, it is perfectly possible for the UDM to declare the opposite winner to the ISM (this is particularly likely to happen in those cases where one side has initially extreme vote share). We first consider another plot of the debate result under the UDM compared to the initial For and Against vote shares.&lt;/p&gt;
&lt;!-- ![UDM Box Plot](IQ2_winnerUDMresults.png) --&gt;
&lt;p&gt;&lt;img src=&#34;IQ2_winnerUDMresults.png&#34; /&gt;&lt;/p&gt;
&lt;!-- ![]({{ &#34;/images/IQ2_winnerUDMresults.png&#34;}}) --&gt;
&lt;p&gt;Similar to the ISM, under the UDM it seems the initial popularities are overly determinative of the final result. But we do observe something of a reverse trend: if an initial side has at least 50% share of the starting vote, now they nearly always win. We again break down this effect by considering the voting shifts for debates binned by their initial For vote share. Now, our object of interest is not simply the percentage point change, but a variety of changes (in this plot, we consider the proportion of initially Against voters who shift to For, Undecided persuaded to join the For camp, and initially For voters who defect to the against side). Note that the coloring is done using our initial For popularity partition, unlike above.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;IQ2_UDMboxplots.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This reflects the effects expected from our initial results. Initially high For popularity tends to imply that they will persuade a higher proportion of Against voters, and that a lower proportion of For voters will defect to Against, while the results for Undecided voters are decidedly muddled. This is what we would expect. A high initial For vote share implies a lower Against vote share, so converting a small number of Against voters shows a lower percentage point change, but a large proportional change. The movement of undecided voters does not follow this pattern.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Summary&lt;/h3&gt;
&lt;p&gt;The question of which side is the winner in a debate is far from straightforward. Our intuition tells us that it should depend on the performance of the debaters, and not simply reflect the initial position of the audience. This is why we do not conduct a simple exit poll on the popularity of each side, which the data shows is massively biased towards the initial popularity of the sides. Unsurprisingly, audience members have extremely strong priors on these hotly contested issues, and an hour of debate does not ovewhelm a lifetime of thought on the matter. Intelligence Squared adjusts for this bias by polling the audience before the debate, and considering the percentage point change that each side observe. We hypothesize that this may instead be biased to disfavor initially popular sides. This appears to occur in the observed data, but extreme initial popularity is relatively rare, so it is hard to precisely prove the magnitude of this effect. In fact, the data shows that this metric seems to be a reasonably fair benchmark in practice. We outline one alternative for determining the winner, which is to consider the proportional changes among each sides. Counterintuitively, it is perfectly possible for one side in a debate to lose under the Intelligence Squared system, and yet persuade a greater proportion of both Undecideds and the other side to defect. In fact, we observe this scenario in a number of debates that occur. In practice, following this metric appears to bias results in the opposite manner, where initially popular sides are again favored (although in a similarly plausibly fair way in all but extreme cases). It is worth keeping in mind that the metric used by Intelligence Squared is but one reasonable choice, and while it serves their purposes well, it should not be considered the ultimate determination of efficacy in debate. Luckily, no one would doubt that the experience of the debate is what matters, and the mark of a winner is simply a superficial afterthought.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Scraping the &#34;Intelligence Squared&#34; Debate Results</title>
      <link>/2018/07/26/scraping-is2/</link>
      <pubDate>Thu, 26 Jul 2018 00:00:00 +0000</pubDate>
      <guid>/2018/07/26/scraping-is2/</guid>
      <description>&lt;p&gt;This is a brief companion to the 
&lt;a href=&#34;https://dylanpotteroconnell.github.io/debatefinalsummary/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;post&lt;/a&gt; analyzing the methods of assigning a winner to a debate, using the Intelligence Squared dataset. I will briefly outline here how I assemble that dataset, for trasparency.&lt;/p&gt;
&lt;h3 id=&#34;compiling-the-pages&#34;&gt;Compiling the Pages&lt;/h3&gt;
&lt;p&gt;The results from each Intelligence Squared debate are posted online in pages such as 
&lt;a href=&#34;https://www.intelligencesquaredus.org/debates/globalization-has-undermined-americas-working-class&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this&lt;/a&gt;, including video of the debate, a description of the major positions of each side, the qualifications of the debaters, and most importantly, the results of the audience polling. Unfortunately, there doesn’t seem to be a central hub page that neatly lists all the URLs. However, the desired dataset isn’t huge (about 90 total debates), so there’s no substitute for the occasional work simply manually trawling through the website, and recording the date, name, and URL of each debate in question.&lt;/p&gt;
&lt;h3 id=&#34;scraping-the-numbers&#34;&gt;Scraping the numbers&lt;/h3&gt;
&lt;p&gt;Once we have a full list of all the relevant URLs, luckily, the results themselves are generally presented in a consistent format. Thus,  some simple work with regular expressions gathers the data that we need. One such example.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;debate_vote_results={&amp;quot;live&amp;quot;:{&amp;quot;pre&amp;quot;:{&amp;quot;f&amp;quot;:36,&amp;quot;a&amp;quot;:45,&amp;quot;u&amp;quot;:19,&amp;quot;w&amp;quot;:&amp;quot;a&amp;quot;},&amp;quot;post&amp;quot;:{&amp;quot;f&amp;quot;:32,&amp;quot;a&amp;quot;:61,&amp;quot;u&amp;quot;:7,&amp;quot;w&amp;quot;:&amp;quot;a&amp;quot;},&amp;quot;s&amp;quot;:41,&amp;quot;t&amp;quot;:100,&amp;quot;f&amp;quot;:{&amp;quot;f&amp;quot;:18,&amp;quot;a&amp;quot;:15,&amp;quot;u&amp;quot;:3},&amp;quot;a&amp;quot;:{&amp;quot;f&amp;quot;:6,&amp;quot;a&amp;quot;:38,&amp;quot;u&amp;quot;:1},&amp;quot;u&amp;quot;:{&amp;quot;f&amp;quot;:8,&amp;quot;a&amp;quot;:8,&amp;quot;u&amp;quot;:3}},&amp;quot;online&amp;quot;:{&amp;quot;t&amp;quot;:100,&amp;quot;pre&amp;quot;:{&amp;quot;f&amp;quot;:50,&amp;quot;a&amp;quot;:35,&amp;quot;u&amp;quot;:15,&amp;quot;w&amp;quot;:&amp;quot;&amp;quot;},&amp;quot;post&amp;quot;:{&amp;quot;f&amp;quot;:44,&amp;quot;a&amp;quot;:50,&amp;quot;u&amp;quot;:6,&amp;quot;w&amp;quot;:&amp;quot;&amp;quot;},&amp;quot;f&amp;quot;:{&amp;quot;f&amp;quot;:35,&amp;quot;a&amp;quot;:13,&amp;quot;u&amp;quot;:2},&amp;quot;a&amp;quot;:{&amp;quot;f&amp;quot;:4,&amp;quot;a&amp;quot;:27,&amp;quot;u&amp;quot;:4},&amp;quot;u&amp;quot;:{&amp;quot;f&amp;quot;:6,&amp;quot;a&amp;quot;:10,&amp;quot;u&amp;quot;:0}}};
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In case anyone wants to borrow this sort of simple scrape for their own projects, you can find the code 
&lt;a href=&#34;https://github.com/dylanpotteroconnell/IntelSquaredProject/blob/master/intelsquareddata.R&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;, although the approach is extremely messy. Luckily, with R it’s more important to be fast than it is to be clean, and you can use very awkward code as long as you find it readable and clear. Regular expressions like this can grab the relevant numbers that we need, and we store it in one large data frame.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;gsub(&amp;quot;.*\&amp;quot;f\&amp;quot;:\\{\&amp;quot;f\&amp;quot;:(\\d+\\.*\\d*),\&amp;quot;a\&amp;quot;:(\\d+\\.*\\d*),\&amp;quot;u\&amp;quot;:(\\d+\\.*\\d*).*&amp;quot;, &amp;quot;\\1 \\2 \\3&amp;quot;, post)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where we identify the numbers that we’re interested in. With the numbers compiled into one large data frame (which can be viewed in raw form 
&lt;a href=&#34;https://github.com/dylanpotteroconnell/IntelSquaredProject/blob/master/votingresultsfinal.csv&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;, for those interested in examining the data themselves). In total, there are 88 debates stretching back to 2012 which have all the information needed. The program itself stretches back further, but they only began tracking the subgroup movements more recently.&lt;/p&gt;
&lt;p&gt;In the next post, we can actually dive into the data itself.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Privacy Policy</title>
      <link>/privacy/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 +0100</pubDate>
      <guid>/privacy/</guid>
      <description>&lt;p&gt;&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Terms</title>
      <link>/terms/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 +0100</pubDate>
      <guid>/terms/</guid>
      <description>&lt;p&gt;&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>World Cup Elo Part 3: Results of Toy Model for Group Stage</title>
      <link>/2018/06/01/worldcup-3/</link>
      <pubDate>Fri, 01 Jun 2018 00:00:00 +0000</pubDate>
      <guid>/2018/06/01/worldcup-3/</guid>
      <description>


&lt;p&gt;&lt;em&gt;Part 1 can be found &lt;a href=&#34;/2017/12/05/worldcup-1&#34;&gt;here&lt;/a&gt;, and Part 2 &lt;a href=&#34;/2017/12/10/worldcup-2/&#34;&gt;here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Over the winter, I built a straightforward Elo rating system to model soccer/football national teams during the World Cup. Elo rating systems are used all around us, but most people never bother to delve into the details of how they are constructed, which makes it difficult to draw precise inferences, so this is a useful exercise. Now that the group stage of the 2018 World Cup has wrapped up, I’ll take a quick look at how the Elo rating system performed if it were used as a betting system..&lt;/p&gt;
&lt;div id=&#34;major-caveats&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Major Caveats&lt;/h3&gt;
&lt;p&gt;The resulting Elo model should not be considered truly predictive. It operates with a huge restriction, utilizing only the data from past World Cups and the qualifying rounds (ignoring other international competitions). It also doesn’t draw in any outside information such as injury news, or player retirements. Clearly, we could improve the predictive ability of our model by including more information. The advantage of this simple approach is that the resulting model is readily interpretable. It simply represents our best estimate of a team’s strength based on their World Cup and qualifying results, and seeks to quantify those results in the most accurate way possible (because simply noting the wins and losses is woefully insufficient). Incorporating injury information is valuable, but there’s no straightforward way to do it, and it would be a very subjective process. Thus, we could improve the specific predictions of our model, but we would lose the ability to properly interpret the results. Instead, I think it’s more valuable to have a model that has a narrow but precise use. This provides a foundation which we can use as a baseline for our own subjective analysis (such as the loss of players to injury, or rumors about a team being poorly prepared, or not giving it their all during qualifying matches).&lt;/p&gt;
&lt;p&gt;Even with this narrow approach, there are a vast number of decisions needed to implement an Elo rating system. While we try our best to leave subjective analysis outside of our model, many of these decisions do not have a clear right answer. In a past post (linked here), we outline some of these decisions. These range from home field advantage, the structure of temporal weighting of results (i.e. the “K factor”), and whether to include margin of victory rather than match result. We selected our final parameters using a variety of validation techniques, performing a search of the parameter space and comparing how predictive our model is along the way (compared to the observed results tempered with betting markets). This is just a brief summary, expanded upon in other posts, the point is that we result in an implementation of the Elo rating system that performs well for the peculiar situation of football national teams in the World Cup.&lt;/p&gt;
&lt;p&gt;This preamble leads to the fact that the group stage just wrapped up, and it’s interesting to take a look at the results. Now, simply comparing the actual match result to the percentage chance we predicted for that result isn’t terribly insightful. Our model will always select the “favorite”, so this would just tally up the total number of upsets. In fact, as a draw in football is essentially always less likely than the stronger team winning, this means that we would consider every draw to be an “incorrect” prediction. What we want to quantify is how shocking our model found those upsets relative to other norms.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;evaluating-its-performance&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Evaluating its Performance&lt;/h3&gt;
&lt;p&gt;The natural way to evaluate the Elo rating model is to consider the results of using it to blindly bet on the matches. Again, it cannot be said enough that there’s little reason to think that the raw model would be an accurate tool to beat the betting markets.Bettors take into account all information available, and our model solely uses past results. However, it’s a useful measure to see whether these predictions are at least plausible. Our final caveat here is that a null result doesn’t mean all that much. If the betting markets were perfectly efficient, picking bets at random would be just as effective as any other strategy. Obviously, betting markets are &lt;em&gt;not&lt;/em&gt; perfectly efficient, they represent our best collective wisdom, which may well be misguided. But it’s important to remember that we would be surprised to find any betting strategy result in huge losses over a long period. The most telling “sanity check” of our model is that by and large, the odds it estimates are quite similar to the implied odds of the betting markets. This shows that the Elo rating system is at least a plausible model of team strength, and that it should be a valid starting point for subjective amendment (taking into account other factors beyond past team performance).&lt;/p&gt;
&lt;p&gt;Our simple betting game goes as follows. For each of the 48 games played in the 2018 group stage, we use our model to compute the likelihood of either team winning, or a draw occuring. We compare those probabilities to the implied odds of those results based on the moneylines given by &lt;a href=&#34;http://www.oddsportal.com/soccer/world/world-cup-2018/results/#/&#34;&gt;OddsPortal&lt;/a&gt;, and we select the result where our prediction is the most above the implied odds. Then, we consider the observed result had we placed a $100 bet on that result. Thus, if we are incorrect, we lose $100, and if we’re right, our winnings are based on the given money line.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;results&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Results&lt;/h3&gt;
&lt;style type=&#34;text/css&#34;&gt;
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-0lax{text-align:left;vertical-align:top}
&lt;/style&gt;
&lt;table class=&#34;tg&#34;&gt;
&lt;tr&gt;
&lt;th class=&#34;tg-0lax&#34;&gt;
Team 1
&lt;/th&gt;
&lt;th class=&#34;tg-0lax&#34;&gt;
Team 2
&lt;/th&gt;
&lt;th class=&#34;tg-0lax&#34;&gt;
Model: P(Win 1)
&lt;/th&gt;
&lt;th class=&#34;tg-0lax&#34;&gt;
Model: P(Draw)
&lt;/th&gt;
&lt;th class=&#34;tg-0lax&#34;&gt;
Model: P(Win 2)
&lt;/th&gt;
&lt;th class=&#34;tg-0lax&#34;&gt;
Line: Win 1
&lt;/th&gt;
&lt;th class=&#34;tg-0lax&#34;&gt;
Line: Tie
&lt;/th&gt;
&lt;th class=&#34;tg-0lax&#34;&gt;
Line: Win 3
&lt;/th&gt;
&lt;th class=&#34;tg-0lax&#34;&gt;
Profit
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Russia
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Saudi Arabia
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.562
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.233
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.206
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-217
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
334
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
807
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-100
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Russia
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Egypt
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.566
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.231
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.203
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-105
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
257
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
330
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
95.2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Russia
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Uruguay
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.462
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.262
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.276
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
193
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
208
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
177
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-100
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Saudi Arabia
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Egypt
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.36
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.289
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.351
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
445
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
263
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-130
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
445
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Saudi Arabia
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Uruguay
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.278
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.262
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.46
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
1700
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
573
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-455
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-100
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Egypt
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Uruguay
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.274
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.261
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.465
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
744
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
309
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-196
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-100
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Portugal
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Spain
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.307
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.273
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.421
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
338
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
227
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
104
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-100
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Portugal
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Morocco
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.586
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.225
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.189
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-143
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
270
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
504
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-100
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Portugal
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Iran
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.528
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.243
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.23
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-167
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
309
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
539
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-100
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Spain
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Morocco
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.641
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.207
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.152
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-278
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
417
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
927
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-100
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Spain
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Iran
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.587
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.225
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.188
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-476
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
586
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
1832
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-100
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Morocco
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Iran
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.307
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.273
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.42
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
123
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
198
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
317
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
317
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
France
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Australia
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.536
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.241
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.224
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-370
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
623
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
944
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-100
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
France
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Peru
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.583
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.226
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.191
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-164
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
287
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
561
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-100
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
France
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Denmark
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.447
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.266
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.287
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
106
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
181
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
434
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-100
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Australia
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Peru
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.407
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.276
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.316
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
193
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
254
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
149
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-100
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Australia
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Denmark
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.287
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.266
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.447
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
369
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
244
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-110
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-100
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Peru
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Denmark
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.25
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.251
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.498
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
271
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
211
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
131
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
131
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Argentina
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Iceland
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.635
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.209
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.156
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-303
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
441
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
1034
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-100
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Argentina
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Croatia
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.516
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.246
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.237
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
108
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
229
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
312
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-100
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Argentina
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Nigeria
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.583
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.226
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.191
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-200
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
426
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
499
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-100
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Iceland
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Croatia
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.257
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.254
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.488
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
341
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
299
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-120
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-100
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Iceland
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Nigeria
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.309
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.274
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.417
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
172
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
210
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
199
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
199
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Croatia
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Nigeria
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.428
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.271
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.301
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-147
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
279
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
514
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-100
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Brazil
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Switzerland
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.561
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.233
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.206
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-208
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
343
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
711
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-100
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Brazil
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Costa Rica
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.556
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.234
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.209
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-476
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
571
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
1817
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-100
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Brazil
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Serbia
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.571
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.23
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.199
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-213
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
368
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
661
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-100
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Switzerland
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Costa Rica
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.352
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.289
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.36
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-167
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
270
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
663
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-100
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Switzerland
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Serbia
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.367
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.287
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.346
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
196
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
204
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
175
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
196
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Costa Rica
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Serbia
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.371
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.286
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.343
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
436
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
243
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-119
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-100
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Germany
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Mexico
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.536
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.24
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.224
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-204
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
358
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
624
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
624
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Germany
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Sweden
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.56
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.233
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.207
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-213
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
366
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
664
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-100
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Germany
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
South Korea
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.698
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.187
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.115
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-588
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
740
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
1837
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-100
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Mexico
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Sweden
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.381
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.283
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.335
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
121
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
240
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
259
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
259
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Mexico
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
South Korea
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.547
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.237
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.216
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-143
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
281
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
473
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-100
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Sweden
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
South Korea
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.523
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.244
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.233
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
127
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
204
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
293
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
127
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Belgium
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Panama
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.608
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.218
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.174
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-455
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
577
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
1662
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-100
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Belgium
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Tunisia
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.488
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.254
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.258
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-303
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
419
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
1104
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-100
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Belgium
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
England
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.309
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.274
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.418
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
272
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
184
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
147
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
272
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Panama
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Tunisia
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.259
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.255
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.486
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
337
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
278
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-114
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-100
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Panama
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
England
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.14
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.201
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.659
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
1773
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
548
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-455
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-100
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Tunisia
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
England
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.216
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.237
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.547
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
755
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
323
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-204
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-100
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Poland
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Senegal
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.397
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.279
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.324
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
152
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
206
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
233
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
233
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Poland
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Colombia
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.204
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.232
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.563
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
249
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
246
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
121
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
121
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Poland
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Japan
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.314
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.276
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.411
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
171
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
215
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
195
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-100
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Senegal
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Colombia
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.179
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.221
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.6
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
419
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
282
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-132
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
75.8
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Senegal
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Japan
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.283
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.264
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.452
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
165
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
199
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
221
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-100
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Colombia
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
Japan
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.512
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.248
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0.24
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
-112
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
234
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
411
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
411
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;!-- | Team 1       | Team 2       | Model: P(Win 1) | Model: P(Draw) | Model: P(Win 2) | Line: Win 1 | Line: Tie | Line: Win 3 | Profit | --&gt;
&lt;!-- |--------------|--------------|-----------------|----------------|-----------------|-------------|-----------|-------------|--------| --&gt;
&lt;!-- | Russia       | Saudi Arabia | 0.562           | 0.233          | 0.206           | -217        | 334       | 807         | -100   | --&gt;
&lt;!-- | Russia       | Egypt        | 0.566           | 0.231          | 0.203           | -105        | 257       | 330         | 95.2   | --&gt;
&lt;!-- | Russia       | Uruguay      | 0.462           | 0.262          | 0.276           | 193         | 208       | 177         | -100   | --&gt;
&lt;!-- | Saudi Arabia | Egypt        | 0.36            | 0.289          | 0.351           | 445         | 263       | -130        | 445    | --&gt;
&lt;!-- | Saudi Arabia | Uruguay      | 0.278           | 0.262          | 0.46            | 1700        | 573       | -455        | -100   | --&gt;
&lt;!-- | Egypt        | Uruguay      | 0.274           | 0.261          | 0.465           | 744         | 309       | -196        | -100   | --&gt;
&lt;!-- | Portugal     | Spain        | 0.307           | 0.273          | 0.421           | 338         | 227       | 104         | -100   | --&gt;
&lt;!-- | Portugal     | Morocco      | 0.586           | 0.225          | 0.189           | -143        | 270       | 504         | -100   | --&gt;
&lt;!-- | Portugal     | Iran         | 0.528           | 0.243          | 0.23            | -167        | 309       | 539         | -100   | --&gt;
&lt;!-- | Spain        | Morocco      | 0.641           | 0.207          | 0.152           | -278        | 417       | 927         | -100   | --&gt;
&lt;!-- | Spain        | Iran         | 0.587           | 0.225          | 0.188           | -476        | 586       | 1832        | -100   | --&gt;
&lt;!-- | Morocco      | Iran         | 0.307           | 0.273          | 0.42            | 123         | 198       | 317         | 317    | --&gt;
&lt;!-- | France       | Australia    | 0.536           | 0.241          | 0.224           | -370        | 623       | 944         | -100   | --&gt;
&lt;!-- | France       | Peru         | 0.583           | 0.226          | 0.191           | -164        | 287       | 561         | -100   | --&gt;
&lt;!-- | France       | Denmark      | 0.447           | 0.266          | 0.287           | 106         | 181       | 434         | -100   | --&gt;
&lt;!-- | Australia    | Peru         | 0.407           | 0.276          | 0.316           | 193         | 254       | 149         | -100   | --&gt;
&lt;!-- | Australia    | Denmark      | 0.287           | 0.266          | 0.447           | 369         | 244       | -110        | -100   | --&gt;
&lt;!-- | Peru         | Denmark      | 0.25            | 0.251          | 0.498           | 271         | 211       | 131         | 131    | --&gt;
&lt;!-- | Argentina    | Iceland      | 0.635           | 0.209          | 0.156           | -303        | 441       | 1034        | -100   | --&gt;
&lt;!-- | Argentina    | Croatia      | 0.516           | 0.246          | 0.237           | 108         | 229       | 312         | -100   | --&gt;
&lt;!-- | Argentina    | Nigeria      | 0.583           | 0.226          | 0.191           | -200        | 426       | 499         | -100   | --&gt;
&lt;!-- | Iceland      | Croatia      | 0.257           | 0.254          | 0.488           | 341         | 299       | -120        | -100   | --&gt;
&lt;!-- | Iceland      | Nigeria      | 0.309           | 0.274          | 0.417           | 172         | 210       | 199         | 199    | --&gt;
&lt;!-- | Croatia      | Nigeria      | 0.428           | 0.271          | 0.301           | -147        | 279       | 514         | -100   | --&gt;
&lt;!-- | Brazil       | Switzerland  | 0.561           | 0.233          | 0.206           | -208        | 343       | 711         | -100   | --&gt;
&lt;!-- | Brazil       | Costa Rica   | 0.556           | 0.234          | 0.209           | -476        | 571       | 1817        | -100   | --&gt;
&lt;!-- | Brazil       | Serbia       | 0.571           | 0.23           | 0.199           | -213        | 368       | 661         | -100   | --&gt;
&lt;!-- | Switzerland  | Costa Rica   | 0.352           | 0.289          | 0.36            | -167        | 270       | 663         | -100   | --&gt;
&lt;!-- | Switzerland  | Serbia       | 0.367           | 0.287          | 0.346           | 196         | 204       | 175         | 196    | --&gt;
&lt;!-- | Costa Rica   | Serbia       | 0.371           | 0.286          | 0.343           | 436         | 243       | -119        | -100   | --&gt;
&lt;!-- | Germany      | Mexico       | 0.536           | 0.24           | 0.224           | -204        | 358       | 624         | 624    | --&gt;
&lt;!-- | Germany      | Sweden       | 0.56            | 0.233          | 0.207           | -213        | 366       | 664         | -100   | --&gt;
&lt;!-- | Germany      | South Korea  | 0.698           | 0.187          | 0.115           | -588        | 740       | 1837        | -100   | --&gt;
&lt;!-- | Mexico       | Sweden       | 0.381           | 0.283          | 0.335           | 121         | 240       | 259         | 259    | --&gt;
&lt;!-- | Mexico       | South Korea  | 0.547           | 0.237          | 0.216           | -143        | 281       | 473         | -100   | --&gt;
&lt;!-- | Sweden       | South Korea  | 0.523           | 0.244          | 0.233           | 127         | 204       | 293         | 127    | --&gt;
&lt;!-- | Belgium      | Panama       | 0.608           | 0.218          | 0.174           | -455        | 577       | 1662        | -100   | --&gt;
&lt;!-- | Belgium      | Tunisia      | 0.488           | 0.254          | 0.258           | -303        | 419       | 1104        | -100   | --&gt;
&lt;!-- | Belgium      | England      | 0.309           | 0.274          | 0.418           | 272         | 184       | 147         | 272    | --&gt;
&lt;!-- | Panama       | Tunisia      | 0.259           | 0.255          | 0.486           | 337         | 278       | -114        | -100   | --&gt;
&lt;!-- | Panama       | England      | 0.14            | 0.201          | 0.659           | 1773        | 548       | -455        | -100   | --&gt;
&lt;!-- | Tunisia      | England      | 0.216           | 0.237          | 0.547           | 755         | 323       | -204        | -100   | --&gt;
&lt;!-- | Poland       | Senegal      | 0.397           | 0.279          | 0.324           | 152         | 206       | 233         | 233    | --&gt;
&lt;!-- | Poland       | Colombia     | 0.204           | 0.232          | 0.563           | 249         | 246       | 121         | 121    | --&gt;
&lt;!-- | Poland       | Japan        | 0.314           | 0.276          | 0.411           | 171         | 215       | 195         | -100   | --&gt;
&lt;!-- | Senegal      | Colombia     | 0.179           | 0.221          | 0.6             | 419         | 282       | -132        | 75.8   | --&gt;
&lt;!-- | Senegal      | Japan        | 0.283           | 0.264          | 0.452           | 165         | 199       | 221         | -100   | --&gt;
&lt;!-- | Colombia     | Japan        | 0.512           | 0.248          | 0.24            | -112        | 234       | 411         | 411    | --&gt;
&lt;p&gt;In the end, our net profit is a whopping $106, after wagering $4,800 total. Obviously, over a sample size of 48 games, this is a quite insignificant result. But it’s actually slightly more impressive than it initially seems, because the betting lines take into account the rake of the sportsbook. If we picked randomly at a perfectly efficient market, we would on average have about a -5% return on investment (ROI). So our observed 2% ROI could charitably be considered in the context of being about a 7% improvement on the accuracy of the betting market, but obviously this is still not terribly significant over such a small sample.&lt;/p&gt;
&lt;p&gt;So far from revolutionary, but seeing the predictions turn a small profit above the rake is neat as our model bet on &lt;em&gt;all&lt;/em&gt; games, rather than just the ones it had confidence in. One would hope that this result could be improved upon by actually using the information available leading up to the world cup, such as squad selection, fitness rumors, and injury absences, to improve the result (however, beating the betting markets tends to be trickier than people expect, even with additional information).&lt;/p&gt;
&lt;p&gt;Our most profitable selection was Mexico’s win over Germany, which had an incredible line of +624. Our system would never give a competent team like Mexico such low odds, given the historical likelihood of upsets at the World Cup. This is a nice reminder about how large a sample would be necessary to draw any inferences about an actual betting strategy. Our system is sometimes selecting events which the betting lines have at &amp;lt;15%, and believing that the true number is slightly higher. We would lose money on this bet the vast majority of the time, even if we actually had a significant edge on the field.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Summary&lt;/h3&gt;
&lt;p&gt;In sum, this Elo rating system is not intended to be predictive, but to be a foundation for how to quantify the team strength shown in past world cup and qualifying games. It’s promising to see that the resulting predicted outcome percentages are close to those implied by the betting odds prior to the match. When we use the resulting model as a betting strategy for every single group stage match, we see that it turns a small profit beyond the rake taken by the bookies, although over a 48 game sample this effect is much too small to prove anything besides the rating system being at least plausible.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>OddsConverter Part 3: Uploading to the Chrome Store</title>
      <link>/2018/03/01/oddsconverter-part3/</link>
      <pubDate>Thu, 01 Mar 2018 00:00:00 +0000</pubDate>
      <guid>/2018/03/01/oddsconverter-part3/</guid>
      <description>&lt;p&gt;Brief update to the 
&lt;a href=&#34;https://dylanpotteroconnell.github.io/bettingextension2/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;past post&lt;/a&gt;. I knew a few people who wanted to try the extension, so I thought it’d be a fun exercise to go through the process of uploading it to the Chrome store. You can view and add the extension to your own browser 
&lt;a href=&#34;https://chrome.google.com/webstore/detail/odds-converter/klechkhopfnjihobbcfeheooaigjjgdg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;. This is far from a polished state, given that it was meant to be a quick way to explore something new. However, I do find myself using the finished product a surprising amount, so I thought it was worth keeping on the Webstore. And it’s nice to see how easy Google makes it to upload a simple app.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Odds Converter Part 2: Conversion Webpage</title>
      <link>/2018/01/25/oddsconverter-part2/</link>
      <pubDate>Thu, 25 Jan 2018 00:00:00 +0000</pubDate>
      <guid>/2018/01/25/oddsconverter-part2/</guid>
      <description>&lt;h2 id=&#34;opening-a-new-page&#34;&gt;Opening a New Page&lt;/h2&gt;
&lt;p&gt;Our simple extension from 
&lt;a href=&#34;https://dylanpotteroconnell.github.io/bettingextension/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;last time&lt;/a&gt; works quite well to replace text on a page, but whenever text isn&amp;rsquo;t displayed in a nice format (which is true for many mdoern websites), it won&amp;rsquo;t do much good. The hope is to add functionality to it so that the user can easily and manually input the odds that they need. My vision was to let the user right click, and open up a page that lets them impute the odds to convert in a more detailed format.&lt;/p&gt;
&lt;p&gt;As before, I don&amp;rsquo;t know enough about Javascript to start from scratch, so I borrow and adapt from some others online. 
&lt;a href=&#34;https://stackoverflow.com/questions/4376167/text-selection-and-display-in-context-menu-chrome-extension&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;This StackOverflow answer&lt;/a&gt; provides an example of creating a context menu upon a user right click that is the right structure. Importantly, this also provides a framework for storing any selected text that the user right clicks. The end goal is to let them auto populate our odds converter page, so we leave in that functionality.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;\\ rightClick.js
var selection_callbacks = [];
function getSelection(callback) {
 selection_callbacks.push(callback);
    chrome.tabs.executeScript(null, { file:&amp;quot;selection.js&amp;quot; });
  };
  chrome.extension.onRequest.addListener(function (request) {
    var callback = selection_callbacks.shift();
    callback(request);
  });

function openConverter(selectedText) {
  var serviceCall = &#39;converter.html&#39;
  chrome.tabs.create({ url: serviceCall });
}
var tx = getSelection();
var title = &amp;quot;Convert: &#39;&amp;quot; + tx + &amp;quot;&#39; title&amp;quot;;
var id = chrome.contextMenus.create({
    title: &amp;quot;Convert Odds?&amp;quot;,
    contexts:[&amp;quot;selection&amp;quot;],
    onclick: function(info, tab) {
        openConverter(info.selectionText);
    }
});
console.log(&amp;quot;selection item:&amp;quot; + id);
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;\\ selection.js
chrome.extension.sendResponse(convert(window.getSelection().toString()));
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is pretty rote, with small changes from the linked work (and final code 
&lt;a href=&#34;https://github.com/dylanpotteroconnell/oddsextension/blob/master/replaceScript.js&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;). Initially, I hoped to perform some calculations with the selected text, and then display the result in the context menu itself. After some trial and error, it seems that this is quite a bit more complicated than it would seem. While you can reference the text in the selection using &amp;ldquo;%s&amp;rdquo;, the issue is that this is passed through our function as the literal string &amp;ldquo;%s&amp;rdquo;, and not the selection itself, until it is displayed (at which point it is replaced by tthe selection). This means we can&amp;rsquo;t easily perform a calculation with it. The underlying challenge is that Chrome wants to display the text on the context menu immediately, so we would need to manually program in a structure that lets it wait for us to convert the text. For now, we instead focusing on simply opening the useful HTML page &amp;ldquo;oddsconverter.html&amp;rdquo;.&lt;/p&gt;
&lt;h2 id=&#34;odds-converter-html-page&#34;&gt;Odds Converter HTML Page&lt;/h2&gt;
&lt;p&gt;We want an HTML page that neatly converts between each sort of odds. For a rough idea of the base template, we borrow 
&lt;a href=&#34;https://www.daniweb.com/programming/web-development/threads/468068/auto-calculate-two-textfields&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this&lt;/a&gt; guide. While their goal is quite unrelated, this form has the neat feature where it calculates its result after &lt;em&gt;each&lt;/em&gt; keystroke. I want this extension to be light and nimble, so I like the idea of it constantly updating based on new input.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;&amp;lt;!doctype html&amp;gt;
&amp;lt;html&amp;gt;
&amp;lt;!-- https://www.daniweb.com/programming/web-development/threads/468068/auto-calculate-two-textfields --&amp;gt;
&amp;lt;head&amp;gt;
&amp;lt;script src=&amp;quot;converter.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;

&amp;lt;/head&amp;gt;
&amp;lt;form name=&amp;quot;convert&amp;quot; action=&amp;quot;&amp;quot; id=&amp;quot;convert&amp;quot; &amp;gt;
    &amp;lt;span id=&amp;quot;update&amp;quot;&amp;gt;Odds Converter&amp;lt;/span&amp;gt;
    &amp;lt;p&amp;gt;&amp;lt;input type=&amp;quot;text&amp;quot; id=&amp;quot;ml&amp;quot; name=&amp;quot;ml&amp;quot; onkeyup=&amp;quot;convML(this)&amp;quot;/&amp;gt; Money Line &amp;lt;/p&amp;gt;
    &amp;lt;p&amp;gt;&amp;lt;input type=&amp;quot;text&amp;quot; id=&amp;quot;frac&amp;quot; name=&amp;quot;frac&amp;quot; onkeyup=&amp;quot;convFrac(this)&amp;quot;/&amp;gt; Fractional Odds &amp;lt;/p&amp;gt;
    &amp;lt;p&amp;gt;&amp;lt;input type=&amp;quot;text&amp;quot; id=&amp;quot;ip&amp;quot; name=&amp;quot;ip&amp;quot; onkeyup=&amp;quot;convIP(this)&amp;quot;/&amp;gt; Implied Probability &amp;lt;/p&amp;gt;
    &amp;lt;input type=&amp;quot;hidden&amp;quot; id=&amp;quot;total&amp;quot; name=&amp;quot;total&amp;quot; value=&amp;quot;0&amp;quot; /&amp;gt;
&amp;lt;/form&amp;gt;
&amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The heavy lifting is of course done by the functions stored in the javascript file &amp;ldquo;converter.js&amp;rdquo;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;function convML(obj) {
  if(/^[\+|-] ?[0-9]+$/.test(obj.value)){
    var pattern = /([+|-]) ?([0-9]+)/;
    var newIP = obj.value.replace(pattern,
      function(fm,$1,$2)
      {
        if ($1==&amp;quot;+&amp;quot;)
        {
          return(Math.round(100.0/(100.0+parseInt($2))*1000)/10.0)
        } else
        {
          return(Math.round(parseInt($2)/(100.0+parseInt($2))*1000)/10.0)
        }
      });
     var newFrac = String(Math.round(100/(newIP/100.0)-100)/100)+&amp;quot;/&amp;quot;+&amp;quot;1&amp;quot;;
     document.getElementById(&#39;ip&#39;).value = newIP;
     document.getElementById(&#39;frac&#39;).value = newFrac;
  }
}

function convFrac(obj) {
  document.getElementById(&#39;ip&#39;).value = obj.value;
// Add decimals

  if(/^[0-9]+\.?[0-9]*\/[0-9]+\.?[0-9]*$/.test(obj.value)){
    var pattern = /([0-9]+\.?[0-9]*)\/([0-9]+\.?[0-9]*)/;
    var newIP  = obj.value.replace(pattern,
                                          function(fm,$1,$2)
                                          {
                                             return(Math.round(1000*parseFloat($2)/(parseFloat($2)+parseFloat($1)))/10.0)
                                          }
                                    );
     document.getElementById(&#39;ip&#39;).value = newIP;
     document.getElementById(&#39;ml&#39;).value = IPtoML(parseFloat(newIP)/100.0);
  }
}
  if(/^[0-9]+\.?[0-9]*$/.test(obj.value)){
    document.getElementById(&#39;ml&#39;).value = IPtoML(parseFloat(obj.value)/100.0);
  }
}

function IPtoML(IP) {
  if (IP &amp;gt; .5) {
    return(&amp;quot;-&amp;quot; + String(Math.round(100.0*IP/(1-IP))))
  } else if (IP &amp;lt;= .5) {
    return(&amp;quot;+&amp;quot;+String(Math.round(100.0/IP - 100)))
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Each of the conversion functions follows the same format: after each keystroke in the text box, check if the given input is of the correct form, and if it is, upate the other two text boxes with the converted odds. The functionality is very limited, but it fulfills our purposes for now. Converting moneyline odds and implied probability to fractional odds is a tougher challenge, so for now we simiply display them as decimals with a denominator of one (while there is no straightforward one to one mapping to fractional odds, it&amp;rsquo;s possible to write an algorithm to make an educated guess as to what whole number odds are closest, but for now we keep it simple).&lt;/p&gt;
&lt;p&gt;When we open up this HTML page, it works as desired, but when we load it into our extension, it opens up and&amp;hellip; does nothing. None of the fields update themselves, no matter howm much you try. A quick Google search determines that we are barking right up the wrong tree. A few years ago, 
&lt;a href=&#34;https://developer.chrome.com/extensions/contentSecurityPolicy#JSExecution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chrome banned all inline Javascript&lt;/a&gt; from its extension pages. I have no doubt that they had good security related reasons to do so, but it means that our page as written is quite useless.&lt;/p&gt;
&lt;h2 id=&#34;removing-inline-javascript&#34;&gt;Removing Inline Javascript&lt;/h2&gt;
&lt;p&gt;It turns out that for simple HTML forms like this, the switch is relatively painless. We remove the function calls from the HTML file, so &lt;code&gt;&amp;lt;input type=&amp;quot;text&amp;quot; id=&amp;quot;ml&amp;quot; name=&amp;quot;ml&amp;quot; onkeyup=&amp;quot;convML(this)&amp;quot;/&amp;gt;&lt;/code&gt; becomes &lt;code&gt;&amp;lt;input type=&amp;quot;text&amp;quot; id=&amp;quot;ml&amp;quot; name=&amp;quot;ml&amp;quot;/&amp;gt;&lt;/code&gt;. Then, in our &amp;ldquo;converter.js&amp;rdquo; file, we need to add an EventListener function to process each of these events, based on their ID.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;document.addEventListener(&amp;quot;DOMContentLoaded&amp;quot;, function(event) {
  document.getElementById(&amp;quot;ml&amp;quot;).addEventListener(&amp;quot;keyup&amp;quot;, function() {
        convML(document.getElementById(&#39;ml&#39;));
     });
  document.getElementById(&amp;quot;frac&amp;quot;).addEventListener(&amp;quot;keyup&amp;quot;, function() {
        convFrac(document.getElementById(&#39;frac&#39;));
     });
  document.getElementById(&amp;quot;ip&amp;quot;).addEventListener(&amp;quot;keyup&amp;quot;, function() {
        convIP(document.getElementById(&#39;ip&#39;));
     });
});
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The rest of our code can stay the exact same. Once we use the EventListener format, our other Javascript functions work just fine.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>OddsConverter Part 1: Converting Plaintext</title>
      <link>/2018/01/22/oddsconverter-part1/</link>
      <pubDate>Mon, 22 Jan 2018 00:00:00 +0000</pubDate>
      <guid>/2018/01/22/oddsconverter-part1/</guid>
      <description>&lt;p&gt;The same betting odds are often displayed in disparate ways, depending on the context. In America, normally in terms of a Moneyline, where +200 is shorthand for &amp;ldquo;if you bet $100, you can win an additional $200&amp;rdquo;. In other contexts, we generally see the tradition &amp;ldquo;X/Y&amp;rdquo; fractional odds. Both of these have clear mathematical meanings, but for inexperienced bettors, they tend to be poor conveyors of intuitive meaning. We can quickly estimate what &amp;ldquo;+350&amp;rdquo; implies, but for most people, it takes a mental calculation to do so. The most useful intuitive information about betting odds comes from the &amp;ldquo;Implied Probability&amp;rdquo;, which is the probability of the event necessary for you to &amp;ldquo;break even&amp;rdquo; on the bet. Betting sites don&amp;rsquo;t particularly like Implied Probability because it makes the cut they take more obvious (when mutually exclusive and complete events have probabilities that sum up to greater than one), and more intuitively accurate information for bettors can make them realize how daunting it is to actually keep up a positive expected value.&lt;/p&gt;
&lt;p&gt;As someone without the perfect intuition needed to convert these odds in my head, I got tired of glancing at a table of odds and trying to work out their precise value one by one. I thought it would be nice to have a very simple Google Chrome extension that could do it for me. The initial functionality is simple: click on the icon, and it converts any odds it can find on the page into Implied Probability. Click on it again, and it converts back. The challenge is that I have zero knowledge of JavaScript. Luckily, this task was simple enough that it could be cobbled together in the style of Frankenstein&amp;rsquo;s monster, with some slight modifications.&lt;/p&gt;
&lt;h2 id=&#34;click-icon-replace-text&#34;&gt;Click Icon, Replace Text&lt;/h2&gt;
&lt;p&gt;The first feature necessary is simple an extension that runs a script when the icon is clicked. We find a 
&lt;a href=&#34;https://stackoverflow.com/questions/7168362/run-script-each-time-chrome-extension-icon-clicked&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Stack Overflow question&lt;/a&gt; (which will be the source of the code for most of the extension) with this very task. Every Chrome Extension begins with a manifest.json file, which gives an overview of the permissions and structure of the extension. I grab their initial suggested manifest.json, and make a number of modifications which will become necessary later. The &amp;ldquo;contextmenus&amp;rdquo; under permissions is only necessary to activate context menus within Chrome, which seems like a useful feature but is not yet crucial to the Extension&amp;rsquo;s functionality. I also made a set of three simple icons in GIMP, so that it looks natural in the browser. The key parts here are the two scripts under &amp;ldquo;background&amp;rdquo;. &amp;ldquo;rightclick.js&amp;rdquo; provides instructions for how to create a context menu upon right click, which we ignore for now (I played with it, but it&amp;rsquo;s still a work in progress). However, &amp;ldquo;background.js&amp;rdquo; is the script that tells our&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;
{
  &amp;quot;manifest_version&amp;quot;: 2,

  &amp;quot;name&amp;quot;: &amp;quot;Convert Odds into Implied Probability&amp;quot;,
  &amp;quot;description&amp;quot;: &amp;quot;Click this icon to convert moneyline odds into implied probability, and vice versa.&amp;quot;,
  &amp;quot;version&amp;quot;: &amp;quot;1.0&amp;quot;,
  &amp;quot;background&amp;quot; : {
    &amp;quot;scripts&amp;quot; : [&amp;quot;background.js&amp;quot;, &amp;quot;rightclick.js&amp;quot;]
  },
  &amp;quot;icons&amp;quot;: { &amp;quot;16&amp;quot;: &amp;quot;icon16.png&amp;quot;,
             &amp;quot;48&amp;quot;: &amp;quot;icon48.png&amp;quot;,
             &amp;quot;128&amp;quot;: &amp;quot;icon128.png&amp;quot; },

  &amp;quot;browser_action&amp;quot;: {
    &amp;quot;default_icon&amp;quot;: &amp;quot;icon48.png&amp;quot;,
    &amp;quot;default_title&amp;quot;: &amp;quot;Convert Odds&amp;quot;
  },
  &amp;quot;permissions&amp;quot;: [
    &amp;quot;activeTab&amp;quot;,
    &amp;quot;contextMenus&amp;quot;
  ]
}

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The key parts here are the two scripts under &amp;ldquo;background&amp;rdquo;: &amp;ldquo;rightclick.js&amp;rdquo; provides instructions for how to create a context menu upon right click, which we ignore for now (I played with it, but it&amp;rsquo;s still a work in progress), and &amp;ldquo;background.js&amp;rdquo; is a script that calls our text replace script when the Extension icon is clicked.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;chrome.browserAction.onClicked.addListener(function(tab) {
   chrome.tabs.executeScript(null, {file: &amp;quot;replaceScript.js&amp;quot;});
});
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&amp;ldquo;replaceScript.js&amp;rdquo; is the script that  runs a text replacement that converts anything that &amp;ldquo;looks like&amp;rdquo; odds into implied probabilities (and you can view the final script 
&lt;a href=&#34;https://github.com/dylanpotteroconnell/oddsextension/blob/master/replaceScript.js&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;). Being totally unfamiliar with JavaScript, I grab code from an Extension that performs a 
&lt;a href=&#34;https://9to5google.com/2015/06/14/how-to-make-a-chrome-extensions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;simple text replace for each webpage&lt;/a&gt;. The base structure is as follows.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;var elements = document.getElementsByTagName(&#39;*&#39;);
for (var i = 0; i &amp;lt; elements.length; i++) {
    var element = elements[i];
    for (var j = 0; j &amp;lt; element.childNodes.length; j++) {
        var node = element.childNodes[j];
        if (node.nodeType === 3) {
            var pattern = /abc/gi;
            var text = node.nodeValue;
            var replacedText = text.replace(pattern, &#39;def&#39;);
            if (replacedText !== text) {
                element.replaceChild(document.createTextNode(replacedText), node);
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;detect-patterns-calculate-conversions&#34;&gt;Detect Patterns, Calculate Conversions&lt;/h2&gt;
&lt;p&gt;Next, we can use Regular Expressions to only replace certain structures of characters, rather than just one static string. To start, we want to try and identify sequences of exactly a plus or minus sign, followed by at least one digit (e.g. &amp;ldquo;-220&amp;rdquo;). The parentheses &amp;ldquo;capture&amp;rdquo; the characters inside, as we need to not just find these patterns, but extract the numbers. We want to find the end of the digits, but we want to leave the following character in the page&amp;rsquo;s HTML, so we &amp;ldquo;grab&amp;rdquo; the first nondigit character (the [^0-9), but only if it exists (the &amp;ldquo;?&amp;quot;) using &amp;ldquo;([^0-9]?)&amp;quot;, so that we can place that character after our converted expression, &lt;code&gt;var pattern = /([+|-])([0-9]+)([^0-9]?)/gi;&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Now, the challenge is that we want to perform a small calculation using the number provided. That means we need to be able to feed captured characters (preferably, the numbers and the +/- sign) as inputs into a function that determines the output of the replace function of text. I don&amp;rsquo;t know of the optimal way to do this in JavaScript, but with a Google search we find a way to do just that 
&lt;a href=&#34;https://www.bennadel.com/blog/55-using-methods-in-javascript-replace-method.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We can replace the previous string used for our pattern with a lambda function. Note that the link provided is inaccurate in its description of the inputs to the lambda function, and this is corrected to the comments. The inputs to the dummy function here are always &amp;ldquo;the full match, $1, $2, $3, &amp;hellip;&amp;rdquo; (where $X refers to the xth capture element), while in the linked tutorial they use &amp;ldquo;$1$&amp;rdquo; in the function input to refer to the full text. We can adapt their function to convert an American moneyline into its implied odds as follows.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;
var replacedText = text.replace(pattern,
                                function(fm,$1,$2,$3)
				    {
				      if ($1==&amp;quot;+&amp;quot;)
				      {
					return(&amp;quot;|&amp;quot;+String(Math.round(100.0/(100.0+parseInt($2))*1000)/10.0)+&amp;quot;\%&amp;quot;+&amp;quot;|&amp;quot;+$3)
				      } else
				      {
					return(&amp;quot;|&amp;quot;+String(Math.round(parseInt($2)/(100.0+parseInt($2))*1000)/10.0)
						 +&amp;quot;\%&amp;quot;+&amp;quot;|&amp;quot;+$3)
				      }
				    });
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It&amp;rsquo;s worth breaking this into parts. The inputs to our function are the full text (&amp;lsquo;fm&amp;rsquo;, which we do not use here, but I was not aware of how to suppress this input), and the three strings &amp;lsquo;captured&amp;rsquo; from our regex matching (where $1 refers to the +/- sign, $2 is the number of the line, and $3 is the trailing character). We compute the simple formula to go from a moneyline bet to its implied odds. For a moneyline of +$X, we earn $X+$100 on a win. Thus, the odds (O) needed to break even are $100=0*($X+$100), so O=$100/($X+$100). Similarly, for a moneyline of -$X, we bet $X to win $100, and the implied odds are O=$X/($X+$100). The final return statement is simply cobbled together from some JavaScript documentation. ParseInt turns the &amp;lsquo;$2&amp;rsquo; input into an integer, we then multiply by 1000, and divide by 10 to get the result in the form of a percentage with a single decimal place.&lt;/p&gt;
&lt;p&gt;We repeat this process with two other conversions: 2. Fractional Odds (i.e. &amp;ldquo;3/1&amp;rdquo;) to Implied Probability, and 3. &amp;ldquo;Undoing&amp;rdquo; our first conversion, converting implied probabilities back into moneyline odds. We do not include a separate &amp;ldquo;undo&amp;rdquo; conversion for Fractional Odds, because I wanted to keep this extension very minimal, and detecting the most accurate Fractional Odds to a rounded decimal percentage is a less straightforward conversion.&lt;/p&gt;
&lt;h2 id=&#34;next&#34;&gt;Next&amp;hellip;&lt;/h2&gt;
&lt;p&gt;This results in a simple but effective Chrome Extension that can save quite a bit of time. When on a page with odds in HTML text, you tap the button and can quickly see them replaced by their Implied Probabilities. The major downside is that more complex websites don&amp;rsquo;t always show their odds in such simple HTML text, and learning to detect them for the major bookmakers would be a more substantial challenge. However, most people encounter odds referenced in articles or text discussions, where this Extension works quite cleanly.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>World Cup Elo Part 2: Tuning the Model</title>
      <link>/2017/12/10/worldcup-2/</link>
      <pubDate>Sun, 10 Dec 2017 00:00:00 +0000</pubDate>
      <guid>/2017/12/10/worldcup-2/</guid>
      <description>


&lt;p&gt;&lt;em&gt;Part 1 can be found &lt;a href=&#34;/2017/12/05/worldcup-1&#34;&gt;here&lt;/a&gt;, and a followup discussing the eventual observed World Cup results is found &lt;a href=&#34;/2018/06/01/worldcup-3&#34;&gt;here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;div id=&#34;elo-rating-system-choices&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Elo Rating System Choices&lt;/h3&gt;
&lt;p&gt;As previously described, the Elo rating system is a broad and flexible framework, rather than a monolithic model. In fact, the primary draw of this project was to better understand the subjective choices that any person constructing such a model must make, and how we can use our intuition and domain knowledge to make plausible choices. In this post, I’ll briefly outline how one might approach some of those choices in the context of the World Cup.&lt;/p&gt;
&lt;div id=&#34;calibrating-draws&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Calibrating Draws&lt;/h4&gt;
&lt;p&gt;The Elo rating system only computes an Expected Score for a given matchup, which does not uniquely determine the likelihood of a draw. We consider draws worth half a point (so Expected Score &lt;span class=&#34;math inline&#34;&gt;\(= P(\text{Win}) + P(\text{Draw})/2\)&lt;/span&gt;), but an Expected Score of &lt;span class=&#34;math inline&#34;&gt;\(1/2\)&lt;/span&gt; might correspond to a 50% chance of either team winning (and no chance of a draw), or 100% chance of a draw, or anything in between. Thus, we need to find a formula to convert from Expected Score to the chances of either team winning or a draw. There is no perfect system to do this, the best we can do is examine historical data, and find a plausble mapping.&lt;/p&gt;
&lt;p&gt;The details are covered in Section 3.1 of the corresponding paper. In the &lt;a href=&#34;https://www.progressivebetting.co.uk/statistics/football_statistics/leagues_by_draws/&#34;&gt;top four leagues&lt;/a&gt;, about 26% of matches end in a draw. In our full dataset, only 21.7% of matches end in a draw, but in the group stage, 25.1% end in a draw. This fits out intuition, as the qualifying stages have significantly less parity, which reduces the chance of a draw. As a final ballpark estimate, we consider the draw percentages of individual matches implicitly predicted by Premiere league &lt;a href=&#34;https://pena.lt/y/2015/12/12/frequency-of-draws-in-football/&#34;&gt;sportsbooks&lt;/a&gt;, and note that the draw percentage almost never rises above 31%, with a heavy left tail. Thus, for our initial estimate, we choose that two evenly matched teams have about a 29% chance of a draw (about one standard deviation above what we see in top leagues). Then, when we run our Elo model, we can select values of a tuning parameter (which we call &lt;span class=&#34;math inline&#34;&gt;\(k_d\)&lt;/span&gt;), which increases the likelihood of a draw, as shown in . Again, the messiness of these models lies in the fact that there is no truly rigorous approach. However, this fits our intuition, and can be adjusted until it best fits our variety of historical data.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation} P(\text{Draw}) = .29(1-2|ES-1/2|)^{k_d}.\label{eqn:draws}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;incorporating-margin-of-victory-mov&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Incorporating Margin of Victory (MOV)&lt;/h3&gt;
&lt;p&gt;So far, when we refer to the “score” in the context of our Elo ratings, we have been referring to the match result (with &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; denoting a win, &lt;span class=&#34;math inline&#34;&gt;\(1/2\)&lt;/span&gt; a tie, and &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; a loss). This approach is sensible, but it is possible that we are missing out on additional valuable information by ignoring the margin of victory (which is the difference between the scores of the two respective teams). We have to be careful not to overweight margin of victory, as it is match result that ultimately determines advancement (in qualifying and group stage matches, margin of victory is only ever a tiebreaker), so teams do not have as much incentive to widen a blowout lead as they do to gain the lead. Further, we have to be concerned with the concept of “garbage time”, which is a widely believed phenomena where teams will not necessarily play their hardest once flipping the game result is essentially out of reach (the idea is that if a team is down multiple goals without much time left, they may “give up” and be more likely to concede additional goals, leading to a wider blowout). Thus, we wish to test margin of victory as an alternative approach to our regular score format.&lt;/p&gt;
&lt;p&gt;We still need to have the score range from &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;, with &lt;span class=&#34;math inline&#34;&gt;\(1/2\)&lt;/span&gt; denoting a draw. However, when using margin of victory, we can now treat narrow victories as not being worth a full point, and the same for narrow defeats being worth more than &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; points. As mentioned before, we want to minimize the impact of “garbage time” goals on our rating, thus we want to make sure that the difference between a 1 goal win and a draw is substantially more than the difference between a 5 goal win and a 4 goal win. As before, let &lt;span class=&#34;math inline&#34;&gt;\(S_A\)&lt;/span&gt; be the observed score of the match, and let &lt;span class=&#34;math inline&#34;&gt;\(M_A\)&lt;/span&gt; be team A’s margin of victory (which is negative in a loss). Then, we choose our score function so that each additional goal brings the score half the remaining distance to &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; (or &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;), as shown in Equation  (for &lt;span class=&#34;math inline&#34;&gt;\(M_A=0\)&lt;/span&gt;, we define this sum to be &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;, so &lt;span class=&#34;math inline&#34;&gt;\(S_A=1/2\)&lt;/span&gt;). Thus, a draw is worth &lt;span class=&#34;math inline&#34;&gt;\(1/2\)&lt;/span&gt; score, winning by one goal is worth &lt;span class=&#34;math inline&#34;&gt;\(3/4\)&lt;/span&gt; score, and losing by 2 goals is worth &lt;span class=&#34;math inline&#34;&gt;\(1/8\)&lt;/span&gt; score, and so on. Thus, the most significant goal is still the difference between a win and a draw, and each successive goal adds more (but decreasing) weight to the significance of the result. It is not obvious that this approach is superior to that of our original simple score system, and we have to test this approach and see if it provides a better hit to the histrical data.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
S_A =  1/2 + \text{Sign}(\{M_A\})\sum_{i=1}^{|M_A|} (1/2)^{i+1} \label{eqn:mov}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;weighting-historical-results&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Weighting Historical Results&lt;/h3&gt;
&lt;p&gt;Perhaps the most fundamental challenge for national team predictions is that for any given team, the matches are a small sample stretched over a long period of time. The 2018 group stage team with the largest number of total matches in our dataset is Mexico (with 74), but most play far fewer (with the average being &lt;span class=&#34;math inline&#34;&gt;\(48\)&lt;/span&gt; matches played in our dataset). We can see that Mexico’s roster has shifted dramatically during this span (indeed, no players from the 2002 squad are still on the roster), which calls into question the predictive efficacy of looking at results from the distant past. However, the best world cup nations tend to show consistent success even as rosters shift (Germany, Italy, and Brazil combine for 13 World Cup victories, while the rest of the world only has a combined 7), likely due to their infrastructure for the sport. On the other hand, if a collection of quality players mature at the same time, a roster can achieve success that is at odds with the nation’s historical strength,. For instance, in the five World Cups from 1990 to 2010, Belgium did not qualify twice, had one group stage exit, and two round of 16 exits. However, entering the 2014 World Cup, bookmakers gave them the fifth best odds to win the tournament (behind perennial powerhouses Brazil, Argentina, Germany, and Spain). This example demonstrates a major challenge that we face. Historical success is a reliable initial predictor, but it does not capture the rise of a particular roster. Indeed, en route to qualifying for the 2014 World Cup, Belgium only played 6 matches, mostly against opponents who were not strong enough to qualify for the World Cup themselves. It is hard to predict the Belgium squad’s strength based on their historical success, but it is dangerous to weight their small sample size of qualifying victories against lesser opponents too highly. Thus, we want to find a delicate balance between weighting a team’s full set of games and their smaller sample of recent games that better matches their current roster.&lt;/p&gt;
&lt;p&gt;The main way that we account for this balance is in the &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; weights of the Elo system (Equation ). The shift in Elo after each match result is tuned by the &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; parameter, with larger values placing higher weight on the match. We can adjust the &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; values to be small for games far in the past, and large for recent games. However, our weighting need not be limited to simply the date of the match. We will also experiment with giving higher weight to main event (group stage or knockout) matches over qualification matches. This makes intuitive sense, as qualification matches are more likely to be disrupted by rushed travel, and might not be as predictive. There is also a higher likelihood of the match being comparatively unimportant to the team (this year, Brazil secured qualification with several qualification matches remaining on their schedule).&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(c_t\)&lt;/span&gt; be our tuning parameter to weight match recency, and &lt;span class=&#34;math inline&#34;&gt;\(c_q\)&lt;/span&gt; be our tuning parameter to weight matches which are played in the main event. In each case, a value of &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; denotes that there is no additional weighting, and a value greater than 1 increases the weighting. Let &lt;span class=&#34;math inline&#34;&gt;\(k_b\)&lt;/span&gt; be the base &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; value (which will generally be initialized to around &lt;span class=&#34;math inline&#34;&gt;\(10\)&lt;/span&gt;, leading to intuitive shifts in Elo, but it will vary based on our other tuning parameters), let &lt;span class=&#34;math inline&#34;&gt;\(Y_m\)&lt;/span&gt; be the year of the match we are considering, and &lt;span class=&#34;math inline&#34;&gt;\(Y_c\)&lt;/span&gt; the year we are projecting for (it will ultimately be 2018, but we will test our models on earlier years). Then our weighting formula is given by&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation} K(k_b, c_t, c_q) = k_b + \frac{5}{8}(c_t-1)(16-(Y_c-Y_m)) + k_b (c_q-1) \mathbb{1}\\{\text{Main Event Match}\\}\label{eqn:k}.
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We have a broad choice in defining this formula, but this leads to fairly intuitive results. We note that the additional weighting (beyond the &lt;span class=&#34;math inline&#34;&gt;\(k_b\)&lt;/span&gt; base value) is only present if we increase &lt;span class=&#34;math inline&#34;&gt;\(c_q\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(c_t\)&lt;/span&gt; beyond &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;, and when they are present they are additive to the base value. We construct the time modification so that at the tail end of our dataset (16 years prior), the time bonus is &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;, and it scales up to &lt;span class=&#34;math inline&#34;&gt;\(10(c_t -1)\)&lt;/span&gt; in the present day. The qualifying tuning parameter simply adds a constant based on whether the match was part of the main event of the World Cup. If we include these additive values (and set &lt;span class=&#34;math inline&#34;&gt;\(k_t, k_q &amp;gt; 1\)&lt;/span&gt;), we can simply reduce &lt;span class=&#34;math inline&#34;&gt;\(k_b\)&lt;/span&gt; so that the total Elo shifts do not become too dramatic. These tuning parameters will be a core part of our search for optimal parameters (Section ), and we will select values for which they give us intuitive results.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;resulting-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Resulting Model&lt;/h3&gt;
&lt;p&gt;With these options laid out, we have to perform a search of the multidimensional parameter space to find the model which is the most reasonable fit with our intuition and the historical data. This process is more substantial, and can be found &lt;a href=&#34;%7B%7B%20site.url%20%7D%7D/images/doconnell_practicalexamreport.pdf&#34;&gt;in the original report&lt;/a&gt; (in the future I might createa summary for the webpage). Ultimately, this search is less interesting than the lessons learned in setting up the model (although there is much to be discussed when comparing the efficacy of different parameter choices on the historical data). In the future, I’d like to write that up in more detail. For now, I will simply leave the results of our final model. Our final parameter choices are&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;For our scaling &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; factor, we use tuning parameters of &lt;span class=&#34;math inline&#34;&gt;\(c_q = 2, c_t = 2,\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(k_b = 10\)&lt;/span&gt; (see Equation ).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We incorporate margin of victory into our scoring system (see Equation ).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We select the tuning parameter for the likelihood of draws &lt;span class=&#34;math inline&#34;&gt;\(k_d = .5\)&lt;/span&gt; (see Equation ).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We assume that home teams have a rating advantage of 90 points, and that this applies for World Cup hosts as well.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The final estimated Elo ratings entering the 2018 World Cup are as follows. These show remarkable consistency with betting markets leading into the World Cup, showing that the vast majority of our World Cup predictions can be accounted for by a simple analysis of past results, with informatin about shifting rosters and practice rumors and analysis of aging players only a very slight factor.&lt;/p&gt;
&lt;style type=&#34;text/css&#34;&gt;
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-xldj{border-color:inherit;text-align:left}
.tg .tg-quj4{border-color:inherit;text-align:right}
.tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}
.tg .tg-dvpl{border-color:inherit;text-align:right;vertical-align:top}
&lt;/style&gt;
&lt;table class=&#34;tg&#34;&gt;
&lt;tr&gt;
&lt;th class=&#34;tg-xldj&#34;&gt;
Team
&lt;/th&gt;
&lt;th class=&#34;tg-quj4&#34;&gt;
Elo Rating
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
Germany
&lt;/td&gt;
&lt;td class=&#34;tg-dvpl&#34;&gt;
1413.74
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
Brazil
&lt;/td&gt;
&lt;td class=&#34;tg-dvpl&#34;&gt;
1375.23
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
Spain
&lt;/td&gt;
&lt;td class=&#34;tg-dvpl&#34;&gt;
1351.30
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
Argentina
&lt;/td&gt;
&lt;td class=&#34;tg-dvpl&#34;&gt;
1338.77
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
France
&lt;/td&gt;
&lt;td class=&#34;tg-dvpl&#34;&gt;
1321.57
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
Portugal
&lt;/td&gt;
&lt;td class=&#34;tg-dvpl&#34;&gt;
1311.52
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
Colombia
&lt;/td&gt;
&lt;td class=&#34;tg-dvpl&#34;&gt;
1310.96
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
England
&lt;/td&gt;
&lt;td class=&#34;tg-dvpl&#34;&gt;
1303.28
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
Mexico
&lt;/td&gt;
&lt;td class=&#34;tg-dvpl&#34;&gt;
1301.45
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
Sweden
&lt;/td&gt;
&lt;td class=&#34;tg-dvpl&#34;&gt;
1285.50
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
Denmark
&lt;/td&gt;
&lt;td class=&#34;tg-dvpl&#34;&gt;
1265.82
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
Belgium
&lt;/td&gt;
&lt;td class=&#34;tg-dvpl&#34;&gt;
1265.15
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
Uruguay
&lt;/td&gt;
&lt;td class=&#34;tg-dvpl&#34;&gt;
1257.39
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
Costa Rica
&lt;/td&gt;
&lt;td class=&#34;tg-dvpl&#34;&gt;
1249.41
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
Switzerland
&lt;/td&gt;
&lt;td class=&#34;tg-dvpl&#34;&gt;
1246.65
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
Serbia
&lt;/td&gt;
&lt;td class=&#34;tg-dvpl&#34;&gt;
1239.61
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
Croatia
&lt;/td&gt;
&lt;td class=&#34;tg-dvpl&#34;&gt;
1239.21
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
Russia
&lt;/td&gt;
&lt;td class=&#34;tg-dvpl&#34;&gt;
1232.75
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
Japan
&lt;/td&gt;
&lt;td class=&#34;tg-dvpl&#34;&gt;
1214.17
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
Australia
&lt;/td&gt;
&lt;td class=&#34;tg-dvpl&#34;&gt;
1209.51
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
Iran
&lt;/td&gt;
&lt;td class=&#34;tg-dvpl&#34;&gt;
1204.75
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
Nigeria
&lt;/td&gt;
&lt;td class=&#34;tg-dvpl&#34;&gt;
1194.97
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
Saudi Arabia
&lt;/td&gt;
&lt;td class=&#34;tg-dvpl&#34;&gt;
1193.30
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
Egypt
&lt;/td&gt;
&lt;td class=&#34;tg-dvpl&#34;&gt;
1190.45
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
Tunisia
&lt;/td&gt;
&lt;td class=&#34;tg-dvpl&#34;&gt;
1183.82
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
South Korea
&lt;/td&gt;
&lt;td class=&#34;tg-dvpl&#34;&gt;
1181.74
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
Poland
&lt;/td&gt;
&lt;td class=&#34;tg-dvpl&#34;&gt;
1180.45
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
Peru
&lt;/td&gt;
&lt;td class=&#34;tg-dvpl&#34;&gt;
1177.72
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
Morocco
&lt;/td&gt;
&lt;td class=&#34;tg-dvpl&#34;&gt;
1165.22
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
Iceland
&lt;/td&gt;
&lt;td class=&#34;tg-dvpl&#34;&gt;
1157.48
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
Senegal
&lt;/td&gt;
&lt;td class=&#34;tg-dvpl&#34;&gt;
1154.91
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
Panama
&lt;/td&gt;
&lt;td class=&#34;tg-dvpl&#34;&gt;
1103.44
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>World Cup Elo Part 1; Predictive Efficacy of Prior Results </title>
      <link>/2017/12/05/worldcup-1/</link>
      <pubDate>Tue, 05 Dec 2017 00:00:00 +0000</pubDate>
      <guid>/2017/12/05/worldcup-1/</guid>
      <description>


&lt;p&gt;&lt;em&gt;Part 2 can be found &lt;a href=&#34;/2017/12/10/worldcup-2/&#34;&gt;here&lt;/a&gt;, and a followup discussing the eventual observed World Cup results is found &lt;a href=&#34;/2018/06/01/worldcup-3&#34;&gt;here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The FIFA World Cup is the biggest biggest spectacle of the sports and entertainment, world. With tens of billions of dollars bet on its results, it presents a fascinating predictive challenge. Its structure defies that of regular domestic sports leagues, which offer regular and consistent competitive results which we can use to gauge future performance. National teams play each other infrequently, and there are frequently large shifts in roster between each meaningful competition. True World Cup predictions have to go far beyond standard sabermetric models, and have to consider a wide array of input information.&lt;/p&gt;
&lt;p&gt;This provides an interesting opportunity to consider the practical limits of an analytical model in such a challenging situation. We consider the efficacy of predicting national team strength using solely the World Cup results of the past 20 years. The core of this short project was originally written in five days as part of a curricular project (knowing little about international football), hence some of the initial restrictions, but it has been expanded since. The original paper goes into some detail &lt;a href=&#34;/pdfs/doconnell_practicalexamreport.pdf&#34;&gt;here&lt;/a&gt;, but we highlight some of the points here. This post will describe the setup, challenge, and data setting, and &lt;a href=&#34;/2017/12/10/worldcup-2/&#34;&gt;Part 2&lt;/a&gt; will describe some of the choices made in tuning the model (with Part 3 to come).&lt;/p&gt;
&lt;div id=&#34;challenges&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Challenges&lt;/h3&gt;
&lt;p&gt;We start with a brief overview of why common statistical approaches are poorly suited to this problem. This will be fairly obvious to most, but these challenges inform any more successful approach, and need to be kept in mind. A naive approach to predict the outcome of a match might be to consider the past results between the two teams in question. However, the sparsity of the dataset makes this impossible. Of the 48 matches played in the 2018 group stage, only 14 of them involve a pair of teams which have a single game played in our dataset. This is not solely due to the vast number of national teams and the infrequency of scheduled matches, but becuase the qualifying and group selection process specifically encourages matchup diversity. Qualfication is done by region, so a large number of a team’s total games will be against the same consistent competition, while group selection encourages a variety of regional representation.&lt;/p&gt;
&lt;p&gt;The closed nature of regional qualification highlights the primary challenge in this task. Almost any measurable result in football matters only relative to its competition. As a toy example, compare the basic performance metrics between Argentina split between their qualification matches and their group stage matches in our dataset.&lt;/p&gt;
Qualification Performance
&lt;style type=&#34;text/css&#34;&gt;
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-xldj{border-color:inherit;text-align:left}
.tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}
&lt;/style&gt;
&lt;table class=&#34;tg&#34;&gt;
&lt;tr&gt;
&lt;th class=&#34;tg-xldj&#34;&gt;
&lt;/th&gt;
&lt;th class=&#34;tg-xldj&#34;&gt;
Wins
&lt;/th&gt;
&lt;th class=&#34;tg-xldj&#34;&gt;
Draws
&lt;/th&gt;
&lt;th class=&#34;tg-xldj&#34;&gt;
Losses
&lt;/th&gt;
&lt;th class=&#34;tg-0pky&#34;&gt;
Mean Goals For
&lt;/th&gt;
&lt;th class=&#34;tg-0pky&#34;&gt;
Mean Goals Against
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
Australia
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
36
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
11
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
10
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
3.14
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
0.84
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
Argentina
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
39
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
19
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
11
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
1.69
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
0.91
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
Group Stage Performance
&lt;style type=&#34;text/css&#34;&gt;
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-xldj{border-color:inherit;text-align:left}
.tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}
&lt;/style&gt;
&lt;table class=&#34;tg&#34;&gt;
&lt;tr&gt;
&lt;th class=&#34;tg-xldj&#34;&gt;
&lt;/th&gt;
&lt;th class=&#34;tg-xldj&#34;&gt;
Wins
&lt;/th&gt;
&lt;th class=&#34;tg-xldj&#34;&gt;
Draws
&lt;/th&gt;
&lt;th class=&#34;tg-xldj&#34;&gt;
Losses
&lt;/th&gt;
&lt;th class=&#34;tg-0pky&#34;&gt;
Mean Goals For
&lt;/th&gt;
&lt;th class=&#34;tg-0pky&#34;&gt;
Mean Goals Against
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
Australia
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
2
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
2
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
6
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
1.22
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
2.22
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
Argentina
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
9
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
2
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
1
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
1.92
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
0.58
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;This is an insultingly trivial example (no one truly expects simply comparing average goals scored to be perfectly predictive, and this simply looks at the dataset as a whole, rather than considering the temporal shifts), but the magnitude of the result is still striking. In qualification matches, Australia is completely dominant, with their mean result being better than a decisive 3-1 blowout, while Argentina has a consistent but comparatively pedestrian winning record. However, in the group stage, Argentina’s performance actually &lt;em&gt;improves&lt;/em&gt;, while Australia becomes a decidedly losing team. Argentina’s route to qualification lies in the ASEAN East Asian federation, which is largely comprised of small island countries with minimal football infrastructure, while Argentina has to qualify in the elite CONMEBOL South American region, with a dozen football countries that can field top tier programs. The average result for Australia is inflated by matches such as their 31-0 defeat of Amerian Samoa, while Argentina is forced to grind out tough matches against Brazil and Uruguay.&lt;/p&gt;
&lt;p&gt;This ultimately presents a circular challenge. Every single recorded aspect of a match result is only meaningful relative to the strength of competition, and yet our goal is to estimate the strength of these teams. This is a task best suited to iteratively updating systems. For thi problem, we utilize the Elo rating system, the most widespread and popular used in competition since its invention. Elo rating systems are far from perfect, and we will see that this is another example of an occasionally problematic fit. But Elo rating systems provide numerous advantages.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Elo is naturally self correcting. A surprising result causes a larger shift in its perception of the strength of each team, so large errors tend to be smoothed out over time, with sufficient data. Elo is &lt;em&gt;not&lt;/em&gt; necessarily well suited for delicate inference tasks when the data violates its underlying assumptions, but Elo on the whole is extremely robust to these inaccuracies.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Elo has a vast array of customization. In common parlance, people frequently refer to “the Elo model”, which depending on context, is probably correct. But it does cause some people to not realize that Elo simplifies defines a class of rating systems. The constructor of the model has to make a large number of important and ultimately subjective choices in order to complete the model. This customizability allows Elo systems to adapt to a wider array of problems, and also makes this a worthwhile project, because it’s difficult to understand the choices inherent in constructing an Elo system until one tries it in action. And constructing this model will better inform us as we hear about Elo models in other contexts in the future.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;a-brief-introduction-to-elo&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;A Brief Introduction to Elo&lt;/h3&gt;
&lt;p&gt;Elo ratings provide a broad framework for continuously updating the relative strength of each national team at the time of each successive match, which we will need in order to use the past results for future predictions. The core setup is as follows. Each team has a rating (which we denote &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt;), that is updated as after each match they play. If team A and team B (with ratings &lt;span class=&#34;math inline&#34;&gt;\(R_A\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(R_B\)&lt;/span&gt;) play each other in a match, we can calculate the Expected Score for team A (&lt;span class=&#34;math inline&#34;&gt;\(E_A\)&lt;/span&gt;, shown in the equation below) in that match. Here, score is a function of the match result (not the goals scored), which in its base form is generally &lt;span class=&#34;math inline&#34;&gt;\(E_A=1\)&lt;/span&gt; for a win, &lt;span class=&#34;math inline&#34;&gt;\(E_A = 1/2\)&lt;/span&gt; for a draw, and &lt;span class=&#34;math inline&#34;&gt;\(E_A=0\)&lt;/span&gt; for loss (we will see that we can adjust this concept of “score” to account for margin of victory in a later section. Then, once we observe the result of the match (which has observed score &lt;span class=&#34;math inline&#34;&gt;\(S_A\)&lt;/span&gt;), we can use this, the expected score (&lt;span class=&#34;math inline&#34;&gt;\(E_A\)&lt;/span&gt;), and a tuning parameter &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; weighting the match (which we will discuss in detail in later sections) to update the Elo rating for team A to its new value, &lt;span class=&#34;math inline&#34;&gt;\(R_A^*\)&lt;/span&gt; (shown in Equation , with a corresponding update for team B).&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
E_A &amp;amp;= \frac{1}{1 + 10^{-(R_A - R_B)/400}}\label{eqn:eloes}\\\\\\
R_A^* &amp;amp;= R_A + K(S_A - E_A)\\label{eqn:eloupdate}
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is a naturally self correcting system, because an upset causes a large shift in score for both teams, while a result that is expected will cause a much smaller shift in ratings. This property is particularly appealing to us, because we know that a primary challenge will be that we naturally expect the strength of national teams to shift in ways different than if we were measuring the rating of an individual. As players retire and new ones join, the team composition itself can change dramatically between competitions, so we want a rating system that works to quickly adjust to data that conflicts with its previous belief. Further, if two teams play each other endlessly with a fixed probability of match outcomes, their Elo ratings will reach equilibrium, rather than endlessly diverging, even if one team has a positive winning percentage. In fact, it will reach the equilibrium where that winning percentage corresponds with the given Expected Score between this pair of Elo ratings. Certain rating systems tend to blindly reward teams for playing additional matches, while this system only cares about the quality of a team’s results. The &lt;span class=&#34;math inline&#34;&gt;\(400\)&lt;/span&gt; term in Equation above determines the scale of the rating system. This is the constant used by the FIDE chess rating system, and it implies that a team with a &lt;span class=&#34;math inline&#34;&gt;\(100\)&lt;/span&gt; point advantage in Elo rating has an Expected Score of about &lt;span class=&#34;math inline&#34;&gt;\(0.64\)&lt;/span&gt;, which is a fairly intuitive scale to work with.&lt;/p&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Click here for brief overview of some issues with Elo&lt;/summary&gt;&lt;/p&gt;
&lt;p&gt;(Quoted from original paper).&lt;/p&gt;
&lt;p&gt;
Elo rating systems are not without their flaws. For example, one study of Elo ratings in chess showed that it tends to underrate &lt;a href=&#34;http://glicko.net/research/chance.pdf&#34;&gt;the chance of an upset&lt;/a&gt; in very lopsided matches. A primary reason hypothesized for this is that weaker players tend to improve more quickly between tournaments than stronger players, which means the algorithm will tend to underestimate the weaker opponent’s chances in a match. This by itself is not likely to be a major concern for national teams in soccer. In chess, each player tends to improve as their career progresses, while this cannot universally be true among soccer national teams, as the pool of top competitors is largely fixed (besides political shifts in country definitions) and their skill is determined relative to the pool of teams. Thus, we should be concerned that the strength of teams fluctuates between World Cups (which it does), but it is unlikely to be systemically true that all teams tend to improve over time, as team strength is relative to a fixed pool of national teams.
&lt;/p&gt;
&lt;p&gt;
Precise analysis of Elo ratings requires assumptions about the dataset that are unlikely to be exactly true, but the system is somewhat robust against these inaccuracies, due to its self correcting nature. We assume that each team has some true strength at a given moment in time, which we cannot directly measure. The crucial Expected Score calculation (described below) assumes that each team has the same standard deviation for their observed performance in a given match (which is randomly distributed around their true strength at that time) . This is a core assumption that may not precisely fit our data, as it is difficult to prove that some national teams could not have a higher standard deviation of observed performance given such limited data. Further, Elo ratings assume that shifts in the true strength of a team are gradual over time. This depends on the time frame that one considers, but among soccer national teams this is unlikely to always be true. Sometimes a large number of players will retire between World Cups, or for a specific match, a crucial star player may be missing due to injury. Unfortunately, it is entirely possible for a national team to have a rapid shift in true strength. We note that this will prove problematic no matter our approach. It incentivizes us more heavily weighting extremely recent matches rather than taking a broader look at past performance. It is reasonable to place a high weight on recency, but given the sparsity of our dataset, and the inherent randomness involved in soccer, we have to strike a balance. It is trivial to find cases where a team has an excellent match on one day, and plays poorly soon after, with no changes to be found between the games, as we understand that the results of a soccer match have a relatively high variance.
&lt;/p&gt;
&lt;p&gt;
Thus, we can see that there are elements of Elo rating assumptions that are not precise fits for our data. However, by and large, similar assumptions are unavoidable for any insightful analysis, and an Elo rating system is well equipped to produce reasonable results even with some violation of assumptions. Ultimately, the way to address these concerns is to carefully examine our resulting model, and ensure that the results are intuitive and accurate along the way. Indeed, much of our work will come from trying a variety of Elo based approaches, and analyzing the results.
&lt;/p&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;the-dataset&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Dataset&lt;/h3&gt;
&lt;p&gt;We consider World Cup results (group stage, knockout, and qualifying) from 2002-2018, with the data compiled by the &lt;a href=&#34;www.rsssf.com&#34;&gt;Rec.Sport.Soccer Statistics Foundation&lt;/a&gt;. After removing an assortment of matches without results (qualifying matches are occasionally canceled or annuled for various reasons), we are left with 2964 complete match results. The primary data cleaning challenge is creating a complete mapping from these matches, to the national teams involved. Not only does the dataset use a variety of abbreviations for each country, many political borders have shifted during the course of this dataset. Luckily, this largely applies to teams who are not qualified for the 2018 World Cup, but we surely cannot ignore these because our model will be predicated on an accurate assessment of not just the teams directly qualified, but for the teams which they played in order to qualify (to give tehir results context). Thus, there is no solution for this but a laborious manual inspection of this messy situation. Ultimately, we compile a list of 115 mappings (applying to 42 different countries), with certain selections being routine (the different ways a country can be abbreviated), and others being more impactful (when countries split). Details can be found in the &lt;a href=&#34;/pdfs/doconnell_practicalexamreport.pdf&#34;&gt;paper&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The most significant choice involves the Serbian national team, which is qualified for the 2018 World Cup. Up through 2006, it competed as the joint Serbia &amp;amp; Montenegro national team, until Montenegro declared independence. As the majority of players stayed on the Serbian national team, we simply count that as a singular unified team, and consider the Montenegro national team to be “newly formed” in 2006. While the dataset is overall sparse (due to the vast number of teams in FIFA), thankfully the 32 teams for the 2018 World Cup are reasonably well represented. The least prolific is Iceland (with 31 matches recorded), but most countries have 40 to 60 that we cn use.&lt;/p&gt;
&lt;p&gt;In the next post, we will outline the myriad choices required in constructing this model.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>External Project</title>
      <link>/project/external-project/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>/project/external-project/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Internal Project</title>
      <link>/project/internal-project/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>/project/internal-project/</guid>
      <description>&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An example journal article</title>
      <link>/publication/journal-article/</link>
      <pubDate>Tue, 01 Sep 2015 00:00:00 +0000</pubDate>
      <guid>/publication/journal-article/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Slides&lt;/em&gt; button above to demo Academic&amp;rsquo;s Markdown slides feature.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Supplementary notes can be added here, including 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code and math&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An example conference paper</title>
      <link>/publication/conference-paper/</link>
      <pubDate>Mon, 01 Jul 2013 00:00:00 +0000</pubDate>
      <guid>/publication/conference-paper/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Slides&lt;/em&gt; button above to demo Academic&amp;rsquo;s Markdown slides feature.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Supplementary notes can be added here, including 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code and math&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
