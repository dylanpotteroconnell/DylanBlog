---
title: 'Bayesian Structural Time Series, and Movie Ticket Sales Sales'
author: Dylan O'Connell
date: '2020-05-28'
slug: bsts-boxofice
categories: []
tags:
  - bayesian
  - R
subtitle: ''
summary: ''
authors: []
lastmod: '2020-05-28T13:54:36-04:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---




# Introduction

In this tutorial, I'll provide an introduction to the construction of Bayesian structural time series (BSTS) models, using Steve Scott's [bsts package](https://cran.r-project.org/web/packages/bsts/bsts.pdf), applied to the total weekly box office gross (i.e. ticket sales) of American movie theaters. This is meant as a proof of concept and introduction (for myself, I'm making sure I understand the material), albeit geared at those already comfortable with statistics and R (I'll be explaining the parts unique to this application, but not any background). 

As we'll see, the class of models are likely not a particularly apt fit to the domain in question. This doesn't mean that there's no inferential value in the final result, but it does mean we have to be very conservative about what inferences we draw. BSTS models are exactly the sort of flexible structure that can provide alluringly useful accurate fits in certain contexts, only for the model to totally fall apart when stretched too far. This result is mostly due to my choice of data set (which I picked without careful consideration for its final use), but I think it's an illustrative example, because it can feel very natural to apply these models in ways which may not be theoretically sound (and they still might provide some insight in those cases, if used with appropriate caution). Above all, this is simply an opportunity for me to dip my toes into an unfamiliar topic, and I'm using this post as an excuse to articulate my own understanding of the topic, and make sure I'm comfortable with the basics.

# The Data

```{r, include = FALSE}
library(tidyverse)
library(bsts)
library(lubridate)
``` 

Our data is drawn from [Box Office Mojo]("https://www.boxofficemojo.com/weekly/"), and it tallies the total dollar gross for each week of a given year. We will consider all weeks dating back to 1993.^[I.e. my lifetime.]

Box Office Mojo also tracks a few , like "Releases",^{The total number of distinct movies which grossed money that week.},"Top Release", and whether or not it is a long weekend. 

```{r, message = FALSE}
weekly.tb <- read_csv("weekly_gross.csv")
select <- dplyr::select
weekly.tb %>% names()
weekly.tb <- weekly.tb %>%
  as_tibble() %>%
  filter(year >= 1993) 
weekly.tb %>% head(3)
```


```{r}
weekly.tb <- weekly.tb %>%
  dplyr::select(start_date, gross, releases, everything())
```

```{r}
weekly.tb %>%
  ggplot(aes(x = start_date, y = gross)) + 
  geom_line() + 
  xlab("Year") + 
  ylab("Gross ($)") + 
  ggtitle("Weekly (American) Box Office Gross") 
```


# Bayesian Structural Time Series Model
For this introduction, we will cite a Bayesian Structural Time Series (BSTS) model. We can take this piece by piece. A "Time Series" is a sequence of data points which are indexed by time. It's a particular category of statistics, because it fundamentally violates the core "independent and identically distributed" assumption that underpins much of statistical inference. While that renders many classical statistical procedures infeasible, the temporal relationship amongst the data is the whole point! We want to understand this evolution, which means that we must rely on a model which leverages this index on time. For notation, we write $y_1, \ldots, y_T$ for the response variables of interest (in this case, the log gross).

"Bayesian" follows its standard applied definition, as our objective is not a hypothesis test of point estimate with some desirable properties, but rather a posterior distribution on the parameters of interest (the parameters which describe our time series model). These parameters are determined by the "Structural" part of the model, whcih requires a careful definition. 

## The "Structural" Model

A "structural time series" is a model of response data indexed by time, whose temporal relationship is defined by the interplay of two equations. First, we have the *transition* equation, which models how a latent (i.e. unobserved state) $\alpha_t$ evolves over time. 
\begin{align}
\alpha_{t+1} &= T_t \alpha_t + R_t \eta_t, ~~~~~~ \eta_t \sim N(0, Q_t)\label{eqn:bsts_transition}
\end{align}
Before we explain any of these pieces, it's best to introduce the second part, the \emph{observation} equation, which models how the observed response $y_t$ is generated using the state variable, $\alpha_t$.
\begin{align}
y_{t} &= Z_t^T \alpha_t + \epsilon_t, ~~~~~~ \epsilon_t \sim N(0, H_t).\label{eqn:bsts_observation}
\end{align}
Thus, while a classic time series might model the relationship between $y_t$ and $y_{t+1}$ directly, a *structural* time series model instead links them through some latent, unobserved state, $\alpha_t$, which evolves underneath unseen, and influences the observed $y_t$ values. 

This is a sensible setup because often time series are understood to be random observations of some underlying "state" in our system, which we cannot observe directly. If you conduct monthly opinion polling with a relatively small sample, this can readily be understood as a structural time series, because the target of interest is the underlying latent state (the public's opinion on a particular issue), while the observations are noisy datum which are heavily influenced by that latent state. Thus, it makes sense for us to model our understanding of the latent state itself, rather than the latest noisy observation.^[Not classical time series don't have good ways of handling this problem too! While it would indeed be foolish to consider the prior observation, the sole piece of information denoting the current state, time series models have a rich array of tools to handle these complex forms of autocorrelation.]

With this understanding, the rest of the terms fall into place. $Z_t$ is the *observation* vector, whose coefficients define the relationship between the latent state and the response (subject to the noise, $\epsilon_t$). $T_t$ is the *state transition matrix*, which determines the evolution of the latent state, along with the noise $\eta_t$ (with state-diffusion matrix $Q_t$). 

This is the most general statement, and it covers a vast array of potential models. In particular, the allure of this structure is that these models are naturally *additive*, which makes for easy modular construction. In this brief introduction, I won't go into much detail about its potential. 

Before we proceed to fitting the data itself, a brief note about the normal errors, $\eta_t$ and $\epsilon_t$. The particular computational appeal of this model is that the additive composition of these normal errors allows the posterior distributions to be normal themselves. This structure allows for the use of the Kalman filter introduction quite clear.] to compute the estimate the distributions of $\alpha_t$ and $y_t$. 

The construction of the Kalman filter (and smoother) is a bit tricky to explain, so it is ommitted here,^[I found [this](https://eml.berkeley.edu/~rothenbe/Fall2007/kalman.pdf) but the important punchline is that this model requires normally distributed errors. There is no particular reason to assume that the errors of weekly box office gross should be normally distributed! We will investigate this further, but in the use of these models, often far too little attention is paid to the model specification (as we will see that we obtain some sensible results, even with a poorly fitting model). 

## Computation

The [bsts package](https://cran.r-project.org/web/packages/bsts/bsts.pdf) is developed by Steve Scott, and provides a host of tools to make ABDDF. His specific approach, combining the Kalman filter with a spike-and-slab prior, was introduced in a [neat paper](https://www.nber.org/chapters/c12995) showing how search trends could be used to "Nowcast".^[Forecasting an unobserved response occurring in the given moment] The bsts package allows for the quick construction of common BSTS models with minimal effort, and provides a host of tools for their analysis.

It's worth noting that Tensorflow has its own package for Bayesian structural time series modeling ([tfp.sts](https://www.tensorflow.org/probability/api_docs/python/tfp/sts)), which provides a useful alternative. 


# Cleaning the Data



## Inflation Adjustment

While there's an obvious upward trend in our time series, just about everything has gotten more expensive in the past 30 years, due to inflation. Our first step is to work with the inflation adjusted dollar weekly gross. If this cleaning step is not taken, and a time series is fit blind to the data, you can waste your time chasing after some temporal trend that simply reflects this natural increase in prices. We cite the priceR package, which can download the relevant CPI data and do the work for us. 


```{r}
library(blscrapeR)
inf_adj <- inflation_adjust(2019)

weekly.tb <- weekly.tb %>% left_join(inf_adj %>%
                                       select(year, 
                                              adj_value) %>%
                                       mutate(year = as.numeric(year)),
                                     by = "year") %>%
  mutate(gross_ia = gross/adj_value) %>%
  select(-adj_value)


# library(priceR)
# 
# weekly.tb <- weekly.tb %>%
#   mutate(gross_ia = adjust_for_inflation(gross, 
#                                          year, 
#                                          country = "US", 
#                                          to_date = 2019))

```

```{r}
weekly.tb %>% 
  group_by(year) %>%
  summarize(mean_gross = mean(gross),
            mean_gross_ia = mean(gross_ia)) %>%
  ungroup() %>%
  ggplot(aes(x = year)) + 
  geom_line(aes(y = mean_gross), col = "red") + 
  geom_line(aes(y = mean_gross_ia), col = "blue")
# Add labels?
```

Thus, the underlying trend is still real, and the surge in box office gross from the 90s through the early 2000s appears quite meaningful. But the natural inclusion of inflation does help reduce some of the impact of the trend, which may influence our modeling.^[While this is an incredibly obvious correction, and I left it until now mostly to make a point about an easy but damaging mistake, it's also not quite so apparent that it is a perfect and tidy fix. Between the 90s and today, it likely is sufficient, but there are other notable societal changes that can make comparing ticket sales difficult.]

## The Scale of the Response Variable

We  will fit our model onto the log of the box office gross (using base 10, rather than the typical $e$, as this has a more intuitive numerical conversion for these large decimals). Intuitively, box office cannot be negative, so if we are fitting models which assume some sort of normality, this conversion is natural. Of course, we will see that while such a conversion fixes the most glaring violation of the assumptions, that does not automatically mean that the model fits! In fact, in this exploration we will see just how difficult it is to find a tidy model for box office gross.


```{r}
weekly.tb <- weekly.tb %>% mutate(log_gross_ia = log10(gross_ia),
                                  log_gross = log10(gross))
```

However, we will plan to *evaluate* the model on the regular scale, as we could imagine that "dollars" tend to mean more to any interested party than "log dollars" (a rather made-up measurement). I'm not sure if there is a rigorous choice here, surely it depends on the setting. But we are choosing the log transformation not because it captures the trend we care about, but because it seems to more plausibly fit the *model* we plan to use. In this case, the model should be evaluated based on the target of interest.^[If we evaluate this transformation, the Mean Absolute Percentage Error becomes nonsensical, where being off by a factor of two leads to only a small penalty.]

# Our Theoretical Model

From here, we could simply fit the time series itself, and look at the results. First, we consider the insights of Tassone & Rohani on the [Google Unofficial Data Science Blog](http://www.unofficialgoogledatascience.com/2017/04/our-quest-for-robust-time-series.html). Their excellent post covers a range of topics regarding the application of time series, with a particular focus on robust systems that can be applied robustly at scale (that is, applied in a wide range of unknown situations, and provide reaonable outcomes). That is, at scale, you often do not have the luxury of considering the underlying mechanism of the time series. They cite the power of aggregating a wide range of models, whose fit onto individual components provides the desired robustness to unknown inputs. 

However, their goal is purely predictive, rather than inferential (i.e. "What happens?" not "Why it happens?").^[In this post, I use "Predictive" to refer to the task of forecasting the future response (including in the very next step), and "Inferential" to refer to understanding trends in the data (e.g. the posterior distribution of the parameters in a sensible Bayesian model should provide useful inference.]  In our case, we have the luxury of thinking about the data setting, and considering what types of models make any sense. Thus, we begin with a discussion of the limits of our inference, before considering several of the tools available, and planning which models we can fit. As we will see, our goals are not necessarily a good fit for a BSTS model! The goal of this post is simply to introduce myself to these tools, and part of that is recognizing these limits. 

## The Limits of Prediction and Inference

It's important to start with the *limits* of the domain. If the goal of our time series was to model the Cosmic Microwave Background noise over time, we can stop right away, because it consists of entirely irreducible uncertainty, and our model has nothing to offer. More commonly, we might not have a *firm* theory of the data, but we have a rough idea of connections which might exist, and we wish to fit some models and see which have predictive and inferential merit. 

There are clear limits to our capabilities given this dataset. Intuitively, the most important factor in determining the weekly box office gross has nothing to do with the general "state of the movie business", and it has everything to do with *what movie was just released*. This presents a significant impediment to our modeling. We have an intuitive grasp of the theory, but it's not something that can easily be modeled! Of course, you *can* model the ticket sales of the biggest new release, but that's a separate challenge, and has little to do with this time series. 

<!-- In an ideal world, every modeling challenge would aim to perfectly capture the theory of the setting. In practice, we often work in situations like these. This post aims to analyze the *limits* of such an appraoch. How "far" can a BSTS model get us, even when we know that it is not a perfect fit for the setting? Reading about people practicing these models online, it's common to find settings with such a theoretical mismatch. -->

Now, assuming that we don't have prescience about the release of "Avengers: Endgame", what are some reasonable theoretical assumptions we can make? Well, first, we might expect the continuity of which movies are in theaters to provide us with a natural form of autocorrelation. The rate of decline in a movie's second week at the same theater is a [widely studied](https://en.wikipedia.org/wiki/Second_weekend_in_box_office_performance) phenomenom, and it is consistently between 40 and 60 percent. Now, there are dozens of movies contributing to the total weekly gross, so the relationship won't be nearly so tidy, but this still provides a clear form of autocorrelation.^[Later, we will consider an attempt to capture this an expected reversion to normalcy after a huge opening weekend.] In the subsequent section, we look at this in some more detail.

More broadly, we might intuitively expect there to be "trends" in the movie industry. The end of the 70s saw the rise of the blockbuster, the grosses of the biggest hits has steadily risen,^[As we will see in a later section, this deserves more careful consideration.], and the relative popularity of different genres is always in flux. We might attempt to capture this with some sort of local slope for the latent state.

Further, we might expect there to be seasonal *weekly* effects. Box office grosses are commonly compared to the same weeks in previous years, because of the assumption of consistent patterns. It is understood that over Christmas, many people go to the movies, whereas the theaters are comparatively dry throughout February.

Finally, we could find a more nuanced understanding of the autocorrelation. By its nature, a BSTS model introduces autocorrelation, because response data which are close in time are linked by the underlying latent state. Given that we expect there to be some degree of continuity movie theaters from week to week (as movies phase in and out slowly, in addition to the aforementioned trends), this is fitting, but the point is that it is possible to be much more precise with how the autocorrelation is modeled (for instance, perhaps a sharp increase in ticket sales in one week implies a sharp drop the next, due to a single huge movie release). The point of this post is for my own education, not to actually produce the most accurate model, so I proceed with a more straightforward BSTS model here, and I will critique it if it appears to be a poor fit. In a later section, I will analyze the autocorrelation of this data in more detail.

# Initial Models

We begin by fitting a few starter models, which we can use to demonstrate the tools to analyze these models. While we will follow our insight above, we will see that without more care, there is little reason to think that these models are a precise fit.

We can start with a local linear trend, which provides a common and flexible state space model (this is a useful starting point because of its generality, but that does not guarantee that it reflects the situation). In short, a local linear trend assumes that both the mean, the trend (i.e. the slope) of the latent state evolution are drawn from a random walk. That is, if $\mu$ is the mean of the latent state (see Equation \ref{eqn:bsts_transition}).^[Most of this is grabbed right from the bsts manual, whose documentation is very helpful.]
\begin{align*}
\mu_{t+1} = \mu_t + \delta_t + \epsilon_{\mu, t},
\end{align*}
where $\epsilon_{\mu,t} \sim N(0, \sigma_\mu^2)$ is our random walk. Then, the evolution of the slope is similarly given by 
\begin{align*}
\delta_{t+1} = \delta_t + \delta_t + \epsilon_{\delta, t},
\end{align*}
where $\epsilon_{\delta,t} \sim N(0, \sigma_\mu^2)$ is another random walk. How these terms relate to our overall transition and observation equations will be clear once they are combined. 

Next, we consider the "seasonal" model, that is, any fixed time cycle with $S$ possible states in the cycle). For these $S$ seasons, we create an $S-1$ dimension state vector $\gamma_t$ (as we constrain the coefficients to sum to $1$) and in essence our model performs regression using these coefficients and dummy variables corresponding to the current season for the given time $t$. Then, the first element of the season state vector $\gamma_t$ evolves as 
\begin{align*}
\gamma_{t+1, 1} = -1*\sum_{s=2}^S \gamma_{t, s} + \epsilon_(\gamma),
\end{align*}
where $\epsilon_(\gamma) \sim N(0, \sigma_\gamma^2)$. 

<!-- "If predictor variables are present, the regression coefficients are fixed (as opposed to time varying, -->
<!-- though time varying coefficients might be added as state component). The predictors and response -->
<!-- in the formula are contemporaneous, so if you want lags and differences you need to put them in -->
<!-- the predictor matrix yourself." -bsts Docs -->

With these two commponents, we can fit our twin initial models. Our "season" model consists of just the seasonal component, with $S=52$ allowing a regression on every single unique week of the year, and the "LLT" model adding to that a local linear trend. When just using the seasonal model, we need to add to it a static intercept term, so that the mean isn't wildly off. While the purpose of this is more educational than anything else, it's useful to be clear on what this could show. Comparing these models essentially takes for granted the folk wisdom that the precise week of the year influences the "state" of ticket sales. Then, comparing these models can help understand if there is some basis for tracing local trends in the movie business, or if that is just overfitting to noise.^[Of course, if neither model captures the situation, this inference can be complicated, but it's still useful to explore the area.]

The "bsts" package makes fitting these models extremely easy. The "state specification" is a list of state components. We construct a seasonal state space, add a static intercept, and fit the model. This post is meant as a reminder of the underlying theory, so it won't go into the technical details of what the model produces (which are covered in the documentation), but broadly the model object which results from bsts()  contains the relevant parameters and samples from the posterior distribution which we want (with the one technical caveat that it returns all iterations of its posterior sampler, so if you want to access those samples yourself, you need to remove the burn-in segment manually).  

```{r}
ss.season <- AddSeasonal(list(), 
                         weekly.tb$log_gross_ia, 
                         nseasons = 52)
ss.season.intercept <- AddStaticIntercept(ss.season,
                                          weekly.tb$log_gross_ia)
model.season <- bsts(weekly.tb$log_gross_ia, 
                     state.specification = ss.season.intercept,
                     niter = 500, 
                     ping = 0, 
                     seed = 21)
```
As the models are modular, to create our local linear trend state specification, we simply add the local linear trend to our already specified seasonal component.
```{r}
ss.llt <- AddLocalLinearTrend(ss.season,
                              weekly.tb$log_gross_ia)
model.llt <- bsts(weekly.tb$log_gross_ia, 
                  state.specification = ss.llt,
                  niter = 500, 
                  ping = 0, 
                  seed = 21)
```

# Comparing the Models

It should first be said that models cannot be analyzed in isolation. In practice, we should begin by specifying our goals, and the costs associated with poor results. Here, we sketch out some relevant tools to analyze these forecasts, recognizing that their actual application is dependent on the specific context.

## The "bsts" Package Tools
The "bsts" package comes preloaded with useful functions for analysis of the results. We start with "CompareBstsModels", which charts the cumulative absolute error of our one-step predictions.

```{r}
CompareBstsModels(list(Season = model.season, 
                       LLT = model.llt))
```
With a single line of code, we can see how the additional flexibility of the local linear trend does indeed make our predictions more accurate. 

However, if you're unfamiliar with BSTS models, it's not immediately clear what exactly this is measuring, and it's important to be very precise. "Cumulative absolute error" is clear enough, but what errors is it referring to? These are the "one-step-ahead" prediction errors, where the data points are drawn from the posterior computed by the Kalman filter. Unfortunately, we can't be too rigorous about this description without going into the theoretical details of the filter, but the gist is that the Kalman filter computes the posterior distribution for the next response data point, $p(y_t | y_{1:t-1})$, recursively through the posterior on the latent state posterior, $p(\alpha_t | y_{1:t-1})$.^[Where $y_{1:t} := (y_1, \ldots, y_t)$, matching the typical Kalman filter notation.] Our model object contains posterior draws (in our case, 500 of them) for each posterior distribution $p(y_t | y_{1:t-1})$. The "errors" above are the difference between these posterior samples, and the actual observed data point.

This poses a problem for applying this function directly to our setting. The mean absolute prediction errors are computed on the response data, which are logarithms, whereas we are most likely interested in errors on the regular scale. The fix is simply to grab the relevant parts of the CompareBstsModels() [source code](https://rdrr.io/cran/bsts/src/R/compare.bsts.models.R), and do a bit of surgery until it suits our purpose. We take the one-step errors directly from the model object, and remove the burn-in. Then, we convert from posterior errors in the log scale, to posterior errors on the regular scale. If such a log-scale posterior error is $\hat{e}$, our response (in regular scale, not log) as $y$, and our predicted value as $\hat{y}$, we have $\hat{e} = \log_{10} \hat{y} - \log_{10}y$. Thus, the errors we want are given by 
\begin{align*}
\hat{e} &= \log_{10} \hat{y} - \log_{10}y\\
\hat{y} - y &= 10^{\hat{e}}y -y = (10^{\hat{e}}-1)y.
\end{align*}

```{r}
# Taken directly from the source code, with some extraneous
# features removed, and conversion added.
CompareBstsModelsLog <- function(model.list) {
  main <- "Cum. One-Step Error"
  burn <- SuggestBurn(.1, model.list[[1]])
  model.names <- names(model.list)
  if (is.null(model.names)) {
    model.names <- paste("Model", 1:length(model.list))
  }
  number.of.models <- length(model.list)
  opar <- par(mfrow = c(2, 1))
  original.margins <- c(5.1, 4.1, 4.1, 2.1)
  margins <- original.margins
  opar$mar <- original.margins
  margins[1] <- 0
  par(mar = margins)
  cumulative.errors <- matrix(nrow = number.of.models, 
                              ncol = length(model.list[[1]]$original.series))
  for (i in 1:number.of.models) {
    # The prediction errors are in the log scale, and we 
    # grab them directly from the model itself.
    e.hat.log <- model.list[[i]]$one.step.prediction.errors
    e.hat.log <- e.hat.log[-(1:burn), , drop = FALSE]
    # We compute our prediction error as \hat{y} - y = (10^{\hat{e}}-1)y
    prediction.errors <- (10^e.hat.log - 1)*10^model.list[[1]]$original.series
    cumulative.errors[i, ] <- cumsum(abs(colMeans(prediction.errors)))
  }
  colors <- c("black", rainbow(number.of.models-1))
  
  # We can add our own timestamps
  idx <- model.list[[1]]$timestamp.info$timestamps
  plot(zoo(cumulative.errors[1, ], order.by = idx),
       ylim = range(cumulative.errors),
       ylab = "cumulative absolute error",
       xaxt = "n",
       lwd = 2,
       yaxs = "i")
  axis(2)
  for (i in 2:number.of.models) {
    lines(zoo(cumulative.errors[i, ], order.by = idx),
          lty = i,
          col = colors[i],
          lwd = 2)
  }
  grid()
  legend("topleft",
         model.names,
         lty = 1:number.of.models,
         col = colors,
         bg  = "white",
         lwd = 2)
  
  margins <- original.margins
  margins[3] <- 0
  par(mar = margins)
  plot(model.list[[1]]$original.series,
       main = "",
       ylab = "scaled values",
       xlab = "Date",
       yaxs = "i")
  grid()
  par(opar)
}
CompareBstsModelsLog(list(Season = model.season, 
                          LLT = model.llt))
```
Of course, this transformation doesn't totally change the picture, it's still measuring the same errors. But the units of absolute error are now interpretable, the difference between the two is more clear, and there could be cases where computing the errors based on the logs would lead to unintuitive results. 

Further, as we keep repeating, one should think carefully about the goals of the analysis. The cumulative error is an intuitive metric, but it's not particularly Bayesian (as it is computed using a simple point estimate on the posterior). If the ultimate output of the model is a sequence of these point estimates, then it is a sensible place to start. If the goal is to construct a full posterior probability that captures the theoretical situation, it won't be very meaningful. 


The "bsts" package offers a [variety of other plotting tools](https://rdrr.io/cran/bsts/man/plot.bsts.html) (such as PlotBstsComponents(), PlotBstsForecastDistribution(), PlotSeasonalEffect(), and more), but they didn't appear very informative when applied directly out of the box to this data set. Which isn't surprising, it's hard to write general purpose plotting functions. In almost all cases, it's better to construct your own, to tackle the task at hand. 
<!-- # ```{r} -->
<!-- # PlotBstsComponents(model.season) -->
<!-- # ``` -->

<!-- ```{r} -->
<!-- ss.slt <- AddSemilocalLinearTrend(ss.season, -->
<!--                                   weekly.tb$log_gross_ia) -->
<!-- model.slt <- bsts(weekly.tb$log_gross_ia,  -->
<!--                   state.specification = ss.slt, -->
<!--                   niter = 500,  -->
<!--                   ping = 0,  -->
<!--                   seed = 21) -->

<!-- CompareBstsModels(list(Season = model.season,  -->
<!--                        LLT = model.llt, -->
<!--                        SLT = model.slt)) -->
<!-- ``` -->


<!-- ```{r} -->
<!-- ss.slt <- AddSeasonal(list(), weekly.tb$log_gross_ia,  -->
<!--                       nseasons = 52) -->
<!-- ss.llt <- AddSemilocalLinearTrend(ss.slt, -->
<!--                                   weekly.tb$log_gross_ia) -->
<!-- model.llt <- bsts(weekly.tb$log_gross_ia,  -->
<!--                   state.specification = ss.slt, -->
<!--                   niter = 500,  -->
<!--                   ping = 0,  -->
<!--                   seed = 21) -->

<!-- CompareBstsModels(list(Season = model.season, -->
<!--                        Season2 = model.season)) -->
<!-- ``` -->

## More Specific Tools

We might want to take the point estimates from the CompareBstsModels() function, and actually chart the fit itself. 

Explain?...

Much of this code is directly taken and modified by Kim Larsen's [excellent blog post](https://multithreaded.stitchfix.com/blog/2016/04/21/forget-arima/).
```{r}
# Take in a model, and return the one-step fitted matrix 
ComputeOneStepFitted <- function(model, dat, response_var) {
  burn <- SuggestBurn(0.1, model)
  response_var <- enquo(response_var)
  Actual_Log <- dat %>% pull(!! response_var)
  tb <- tibble(Fitted = 10^(-colMeans(
    model$one.step.prediction.errors[-(1:burn),])
    + Actual_Log),
    Actual = 10^(Actual_Log),
    Date = dat$start_date)
  return(tb)
}

ComputeMAPE <- function(tb) {
  tb %>% 
    summarize(MAPE = mean(abs(Actual - Fitted)/Actual))
}

PlotOneStepFitted <- function(tb, 
                              Response_Label = 
                                "Log Inf. Adj. Gross ($)") {
  MAPE <- ComputeMAPE(tb)
  tb %>% 
    ggplot(aes(x = Date)) +
    geom_line(aes(y = Actual, color = "Actual"), 
              size = .3) +
    geom_line(aes(y = Fitted, color = "Fitted"), 
              size = .3, linetype = 2) +
    theme_bw() + 
    theme(legend.title = element_blank()) + 
    ylab(Response_Label) + xlab("") +
    geom_vline(xintercept = as.numeric(as.Date("1993-01-01")), 
               linetype = 2) + 
    ggtitle(paste0("BSTS -- One-Step MAPE = ", round(100*MAPE,2), "%")) +
    theme(axis.text.x=element_text(angle = -90, hjust = 0))
}
```

```{r}
tb.llt <- ComputeOneStepFitted(model.llt, weekly.tb, log_gross_ia)
PlotOneStepFitted(tb.llt)

tb.season <- ComputeOneStepFitted(model.season, weekly.tb, log_gross_ia)
PlotOneStepFitted(tb.season)
```


## Coefficients?


# Brief Mention of Long Term Forecasting 

Throughout this exploration, I have mentioned how I am being purposefully fuzzy about the goals of the model. It is important to emphasize that, because in any real application, these goals would have to be foregrounded in the construction of *any* model (whereas the point of this post is to just explore the tools). One example of this is in a critical distinction between types of prediction: forecasting in the long run, and forecasting the next step (or "nowcasting"). 

Thomas Olavson's [blog post](http://www.unofficialgoogledatascience.com/2019/12/humans-in-loop-forecasting-integrating.html) on the objective automation of forecasting helps clarify this distinction. He calls the long run objective "strategic forecasting, which has low update frequency, uncertainty which is difficult to quantify, and whose outputs are used in high stakes decision making (with significant human interpretation). By comparison, "tactical forecasting" is a mostly automated process, making countless micro predictions at scale, whose automation can be guided by frequent backtesting (and related strategies). 

In theory, it seems that Bayesian Structural Time Series models should be perfectly serviceable at either goal. The original paper by Scott & Varian focuses on the short term nowcasting, but the interpretability of the Bayesian framework makes longer term forecasts a reasonable application. However, the models may be built quite differently when we keep these goals in mind. 

Implicitly, our work this far has been geared towards tactical forecasting, using the model to construct point estimates for each subsequent time step, relying on a relatively flexible fit to the data.

Our full model computed earlier uses the Kalman smoother and is fit over *all* the data. Thus, to get a sense of how this might work for forecasting, we need to turn to a training and test set structure. We pick the cut-off arbitrarily at the end of 2016 (we would hope that the model is relatively agnostic about this choice, but we will see that in this case, it is not). In a more serious application, it would likely make sense to compute models for a sequence of forecast cut-offs, but this choice is just for a quick demonstration.

```{r}
cutoff <- 2016
train.tb <- weekly.tb %>% 
  filter(year <= cutoff)
test.tb <- weekly.tb %>% 
  filter(start_date > cutoff)
```

We again compute the same two models as before. 
```{r}
# On the training data, fit the just seasonal model, and then add LLT
ss.season.train <- AddSeasonal(list(), 
                               train.tb$log_gross_ia, 
                               nseasons = 52)
ss.season.intercept.train <- AddStaticIntercept(ss.season.train,
                                                train.tb$log_gross_ia)
model.season.train <- bsts(train.tb$log_gross_ia, 
                           state.specification = ss.season.intercept.train,
                           niter = 500, 
                           ping = 0, 
                           seed = 21)

p.season <- predict.bsts(model.season.train, 
                         horizon = weekly.tb %>% nrow() - 
                           train.tb %>% nrow())
```


```{r}

ss.llt.train <- AddLocalLinearTrend(ss.season.train,
                                    train.tb$log_gross_ia)
model.llt.train <- bsts(train.tb$log_gross_ia, 
                        state.specification = ss.llt.train,
                        niter = 500, 
                        ping = 0, 
                        seed = 21)

p.llt <- predict.bsts(model.llt.train, 
                      horizon = weekly.tb %>% nrow() - 
                        train.tb %>% nrow())
```


```{r}
plot(p.season, plot.original = 52*2)
plot(p.llt, plot.original = 52*2)

```
Punchline: both models, again seem similar, with the more flexible model capturing perhaps a bit more of at trend, but broadly that difference not appearing quite as meaningful when it comes to forecasting. 

We can rework our earlier functions to now compute MAPE for both fitted and predicted.

```{r}
library(gridExtra)
# Take in a model, and return the one-step fitted matrix 
ComputeOneStepPredicted <- function(model, dat, response_var, p) {
  burn <- SuggestBurn(0.1, model)
  response_var <- enquo(response_var)
  Actual_Log <- dat %>% pull(!! response_var)
  tb <- tibble(Fitted = c(10^as.numeric(-colMeans(
    model$one.step.prediction.errors[-(1:burn),])
    + model$original.series), 10^p$mean),
    Actual = 10^(Actual_Log),
    Date = dat$start_date)
  return(tb)
}

PlotOneStepPredicted <- function(tb, 
                                 p,
                                 cutoff,
                                 Response_Label = 
                                   "Log Inf. Adj. Gross ($)") {
  MAPE <- ComputeMAPE(tb)
  posterior.interval <- cbind.data.frame(
    10^as.numeric(p$interval[1,]),
    10^as.numeric(p$interval[2,]), 
    subset(tb, year(Date) > cutoff)$Date)
  names(posterior.interval) <- c("LL", "UL", "Date")
  tb <- left_join(tb, posterior.interval, by="Date")

  plot.full <- tb %>% 
    ggplot(aes(x = Date)) +
    geom_line(aes(y = Actual, color = "Actual"), 
              size = .3) +
    geom_line(aes(y = Fitted, color = "Fitted"), 
              size = .3, linetype = 2) +
    theme_bw() + 
    theme(legend.title = element_blank()) + 
    ylab(Response_Label) + xlab("") +
    geom_vline(xintercept = as.numeric(as.Date(paste0(cutoff, "-12-31")), linetype=2)) + 
  geom_ribbon(aes(ymin=LL, ymax=UL), fill="grey", alpha=0.5) +
    ggtitle(paste0("BSTS -- Total One-Step MAPE = ", round(100*MAPE,2), "%")) #+
   # theme(axis.text.x=element_text(angle = -90, hjust = 0))
  
  plot.forecast <- subset(tb, year(Date) > cutoff) %>%
    ggplot(aes(x = Date)) +
    geom_line(aes(y = Actual, color = "Actual"), 
              size = .3) +
    geom_line(aes(y = Fitted, color = "Fitted"), 
              size = .3, linetype = 2) +
    theme_bw() + 
    theme(legend.title = element_blank()) + 
    ylab(Response_Label) + xlab("") +
    geom_vline(xintercept = as.numeric(as.Date(paste0(cutoff, "-12-31")), linetype=2)) + 
  geom_ribbon(aes(ymin=LL, ymax=UL), fill="grey", alpha=0.5) 
    #ggtitle(paste0("BSTS -- One-Step MAPE = ", round(100*MAPE,2), "%")) #+
    #theme(axis.text.x=element_text(angle = -90, hjust = 0))
  grid.arrange(plot.full, plot.forecast, nrow = 2)
}
```

```{r}
plot(p.season, plot.original = 52*2)
plot(p.llt, plot.original = 52*2)
tb.season.train <- ComputeOneStepPredicted(model.season.train,
                                     weekly.tb,
                                     log_gross_ia,
                                     p.season)
PlotOneStepPredicted(tb.season.train, p.season, 2016)
```

```{r}
tb.llt.train <- ComputeOneStepPredicted(model.llt.train,
                                     weekly.tb,
                                     log_gross_ia,
                                     p.llt)
PlotOneStepPredicted(tb.llt.train, p.season, 2016)
```



However, we can demonstrate how little it takes for a model to go from simply incomplete, to totally nonsensical. What happens if we remove our seasonal correction? We certainly would expect our accuracy to plummet, we've established that the week of the year is a critical predictor of the weekly gross. However, what we'll see below, that the removal of the seasonal adjustment makes the model completely useless when it comes to longer term forecasting, not simply "inaccurate".

```{r}
# LLT, unadjusted, no season
ss.base <- AddLocalLinearTrend(list(),
                               train.tb$log_gross_ia)
model.base <- bsts(train.tb$log_gross_ia, 
                   state.specification = ss.base,
                   niter = 500, 
                   ping = 0, 
                   seed = 21)

p.base <- predict.bsts(model.base, 
                       horizon = weekly.tb %>% nrow() - 
                         train.tb %>% nrow())

plot(p.base, plot.original = 52*2)

CompareBstsModelsLog(list(model_base = model.base,
                          model_llt.seasons = model.llt.train))

```



```{r}
tb.llt.train <- ComputeOneStepPredicted(model.llt.train, 
                                   weekly.tb, 
                                   log_gross_ia, 
                                   p.llt)

PlotOneStepPredicted(tb.llt.train, p.llt, 2016)
```

```{r}
tb.base <- ComputeOneStepPredicted(model.base, 
                                   weekly.tb, 
                                   log_gross_ia, 
                                   p.base)
tb.base %>% tail()
#PlotOneStepPredicted(tb.base, p.base, 2016)
```
It goes without saying that the forecasting 
The 95% prediction intervals in this case range from 

```{r}
# Check if this is right...
tb.temp  <- ComputeOneStepFitted(model.base, 
                                   train.tb, 
                                   log_gross_ia)
PlotOneStepFitted(tb.temp)
```




```{r}
# This is for not adjusting for inflation. 
# ss.season.train.orig <- AddSeasonal(list(), 
#                                train.tb$log_gross, 
#                                nseasons = 52)
# ss.llt.train.orig <- AddLocalLinearTrend(ss.season.train,
#                                          train.tb$log_gross)
# model.llt.train.orig <- bsts(train.tb$log_gross, 
#                              state.specification = ss.llt.train.orig,
#                              niter = 500, 
#                              ping = 0, 
#                              seed = 21)
# 
# p.llt.orig <- predict.bsts(model.llt.train.orig, 
#                            horizon = weekly.tb %>% nrow() - 
#                              train.tb %>% nrow())
# plot(p.llt.orig, plot.original = 52*2)

```



<!-- TO BE CONTINUED  -->



<!-- # Unfinished  -->

<!-- ## Exploring the Autocorrelation -->

<!-- One challenge is that these effects are all closely related. The introduction of regular seasonal effects can mask effects of auto correlation. To be honest, I don't know the best way to solve this. Eventually, the model should be well equipped to sort them out, but in the exploration stage, it's not so clear. For now, I will simply resort to a crude method at de-seasoning the model, by normalizing the grosses by the mean gross for each of the 52 weeks. This should help uncover autocorrelative effects. However, the model itself will be fit on data without this adjustment, because it will apply a more delicate (and Bayesian) treatment to these seasonal terms. I'm curious what the best pratice here would be.  -->

<!-- ```{r} -->
<!-- weekly.tb <- weekly.tb %>%  -->
<!--   group_by(week) %>% -->
<!--   mutate(week_mean_ia = mean(gross_ia)) %>% -->
<!--   ungroup() %>% -->
<!--   mutate(gross_ia_week_adj = gross_ia - week_mean_ia + mean(gross_ia)) -->
<!-- ``` -->

<!-- We use the Acf function from the "forecast" package, which computes the correlation of the data for each "lag" (the number of steps back). Thus, we can see here that there is a strong positive correlation between the inflation adjusted weekly gross and the previous week, but once you get to around 10 weeks or so, the correlation becomes negative (if slight).  -->

<!-- ```{r} -->
<!-- library(forecast) -->
<!-- Acf(weekly.tb$gross_ia) -->
<!-- ``` -->
<!-- However, this is applied to our original data, and the presumed seasonal effects might be causing potential problems. When we instead compute an ACF plot for the week-normalized gross, we see instead that the positive autocorrelation is *consistent*. This makes a good deal of sense. The 10 to 15 weeks we were looking at above is a long enough period of time, that you essentially switch "movie seasons". Thus, intuitively it fits that there might be some negative correlation, as the low point of the movie year (around February and March), are roughly 10-20 weeks offset from some of the busiest movie times (Christmas, and the summer, in the two directions). -->

<!-- ```{r} -->
<!-- Acf(weekly.tb$gross_ia_week_adj) -->
<!-- ``` -->

<!-- This local trend is typical of most time series models, but we might want to consider a slightly more advanced version.  -->
<!-- Now, we apply [differencing](https://otexts.com/fpp2/stationarity.html)  -->




<!-- ```{r} -->
<!-- temp <- weekly.tb %>%  -->
<!--   transmute(start_date = start_date, -->
<!--             gross_ia = gross_ia, -->
<!--             diff_gross_ia = gross_ia - lag(gross_ia), -->
<!--             diff2_gross_ia = diff_gross_ia - lag(diff_gross_ia))  -->

<!-- Acf(temp$gross_ia) -->

<!-- Acf(temp$diff_gross_ia) -->

<!-- Acf(temp$diff2_gross_ia) -->


<!-- ggplot(aes(x = start_date)) + -->
<!--   geom_line(aes(y = diff2_gross_ia))+ -->
<!--   xlab("Year") + ylab("") + -->
<!--   ggtitle("") -->


<!-- ``` -->




<!-- ```{r} -->


<!-- # Not sure what's goign on here, reinvestigate -->
<!-- weekly.tb <- weekly.tb %>% -->
<!--   mutate(diff_log_gross_ia = log_gross_ia - lag(log_gross_ia), -->
<!--          diff2_log_gross_ia = diff_log_gross_ia - -->
<!--            lag(diff_log_gross_ia)) -->

<!-- ss.r.diff2 <- AddSeasonal(list(),  -->
<!--                           weekly.tb$log_gross_ia,  -->
<!--                           nseasons = 52) -->

<!-- model.rdiff2 <- bsts(log_gross_ia ~ diff_log_gross_ia + -->
<!--                        diff2_log_gross_ia,  -->
<!--                      state.specification = ss.r.diff2, -->
<!--                      data = weekly.tb %>% slice(-c(1,2)), -->
<!--                      niter = 1000,  -->
<!--                      ping = 0,  -->
<!--                      seed = 21) -->
<!-- burn <-  SuggestBurn(0.1, model.rdiff2) -->


<!-- os.preds.r.diff2 <- ComputeOneStepFitted(model.rdiff2,  -->
<!--                                         weekly.tb %>% slice(-c(1,2)), -->
<!--                                         log_gross_ia) -->

<!-- PlotOneStepFitted(os.preds.r.diff2) -->


<!-- components <- cbind.data.frame( -->
<!--   colMeans(model.regres.diff2$state.contributions[-(1:burn), "trend",]),                                -->
<!--   colMeans(bsts.model$state.contributions[-(1:burn), "seasonal.12.1",]), -->
<!--   as.Date(time(Y)))  -->

<!-- ``` -->



# Final Thoughts

Using time series for forecasting is an exceptionally tricky endeavor, and this post is just a brief introduction as I dip my toe into the field. Looking at how time series are used publicly online can often give you the sense that it's a matter of plugging the data into a flexible model, fitting the curve, and hoping for the best. However, it's important to be precise about the inferential goals of this analysis.

The punch line from this analysis might simply be "don't cite a Bayesian time series when the specified joint probability model doesn't naturally reflect our best understanding of the underlying process". And that might be fair, outside of an introductory context I would be hesitant to say much of anything about such a tenuous approaach to modeling (I'd probably tell my boss just to take a look at the movies that are coming out, and see if he recognizes the names, before I try and use past data to predict the weekly box office gross!). But these models are often applied in imperfect ways, and I've found it illustrative to work carefully through one such example, and try and understand the mechanics of the model under the hood. 

Eric Tassone & Farzan Rohani provide an insightful [blog post](http://www.unofficialgoogledatascience.com/2017/04/our-quest-for-robust-time-series.html) examining the way that time series forecasting can and should cite ensemble averaging.



# Sources

* The [bsts package](https://cran.r-project.org/web/packages/bsts/bsts.pdf).
* [Scott & Varian (2014)](https://www.nber.org/chapters/c12995)
* ["Fitting Bayesian structural time series with the bsts R package"](http://www.unofficialgoogledatascience.com/2017/07/fitting-bayesian-structural-time-series.html) by Scott Varian, on the Google Unoficial Data Science Blog.
* ["Sorry ARIMA, but Iâ€™m Going Bayesian
"](https://multithreaded.stitchfix.com/blog/2016/04/21/forget-arima/), by Kim Larsen.
* Thomas Rothenberg's [notes on the Kalman Filter](https://eml.berkeley.edu/~rothenbe/Fall2007/kalman.pdf).
* ["Our quest for robust time series at scale"](http://www.unofficialgoogledatascience.com/2017/04/our-quest-for-robust-time-series.html), by Tassone & Rohani on the Google Unofficial Data Science Blog.
* ["Forecasting: Principles and Practice"](https://otexts.com/fpp2/), by Rob Hyndman and George Athanasopoulos.
* ["Humans in the Loop"](http://www.unofficialgoogledatascience.com/2019/12/humans-in-loop-forecasting-integrating.html), by Thomas Olavson, on The UnofficialGoogle Data Science Blog.




