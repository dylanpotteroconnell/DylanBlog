---
title: The Perils of Overly Local Optimization
author: Dylan O'Connell
date: '2020-05-27'
slug: local-optimization
categories: []
tags:
  - introductory
  - R
  - optimization
subtitle: ''
summary: ''
authors: []
lastmod: '2020-05-27T18:47:28-04:00'
featured: yes
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---
WORK IN PROGRESS, scattered notes/plots here visualized, will be turned into proper post soon. Aims to explain in layman's terms the way that the addition of noise can help gradient descent escape spurious optima (and how this has an intuitive interpretation in our own lives)

```{r, include = FALSE, message = FALSE}
library(tidyverse)
library(gganimate)
library(transformr)
library(gifski)

```

# Outline

* Note, this was openly inspired by the writings of Tim Harford. I ound his descriptiption of the London Tube Strike and its implications while struggling through the study of Langevin diffusions, and I had been meaning to try and show some visualizations of noisy optimization for a while. He's a wonderful popular economics communicator, and a lot of fun to read.

* In general, try and think of any and all images you can include! minimize text. 

# The Perfect Scrambled Eggs 
(or "our optimized lives")

* [rw]My morning routine is perfectly optimized, a blend of art and science. I’ve settled on an order in which to take the ingredients for scrambled eggs, the exact selection of dishes to use to minimize washing, and the fact that my silver bowl has sides just low enough that beaten egg could escape, and that raw eggs go in the red bowl. I didn’t set out to figure this out, I just made scrambled eggs every morning for years, and over time saw tiny choices that make it go a little smoother. 

To add...

# Disruption, and the London Subway Strike

On February 4th, 2014, the workers of the London Tube (their underground subway), went on strike, forcing a number of station closures throughout the city. For three days, a subset of commuters found their usual routes to work blocked, and were forced to find alternatives. After the strike, the stations reopened, and life went back to normal.

But three economists saw the opportunity for a natural experiment: only certain commuters were disrupted by the closure, and using the tracking data collected by the Tube, their movements could be compared to the unaffected group. And surprisingly, life *didn't* go right back to normal after the ending of the strike. The ID card data showed that some of the impacted group stuck with their newly discovered routes, and their commuting time decreased as a result. 

The station closures forced commuters to break from their finely tuned routines, but a surprising number found themselves dragged unnwillingly along a route that proved faster than the one they had been taking for years. The economists even claim that on net, the strike decreased the total commuting time.^[Albeit, this seems much harder to rigorously prove, than to simply cite the study as a neat thought experiment, like I'm doing here.] Even if only a small fraction found new routes, their new route would benefit them for years to come, and taking this chance only required disruption for a couple of days. 

This story provides a neat way to frame the mathematical challenge of optimization, and in particular, why so many optimization algorithms involve *random noise*, a choice which initially seems bizarre. I'll begin by describing how to think about optimization in intuitive terms, and then detail how that corresponds with the common mathematical algorithms, and why the London Tube Strike provides a neat analogy to very real mathematical properties.^[I have no personal insight into the rigor of the experiment itself, it's being used just as a framing device here.]

# The Mathematical Framework of Optimization
<!-- This is duplicate with above? -->
The purpose of this post is to show how this concept can be naturally articulated in a mathematical framework.^[A rare, and extremely tenuous connection between areas of my research, and reality.] We are describing the challenge of optimization. Here's a sample definition, from the [DeepAI website](https://deepai.org/machine-learning-glossary-and-terms/mathematical-optimization).

> Mathematical optimization is the process of maximizing or minimizing an objective function by finding the best available values across a set of inputs. 

* For London commuters, their "objective function" might be the duration of the commute,^[Or you could add in other factors, like cost and pleasantness.], and the "available values across a set of inputs" might be the route they take.
* If we want to find the highest peak in a mountain range (an example we'll discuss in detail in the next section), the input might be the choice of latitude and longitude, and the objective the elevation at that point.
*  When baking chocolate chip cookies, the possible inputs might be the amount of each ingredient, and the order in which they are combined, and the objective function might be the tastiness of the resulting cookie. This isn't quite a joke, Google once ran a [cookie optimization experiment](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46507.pdf) along these lines.^[I've tasted the resulting cookies, not bad for a cafeteria batch.]

Optimization is vitally important in applied mathematics and statistics, but it can be quite hard. The vast increase in computational power has broadened our horizons for what optimization challenges are possible, but many useful problems remain out of our reach.

Generally, in optimization, we assume that we can check the value of the objective function at some chosen input. We can try a certain route on the subway and time the duration, we can bake a batch of cookies and eat them, and we will imagine that we have some computer which takes in latitude and longitude coordinates and spits out the altitude (this is called a "query" to the objective function, or perhaps "oracle"). 

Our naive optimization strategy might be to just try a bunch of points. Maybe we can define a grid of latitude and longitude coordinates, check the elevation of each one, and pick the highest. But this naive approach only works when the potential choices of input are sufficiently narrow. In particular, we look at the "dimension" of the input space. Latitude and longitude represent a mere 2-dimensions. The number of points within an exhaustive "grid" of inputs grows exponentially with the dimension of the input. Cookies are comprised by some dozen ingredients. The famous Jacques Torres [cookie recipe](https://cooking.nytimes.com/recipes/1015819-chocolate-chip-cookies) has $12$ ingredients. If we wanted to try all combinations of a mere five levels of each ingredient, that would be over two hundred thousand possibilities, which is a bit much for even the hungriest baker. 

We are constantly confronting optimization challenges in our own life, and we rarely take this naive approach. Instead, we tend to look for small *local* optimizations, whose benefits we can easily identify. 

<!-- * Finding the best route on the subway is something we optimize. While often algorithmic services can do the work for us, in other cases (or the past), we have to figure it out ourselves, and we somehow pick our starting point. -->

<!-- * For an example, let's imagine we want to find the highest point in a mountain range. We can check an individual point, and see how high it is. How do we find the highest point? The analogy would be something like, instead of latitude and longitude in a mountain range, in our morning routine it would be that "series of choices for how we get to work".  -->

<!-- * Now, imagine instead of querying a computer for the height, you yourself were actually in that mountain range. What would you do? Well, you'd look for the peak, but what if you were in a forest? You'd go up. Specifically, you would look at the area around you for *local* guidance. -->

# Locality in Optimization

Imagine if we bite into a warm cookie, and the text the texture is just right, but there's not enough chocolate.(more flowery language here?) We wouldn't just throw up our hands and start from scratch... we'd add more chocolate! It's hard to look at a cookie recipe, and imagine what the result will taste like. But we *can* taste a recipe, and imagine what a small tweak would taste like.

The "locality" in optimization refers to "closeness" in the input space. In latitude and longitude, this would be literal distance, while the definitions are a bit less cookie in the space of cookie ingredients and subway commutes. Broadly, we are pretty good at understanding. If we increase the amount of chocolate, we now they'll taste a bit more chocolate-y than they do now. If we increase the amounts of chocolate, butter, baking soda, reduce the amount of flour, and tweak the balance between brown and cane sugar, are we confident we know what the result will taste like? 

In the London Tube experiment, the researchers place some of the blame on the "stylized nature" of the Tube map displayed to commuters. Its spatial distortions make it difficult to spot major inefficiencies in their route, without some external push. By comparison, we can usually estimate the impact of these "local" changes by ourselves. If we get off one stop earlier, maybe our walk to work increases by a minute but we avoid the wait at a crowded station. These slight changes are a bit like increasing just a single ingredient in a cookie.

![](london_tube_map.jpg)

Personally, this describes my relationship with my own food. TO ADD MORE...

* In my own life, I find this with food all the time. I have a set rotation of dishes I love, that fit my criteria (hassle-free ingredients, saves well for leftoers, etc). I'm happy to make small chaanes to what I eat. I recently realized how well a lone, unadorned sweet potato went with some of my staples. That's a one step addition, and I can evaluate its impact as I walk the aisles of the store. Sure enough, it quickly became a staple.
* However, I'm sure there are many full dishes out there that would be just as good as my current rotation. But to find a new dish is a risk. There's no way for me to see all the the links in the chain. Will the ingredients be easy to find? Will I find the cooking burdensome? What will the end result taste like? Are my cooking supplies well suited to the task?

<!-- * We provide some new rules of the game. We know not only the value at these points, we know the shape/slope/etc of the area around it. This is a reasonable relaxation, because we can estimate this shape by querying our computer for the values of the areas near our point (are we going up or down). The rule is that we can't look far away to see where the peak is (perhaps we're stuck in the Appalachian forests, and not the sheer granite of the Sierra Nevada).  -->

<!-- * This isn't arbitrary, it's a natural understanding of how we actually operate. We are inherently *local* optimizers. We can easily assess the impact of small changes, not not large ones. -->

<!-- * On a morning commute, we can estimate whether taking a turn a few blocks early to avoid a stoplight saves us time. It's much harder to figure out in advance whether taking the bus or train is easy. -->

<!-- * Now, I'd wager that generally, we're rather good at making small (later, we will call these "local", due to their geometric interpretation) adjustments. If I realize that by taking a right a few blocks early, I can avoid the long traffic light, I'll often do it.  -->
<!-- * However, there's a reason these are small changes. We can estimate their impact from where we are. If we considered an entirely new route to work, taking a whole different train line, we would have no way to know what that's like. -->

Before, we imagined a challenge where we tried to find a peak of a mountain range, using a computer which could tell us the elevation of any chosen input point. Let's imagine that it also tells us the slope of the incline at that point ("which direction is down"). This better reflects our intuition for local optimization, but it's also not a large change to the game, because we could always simply query the elevation in a small area around our point, and gauge the slope ourselves. Imagine that we are hiking with an altimeter, and we can look around us and see the shape of the nearby slope. However, we can't just look around and find a faraway peak, because we're stuck in the forests of the Appalachians rather than the sheer granite ofthe Sierra Nevada. Our intuition tells us the optimization algorithm which is natural: from wherever we are, "go up".

# Gradient Ascent

This intuitive is called "gradient ascent". Wherever you are, go up. This is quite literal in the case of our mountain range, This section will provide its mathematical definition, but these details aren't important, and you can skip the notation and go straight to the visualizations if you'd like. We begin with the intuitive description, and below write the algorithm out mathematically.  

In a bit more detail, we begin at some initial point. We find the direction of steepest ascent from that point, i.e. up the hill (the magnitude and direction of steepest ascent form the "gradient" vector). Then, we walk for a period in that direction (the amount is determined by the "step size", and how steep the ascent actually is). Once we finish walking, we are at a new point, and we repeat this process again (we determine direction of steepest ascent, and walk in that direction). Once we reach a point that is essentially flat, there is no more direction of steepest ascent to follow, and we are done (this is called "convergence"). 

This is the intuitive definition of what it means to make *local* changes. Wherever we are, we think "what small change could we make to make this better?" If our cookies are too salty, we add a bit less salt, but we don't start over from scratch. 

## Mathematical Definition

<!-- Some explanation of continuous vs discrete space would be nice? -->
Imagine we are at some point $x \in \mathbb{R}^d$ (meaning, $x$ is a point in $d$-dimensional space). 
(Maybe in English first?)

Here's a mathematical definition of a crude version of gradient descent.^[This is for an extremely crude version with fixed step size $\eta$. There is no reason  to use fixed step size in practice, but that adjustment isn't relevant to the demonstration.] 

1. Let $f: \mathbb{R}^d \to \mathbb{R}$ be our objective function. Let $\eta > 0$ be our scaling constant. We begin at some initial point $x^{(0)} \in \mathbb{R}^d$. For $k = 0, \ldots,$, until convergence, repeat steps 2 through 4(?).
2. Compute the gradient $\nabla f(x^{(k)})$.
3. Set $x^{(k+1)} \leftarrow x^{(k)} + \eta \nabla f(x^{(k)})$.
4. If $\nabla f(x^{(k)})$ is sufficiently small, halt the algorithm, and select $x^{(k+1)}$ as our optima. Otherwise, set $k \leftarrow k+1$, and repeat steps 2-4.


<!-- http://timharford.com/2016/10/big-decision-ahead-just-roll-the-dice/ -->



## Visualizing Gradient Ascent

It's one thing to say that we follow the direction of steepest ascent, but what does that actually look like in practice? Imagine we have to summit a hill. This one is perfectly round, for simplicitly, but pretend we don't know that, we can only see the local area around us. 
<!-- Need to go through and do better labels/etc for all these visualizations!  -->s
<!-- (Show the unimodel 2d case ) -->

The following is a contour plot, like what you find on a topographical map. 
```{r, include = FALSE, mesage = FALSE}
library(rayshader)
library(magick)

GradBivarNormal <- function(x){
  -c(prod(dnorm(x))*x[1], prod(dnorm(x))*x[2])
}

x.uni.init <- c(1.8, 1.8)

grid.count <- 200
tb.uni <- crossing(x = seq(-2, 2, length.out = grid.count),
                   y = seq(-2, 2, length.out = grid.count)) %>%
  mutate(z = apply(dnorm(cbind(x,y)), 1, prod))

g.uni <- tb.uni %>% 
  ggplot(aes(x = x, y = y)) +
  geom_tile(aes(fill = z)) +
  geom_contour(aes(z = z), bins = 15, color = "black") +
  scale_fill_gradientn("Elevation", colours = terrain.colors(10)) +
  coord_fixed() 
  

uni.first.point <- geom_point(aes(x = x.uni.init[1], 
                       y = x.uni.init[2]), 
                       col = "red", size = 2)  
g.uni + uni.first.point  
```

It might be more clear if we consider viewing the map at an angle (thanks to the `rayshader` [package](https://www.tylermw.com/3d-ggplots-with-rayshader/).

```{r, include = FALSE}
# # The animations, and the 3D plots, are all slow to run.
# # It is easier to run them once, and store the images for
# # display in the RMD file, rather than run them each time
# # I want to check the look of the post.
# plot_gg(g.uni + uni.first.point,
#         multicore = TRUE,
#         raytrace = TRUE,
#         width = 7,
#         height = 4,
#         scale = 300,
#         windowsize = c(1400, 866),
#         zoom = .5,
#         phi = 30,
#         theta = 15)
# render_snapshot(filename = "uni_vis_3d", clear = TRUE)
```

![](uni_vis_3d.png)


Imagine we start at that red dot in the top right. We look around, and see that the direction of steepest ascent is down and to the right. 

```{r}
eta.temp <- 30
grad.first.step <- GradBivarNormal(x.uni.init)
# Manually compute second step, for visualization
x.uni.2 <- x.uni.init + eta.temp*grad.first.step

g.uni + uni.first.point + 
  geom_segment(aes(x = x.uni.init[1], y = x.uni.init[1],
                   xend = x.uni.2[1], yend = x.uni.2[2]),
               col = "red", size = .25,
               arrow = arrow(length = unit(.2, "cm"), type = "closed"))
```

CONSIDER ONE MORE STEP
```{r, include = FALSE}
grad.second.step <- GradBivarNormal(x.uni.2)
# This algorithm is modified for visualization purposes...
# in reality the first step would be much smaller than the second,
# but we're just trying to give the right intuition here.
x.uni.3 <- x.uni.2 + (eta.temp/2)*grad.second.step

g.second.step <- g.uni + 
  geom_point(aes(x = x.uni.2[1], 
                 y = x.uni.2[2]), col = "red", size = 2) + 
  geom_segment(aes(x = x.uni.2[1], y = x.uni.2[1],
                   xend = x.uni.3[1], yend = x.uni.3[2]),
               col = "red", size = .25,
               arrow = arrow(length = unit(.2, "cm"), type = "closed"))
g.second.step
```
Or in 3D...
```{r, include = FALSE}
# plot_gg(g.second.step,
#         multicore = TRUE,
#         raytrace = TRUE,
#         width = 7,
#         height = 4,
#         scale = 300,
#         windowsize = c(1400, 866),
#         zoom = .4,
#         phi = 30,
#         theta = 15)
# render_snapshot(filename = "uni_vis_3d_step2", clear = TRUE)
```

![](uni_vis_3d_step2.png)



Below, we can watch a gif of the algorithm in action. Sure enough, in this simple setting, we head right to the top.  [rw]


```{r, include = FALSE}
uni.init <- c(1.8, 1.8)

steps <- 50
x.asc.seq <- matrix(rep(NA, 2*steps), ncol=2)
x.asc.seq[1,] <- uni.init
grad.seq <- matrix(rep(NA, 2*steps), ncol=2)
eta <- 2


for (i in 1:(steps-1)) {
  grad.seq[i,] <- GradBivarNormal(x.asc.seq[i,])
  x.asc.seq[i+1,] <-  x.asc.seq[i,] + grad.seq[i,]*eta
}

tb.asc <- tibble(x = x.asc.seq[,1], 
                 y = x.asc.seq[,2],
                 grad.x = grad.seq[,1],
                 grad.y = grad.seq[,2],
                 iter = 1:steps) %>% 
  mutate(x.next = x + grad.x*eta,
         y.next = y + grad.y*eta)
```

<!-- ```{r, include = FALSE} -->
<!-- g.uni <- tb.uni %>%  -->
<!--   ggplot(aes(x = x, y = y)) + -->
<!--   geom_tile(aes(fill = z)) + -->
<!--   geom_contour(aes(z = z), bins = 15, color = "black") + -->
<!--   scale_fill_gradientn("z", colours = terrain.colors(10)) + -->
<!--   coord_fixed() -->
<!-- g.uni -->
<!-- ``` -->

```{r, include = FALSE}
# p.uni.ascent <- g.uni + 
#   geom_point(data = tb.asc,
#              col = "red", 
#              size = 1) +
#   geom_segment(data = tb.asc,
#                aes(x = x, y = y, 
#                    xend = x.next, yend = y.next),
#                col = "red", size = .25,
#                arrow = arrow(length = unit(.2, "cm"), type = "closed"))
# 
# anim.uni.ascent <- p.uni.ascent + transition_states(iter,
#                                             transition_length = 1,
#                                             state_length = 1)
# animate(anim.uni.ascent,
#         nframes = 10 + max(tb.asc$iter),
#         renderer = gifski_renderer("2d_uni_grad_ascent_anim.gif"))
```
![](2d_uni_grad_ascent_anim.gif)
<!-- THIS angle isn't very good, will have to adjust -->

# Not All Mountains Are Friendly

In this simple example, our local minimization strategy ("just go up!") finds the highest peak without any trouble. However, not all mountain ranges are quite so friendly. Imagine there were three different peaks, of differing heights. It's a bit harder to picture, but there's a 3D visualization after the contour plot to help.

```{r, include = FALSE}
w.2d <- c(1/3, 1/3, 1/3)
means.2d <- rbind(matrix(c(0, 4), nrow = 1),
                  matrix(c(sqrt(25/2), -sqrt(25/2)), nrow = 1),
                  matrix(c(-sqrt(25/2), -sqrt(25/2)+1), nrow = 1))
# We assume spherical variances
sd.2d <- c(1.7, 1.5, 2)

NormalMixtureDensity <- function(x) {
  component.probs <- sapply(1:3, function(i) dnorm(x, 
                                                   means.2d[i,], 
                                                   sd.2d[i]))
  apply(component.probs, 2, prod) %*% w.2d
}
NormalMixtureDensity_Vec <- function(x, y) {
  map2_dbl(x, y, function(.x, .y) {NormalMixtureDensity(c(.x,.y))})
}

grid.count <- 200
tb.grid <- crossing(x = seq(-10, 10, length.out = grid.count),
                    y = seq(-10, 10, length.out = grid.count)) %>%
  mutate(z = NormalMixtureDensity_Vec(x, y))
```


```{r}
# Basic plot of the setup
g.mix <- tb.grid %>% 
  ggplot(aes(x = x, y = y)) +
  geom_tile(aes(fill = z)) +
  geom_contour(aes(z = z), bins = 15, color = "black") +
  ylim(-7.5,7.5) + xlim(-7.5, 7.5)+
  scale_fill_gradientn("z", colours = terrain.colors(10)) +
  coord_fixed()
g.mix
```

```{r, include = FALSE}
# plot_gg(g.mix,
#         multicore = TRUE,
#         raytrace = FALSE,
#         width = 7,
#         height = 4,
#         scale = 300,
#         windowsize = c(1400, 866),
#         zoom = .3,
#         phi = 30,
#         theta = 30)
# render_snapshot(file = "mix_vis_3d", clear = TRUE)
```

![](mix_vis_3d.png)


<!-- * 3d view, to help get a sense of it. -->
<!-- ```{r, include = FALSE} -->
<!-- #par(mfrow = c(1, 2)) -->
<!-- # plot_gg(g, width = 7, height = 4, raytrace = FALSE, preview = TRUE) -->
<!-- # plot_gg(g, multicore = TRUE, raytrace = TRUE, width = 7, height = 4,  -->
<!-- #         scale = 300, windowsize = c(1400, 866),  -->
<!-- #         zoom = 0.6, phi = 30, theta = 30) -->

<!-- # plot_gg(g,  -->
<!-- #         raytrace = FALSE,  -->
<!-- #         preview = TRUE) -->
<!-- # plot_gg(g,  -->
<!-- #         multicore = TRUE,  -->
<!-- #         raytrace = TRUE, -->
<!-- #         zoom = 0.6,  -->
<!-- #         phi = 30,  -->
<!-- #         theta = 30) -->
<!-- # render_camera(zoom=0.5,theta=-30,phi=30) -->
<!-- # render_snapshot(clear = TRUE) -->
<!-- # Sys.sleep(0.2) -->
<!-- # render_snapshot(clear = TRUE) -->
<!-- ``` -->

* Animation, currently on 2d, to show how gradient ascent works

```{r, include = FALSE}

# The gradient of the normal w.r.t. x is (x-mu)/sigma^2*density.
GradMixtureNormal <- function(x) {
  # The densities for each of the three components
  component.probs <- apply(sapply(1:3, function(i) dnorm(x, 
                                                         means.2d[i,], 
                                                         sd.2d[i])), 2, prod)
  # Then, each of the three components have their x and y derivative.
  
  # We compute the  six (x-mu_i)/sigma_i^2 parts.
  mu.minus.x <- apply(means.2d, 1, function(r) r-x)
  mu.minus.x.div.sig <- t(apply(mu.minus.x, 1, function(r) r/sd.2d^2))
  
  # We multiply each by their corresponding component density
  temp <- t(apply(mu.minus.x.div.sig, 1, function(r) r*component.probs))
  return(c(temp %*% w.2d))
}

steps <- 200
x.asc.seq <- matrix(rep(NA, 2*steps), ncol=2)
x.asc.seq[1,] <- c(1.3, 0)
grad.seq <- matrix(rep(NA, 2*steps), ncol=2)
eta <- 40


for (i in 1:(steps-1)) {
  grad.seq[i,] <- GradMixtureNormal(x.asc.seq[i,])
  x.asc.seq[i+1,] <-  x.asc.seq[i,] + grad.seq[i,]*eta
}

tb.mix.asc <- tibble(x = x.asc.seq[,1], 
                     y = x.asc.seq[,2],
                     grad.x = grad.seq[,1],
                     grad.y = grad.seq[,2],
                     iter = 1:steps) %>% 
  mutate(x.next = x + grad.x*eta,
         y.next = y + grad.y*eta)
```


```{r, include = FALSE}
# p.mix.ascent <- g.mix +
#   geom_point(data = tb.mix.asc,
#              col = "red",
#              size = 1) +
#   geom_segment(data = tb.mix.asc,
#                aes(x = x, y = y,
#                    xend = x.next, yend = y.next),
#                col = "red", size = .25,
#                arrow = arrow(length = unit(.2, "cm"), type = "closed"))
# 
# anim.mix.ascent <- p.mix.ascent + transition_states(iter,
#                                             transition_length = 1,
#                                             state_length = 1)
# animate(anim.mix.ascent,
#         nframes = 10 + max(tb.mix.asc$iter),
#         renderer = gifski_renderer("2d_mix_grad_ascent_anim.gif"))

```

![](2d_mix_grad_ascent_anim.gif)

## The Perils of "Nonconvexity"


* Which optimization challenges are hard? Math draws a fairly simple distinction. ^[Like maximization and minimization, convex and concave will be used interchangeably. The multiplying a convex function by minus $1$ makes it concave. Concave functions are easy to maximize, but we can switch between the two interchangeably.]

* In short, in the convex realm, local information provides global guidance. The local slope is pointing us in a direction 

* Description of convex vs non-convex optimization, why they are two fundamentally different challenges.


# Embracing Random Noise

* The explanation of the noise step.



# SGD, and Neural Nets

* We have seen randomization introduced quite literally, by a random movement. But randomization is often introduced less explicitly into popular algorithms. 
* Neural networks are the much heralded staple of the rise of machine learning. Their optimization, perhaps consistent of millions or billions of dimensions (we considered a *single* dimension for visualization purposes above)
* Microsoft boasted of a neural network with [17 billion(!)](https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/) parameters, each of which are a dimension to be optimized. 
* The result is an enormous amount of potential spurious optima. 
* What we've found over the past decade, SGD works "unreasonably well". It's explicitly an algorithm for convex optimization, and yet it optimizes these deep networks.
* There are a variety of complex answers, depending on who you ask, but the staple reason which dates back [decades](https://leon.bottou.org/publications/pdf/nimes-1991.pdf) is that its inherent noisiness allows it to escape spurious local optima.^[And "saddle points", which I am skirting past here, as they are a similar concern for this high level explanation.] The noisiness comes not fro our explicit introduction, but from the its approximation error.
* The noise of SGD has little analogy in our own lives, but the way that noise becomes a tool, not a hindrance, in non-convex optimization, is very real.



SGD is the staple way to optimize neural networks. 

# Simulated Annealing/the other random optimization approaches?


# Wrapping Up

Of course, this is *hard*. 

A pointed self help message would say to fine and embrace randomness. But with no such agenda, I can say that’s true, but also incredibly hard to get right 

just look at the gif! this is all powerful theory, but hard to chose the right way to introduce noise. in a clean, mathematical scenario, we can study the sorts of noise that are optimal, without any cost. when it comes to our own personal life, we experience this noise as disruption. 

The famous computer scientist Alan Perlis said "Optimization hinders evolution", and it's true. This is the fundamental "explore vs exploit" trade-off. We could spend our energy searching for better strategies ("explore"), or we could focus on deriving the benefit from the currently known optimal strategy ("exploit"). In almost every application, there is a trade-off, and it's very hard to get right. I think our status quo bias and inherently "local" perspective means we'll typically err on the side of spending too little time exploring, but of course it's hard to know how to do that. 

But the takeaway could be something as simple as welcoming disruption, rather than resisting it. There's no point in getting mad at a change to your commute, the world won't notice (cite quote?), but maybe you can use the simple platitude that this is the necessary phase of exploration. 

Scrambled eggs are a wonderful way to start my morning. I doubt there's much more room to optimize the way I enjoy them. But I'd be foolish to think that there weren't wildly different options that were just as rewarding. [rw] 

```{r, include = FALSE}
location.params <- c(-10, 0, 10)
scale.params <- c(3.5, 3, 4)
weights <- c(.2, .4, .4)

CauchyMixtureDensity <- function(x,
                                 location.params,
                                 shape.params,
                                 weights) {
  densities <- map2(location.params, 
                    scale.params, 
                    function(.loc, .scale) dcauchy(x,
                                                   location = .loc, 
                                                   scale = .scale)) 
  return(do.call(cbind, densities) %*% weights)
}


DerivCauchy <- function(x, location, scale) {
  2*(location-x)/(scale^3*pi*(1+(-location+x)^2/scale^2)^2)
}

CauchyMixtureDerivative <- function(x,
                                    location.params,
                                    shape.params,
                                    weights) {
  densities <- map2(location.params, 
                    scale.params, 
                    function(.loc, .scale) DerivCauchy(x,
                                                       location = .loc, 
                                                       scale = .scale)) 
  return(do.call(cbind, densities) %*% weights)
}

# Scalar for gradient ascent
x.init <- 18
GradientAscentStep <- function(x, 
                               eta, 
                               location.params, 
                               scale.params, 
                               weights,
                               type = "Gradient Ascent") {
  deriv <- CauchyMixtureDerivative(x, 
                                   location.params, 
                                   scale.params, 
                                   weights) %>% 
    as.numeric()
  xprime <- x + eta*deriv + ifelse(type == "Noisy Ascent", 
                                   rnorm(1, 0, .5), 
                                   0)
  return(xprime)
}



RunGradientAscent <- function(x.init, 
                              location.params,
                              scale.params,
                              weights,
                              eta,
                              type = "Gradient") {
  max.iter <- 10000
  ascent.seq <- c(x.init)
  converged <- FALSE
  i <- 1
  while (! converged && i < max.iter) {
    ascent.seq[i+1] <- GradientAscentStep(ascent.seq[i],
                                          eta,
                                          location.params,
                                          scale.params,
                                          weights,
                                          type)
    i <- i + 1
    if (abs(ascent.seq[i] - ascent.seq[i-1])*100 < 10^(-4)) {
      converged <- TRUE
    }
  }
  return(ascent.seq)
}
tb.dens.grid <- tibble(x = seq(-20, 20, .1)) %>%
  mutate(y = as.numeric(CauchyMixtureDensity(x, 
                                             location.params, 
                                             scale.params, 
                                             weights)))
eta.grad.ascent <- 100
grad.ascent.seq <- RunGradientAscent(x.init, 
                                     location.params,
                                     scale.params,
                                     weights,
                                     eta = eta.grad.ascent, 
                                     type = "Gradient Ascent")

```


```{r, include = FALSE}
# tb.grad.ascent <- tibble(x = grad.ascent.seq,
#                          iter = seq_along(grad.ascent.seq)) %>%
#   arrange(iter) %>%
#   mutate(y = as.numeric(CauchyMixtureDensity(x, 
#                                              location.params, 
#                                              scale.params, 
#                                              weights)),
#          grad = as.numeric(CauchyMixtureDerivative(x, 
#                                                    location.params, 
#                                                    scale.params, 
#                                                    weights)),
#          x.next = x + eta.grad.ascent*grad,
#          y.next = as.numeric(CauchyMixtureDensity(x.next, 
#                                                   location.params, 
#                                                   scale.params, 
#                                                   weights)))
# p.ascent <- ggplot(tb.dens.grid, aes(x = x, y = y)) + 
#   geom_line() +
#   geom_point(data = tb.grad.ascent, 
#              col = "red", size = 1) +
#   geom_segment(data = tb.grad.ascent,
#                aes(x = x, y = y, xend = x.next, yend = y.next),
#                col = "red", size = .25,
#                arrow = arrow(length = unit(.2, "cm"), type = "closed"))  
# p.ascent
# 
# anim.ascent <- p.ascent + transition_states(iter,
#                                             transition_length = 1,
#                                             state_length = 1)
# animate(anim.ascent, 
#         nframes = 10 + max(tb.grad.ascent$iter),
#         renderer = gifski_renderer("test_grad_ascent_anim.gif"))
```
![](test_grad_ascent_anim.gif)


```{r, include = FALSE}
# eta.noisy.ascent <- 25
# noisy.ascent.seq <- RunGradientAscent(x.init, 
#                                       location.params,
#                                       scale.params,
#                                       weights,
#                                       eta = eta.noisy.ascent, 
#                                       type = "Noisy Ascent")
# 
# tb.noisy.ascent <- tibble(x = noisy.ascent.seq,
#                           iter = seq_along(noisy.ascent.seq)) %>%
#   arrange(iter) %>%
#   mutate(y = as.numeric(CauchyMixtureDensity(x, 
#                                              location.params, 
#                                              scale.params, 
#                                              weights)),
#          grad = CauchyMixtureDerivative(x, 
#                                         location.params, 
#                                         scale.params, 
#                                         weights),
#          x.next = x + eta.grad.ascent*grad,
#          y.next = as.numeric(CauchyMixtureDensity(x.next, 
#                                                   location.params, 
#                                                   scale.params, 
#                                                   weights)))
# # For now, only plot until it finds the max
# true.max <- max(tb.noisy.ascent$y)
# first.time.max <- tb.noisy.ascent %>% 
#   filter(y > .99*true.max) %>%
#   arrange(iter) %>%
#   slice(1) %>%
#   pull(iter)
# tb.noisy.ascent <- tb.noisy.ascent %>%
#   slice(1:first.time.max)
# 
# 
# 
# 
# p.noisy <- ggplot(tb.dens.grid, aes(x = x, y = y)) + 
#   geom_line() +
#   geom_point(data = tb.noisy.ascent, 
#              col = "red", size = 1) +
#   geom_segment(data = tb.noisy.ascent,
#                aes(x = x, y = y, xend = x.next, yend = y.next),
#                # aes(x = x, y = y, xend = x.next, yend = y.next),
#                col = "red", size = .25,
#                arrow = arrow(length = unit(.2, "cm"), type = "closed")) 
# p.noisy
# 
# anim.noisy <- p.noisy + transition_states(iter,
#                                           transition_length = 1,
#                                           state_length = 1)
# animate(anim.noisy, 
#         nframes = 10 + max(tb.noisy.ascent$iter),
#         renderer = gifski_renderer("test_noisy_ascent_anim.gif"))

```

![](test_noisy_ascent_anim.gif)

<!-- ```{r, include = FALSE} -->
<!-- # f <- function(x) { -->
<!-- #   (-x^2 + 3*x -2)*(x^2 -.5*x + 6) -->
<!-- # } -->
<!-- # x.seq <- seq(-10, 10, .1) -->
<!-- # plot(x.seq, f(x.seq)) -->
<!-- ``` -->


# Sources
* ["Messy"](http://timharford.com/books/messy/), by Tim Harford.
* ["The Benefits of Forced Experimentation: Striking
Evidence from the London Underground Network" ](http://users.ox.ac.uk/~econ0360/FerdinandRauch/Tube.pdf), by Larcom, Rauch, & Willems (2017).
* Short [write-up](http://cep.lse.ac.uk/pubs/download/cp455.pdf) of the London Tube Strike research. 
* ["Bayesian Optimization for a Better Dessert"](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46507.pdf), by Kochanski et al. (2017). 
* The [`rayshader` package](https://www.rayshader.com/).